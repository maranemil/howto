GNU Wget 1.20 Manual
https://www.gnu.org/software/wget/manual/wget.html#Recursive-Download


wget -r --random-wait 1.5 --wait 1.5 http://www.example.com/
wget --spider --force-html -i bookmarks.html                            # check page
wget -r -l1 --no-parent -A.gif http://www.example.com/dir/              # Retrieve the first level
wget -p --convert-links -nH -nd -Pdownload  http://www.ex.com/pg.html   # Retrieve only one HTML page save all those files under a download/ subdirectory
wget -r -l2 -P/tmp ftp://wuarchive.wustl.edu/                           # Retrieve the first two levels
wget -r -l1 --no-parent -A.gif http://www.example.com/dir/              # download all the GIFs from a directory o
wget -nc -r https://www.gnu.org/                                        # resume download

test

wget -r -l1 -Pdownload --random-wait 1.5 --wait 1.5 -A.html --convert-links



wget --content-disposition -i foo.txt


wgetOutPut=filename_or_fullpath_and_name
wget -t 20 --content-disposition $link -O $wgetOutPut


