#######################################
#
#   Best ML package in python
#
#######################################

Orange
PyBrain
scikit-learn
mlpy
PyML

For SVM - libsvm (there is native python binding)
for NaiveBayes - pymc
for NN - pybrain
for all this in one library - scikit-learn

---------------------------------
#######################################
#
#   Pythin Versus C
#
#######################################

Python:
x = 1
while x <= 14:
y = 14 – x
print str(x) + “|” + str(y)
if x**2 + y**2 == 100:
print “match”
x = x + 1

C:
#include (<)stdio.h(>)

int main()
{
int x, y, t;
for (x = 1; x <= 14; x++) {

y = 14 – x;
printf(“%d|%d\n”, x, y);
if ((x*x) + (y*y) == 100)
printf(“match\n”);
}
return 0;
}




#################################################
#
#   J48 tree in R - train and test classification
#
#################################################


library(caTools)
library(RWeka)

data <- read.csv("try.csv")
spl = sample.split(data$someAttribute, SplitRatio = 0.7)

dataTrain = subset(data, spl==TRUE)
dataTest = subset(data, spl==FALSE)

resultJ48 <- J48(as.factor(classAttribute)~., dataTrain)
dataTest.pred <- predict(resultJ48, newdata = dataTest)
table(dataTest$classAttribute, dataTest.pred)


##################################################
#
#   J48 Classifier - python-weka-wrapper — selecting Class attribute
#   http://weka.sourceforge.net/doc.dev/weka/core/Instances.html#setClassIndex(int)
#
##################################################


jvm.start()
loader = Loader(classname="weka.core.converters.ArffLoader")
dataweka = loader.load_file("enumeratedremovedtest-arff.arff")
dataarff = arff.load(open('enumeratedremovedtest-arff.arff', 'rb'))

from weka.classifiers import Classifier
dataweka.class_is_last()
cls = Classifier(classname="weka.classifiers.trees.J48", options=["-C","0.25", "-B", "-M", "2"])
print dataweka.class_attribute
cls.build_classifier(dataweka)
print(cls)
jvm.stop()


----------------------------------------------------------------

########################################################
#
#   Weka_classifier_trees
#   https://en.m.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Packages/RWeka/Weka_classifier_trees
#
########################################################

m1 <- J48(Species ~ ., data = iris)
   summary(m1)
   table(iris$Species, predict(m1))
   if(require("party", quietly = TRUE)) plot(m1)
   write_to_dot(m1)
   library("Rgraphviz")
   ff <- tempfile()
   write_to_dot(m1, ff)
   plot(agread(ff))
   DF2 <- read.arff(system.file("arff", "contact-lenses.arff", package = "RWeka"))
   m2 <- J48(`contact-lenses` ~ ., data = DF2)
   table(DF2$`contact-lenses`, predict(m2))
   if(require("party", quietly = TRUE)) plot(m2)
   DF3 <- read.arff(system.file("arff", "cpu.arff", package = "RWeka"))
   m3 <- M5P(class ~ ., data = DF3)
   if(require("party", quietly = TRUE)) plot(m3)
   DF4 <- read.arff(system.file("arff", "weather.arff", package = "RWeka"))
   m4 <- LMT(play ~ ., data = DF4)
   table(DF4$play, predict(m4))
   if(require("mlbench", quietly = TRUE) && require("party", quietly = TRUE)) {
       data("PimaIndiansDiabetes", package = "mlbench")
       m5 <- J48(diabetes ~ ., data = PimaIndiansDiabetes, control = Weka_control(R = TRUE))
       plot(m5)
   }


-----------------------------------

###############################################
#
#   RegressionTests/Code/J48.R
#   https://github.com/topepo/caret/blob/master/RegressionTests/Code/J48.R
#   https://raw.githubusercontent.com/topepo/caret/master/RegressionTests/Code/J48.R
#
###############################################

timestamp <- Sys.time()
library(caret)
library(plyr)
library(recipes)
library(dplyr)

model <- "J48"

for(i in getModelInfo(model)[[1]]$library)
  do.call("requireNamespace", list(package = i))

#########################################################################

set.seed(2)
training <- twoClassSim(50, linearVars = 2)
testing <- twoClassSim(500, linearVars = 2)
trainX <- training[, -ncol(training)]
trainY <- training$Class

rec_cls <- recipe(Class ~ ., data = training) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

cctrl1 <- trainControl(method = "cv", number = 3, returnResamp = "all",
                       classProbs = TRUE,
                       summaryFunction = twoClassSummary)
cctrl2 <- trainControl(method = "LOOCV",
                       classProbs = TRUE, summaryFunction = twoClassSummary)
cctrl3 <- trainControl(method = "none",
                       classProbs = TRUE, summaryFunction = twoClassSummary)

set.seed(849)
test_class_cv_model <- train(trainX, trainY,
                             method = "J48",
                             trControl = cctrl1,
                             metric = "ROC",
                             preProc = c("center", "scale"))

set.seed(849)
test_class_cv_form <- train(Class ~ ., data = training,
                            method = "J48",
                            trControl = cctrl1,
                            metric = "ROC",
                            preProc = c("center", "scale"))

test_class_pred <- predict(test_class_cv_model, testing[, -ncol(testing)])
test_class_prob <- predict(test_class_cv_model, testing[, -ncol(testing)], type = "prob")
test_class_pred_form <- predict(test_class_cv_form, testing[, -ncol(testing)])
test_class_prob_form <- predict(test_class_cv_form, testing[, -ncol(testing)], type = "prob")

set.seed(849)
test_class_loo_model <- train(trainX, trainY,
                              method = "J48",
                              trControl = cctrl2,
                              metric = "ROC",
                              preProc = c("center", "scale"))

set.seed(849)
test_class_none_model <- train(trainX, trainY,
                               method = "J48",
                               trControl = cctrl3,
                               tuneLength = 1,
                               metric = "ROC",
                               preProc = c("center", "scale"))

test_class_none_pred <- predict(test_class_none_model, testing[, -ncol(testing)])
test_class_none_prob <- predict(test_class_none_model, testing[, -ncol(testing)], type = "prob")

set.seed(849)
test_class_rec <- train(x = rec_cls,
                        data = training,
                        method = "J48",
                        trControl = cctrl1,
                        metric = "ROC")


if(
  !isTRUE(
    all.equal(test_class_cv_model$results,
              test_class_rec$results))
)
  stop("CV weights not giving the same results")

test_class_imp_rec <- varImp(test_class_rec)


test_class_pred_rec <- predict(test_class_rec, testing[, -ncol(testing)])
test_class_prob_rec <- predict(test_class_rec, testing[, -ncol(testing)],
                               type = "prob")

test_levels <- levels(test_class_cv_model)
if(!all(levels(trainY) %in% test_levels))
  cat("wrong levels")

#########################################################################
#
#   test_class_predictors1 <- predictors(test_class_cv_model)
#
#########################################################################

tests <- grep("test_", ls(), fixed = TRUE, value = TRUE)

sInfo <- sessionInfo()
timestamp_end <- Sys.time()

save(list = c(tests, "sInfo", "timestamp", "timestamp_end"),
     file = file.path(getwd(), paste(model, ".RData", sep = "")))

q("no")


--------------------------------------------------------------
#########################################################################
#
#   http://data-mining.business-intelligence.uoc.edu/home/j48-decision-tree
#
#########################################################################

> install.packages(RWeka)
> install.packages(party)
# Load both packages

> library(RWeka)
> library(party)
> str(iris)

'data.frame':   150 obs. of  5 variables:

> m1 <- J48(Species~., data = iris)
> if(require("party", quietly = TRUE)) plot(m1)

> library(FSelector)
> information.gain(Species~., data = iris)
> subset1.iris <- subset(iris, Petal.Width > 0.6)
> information.gain(Species~., data = subset1.iris)
> subset2.iris <- subset(subset1.iris, Petal.Width <= 1.7)
> information.gain(Species~., data = subset2.iris)
> summary(m1)

#########################################################################
#
#   Weka_classifier_trees
#   R/Weka Classifier Trees
#   https://www.rdocumentation.org/packages/RWeka/versions/0.1-0/topics/Weka_classifier_trees
#
#########################################################################

J48(formula, data, subset, na.action, control = NULL)
LMT(formula, data, subset, na.action, control = NULL)
M5P(formula, data, subset, na.action, control = NULL)


Examples
data(iris)
m1 <- J48(Species ~ ., iris)
m1
table(iris$Species, predict(m1))

## Using some Weka data sets ...

## J48
DF2 <- read.arff(system.file("arff", "contact-lenses.arff",
                             package = "RWeka"))
m2 <- J48(`contact-lenses` ~ ., data = DF2)
m2
table(DF2$`contact-lenses`, predict(m2))

## M5P
DF3 <- read.arff(system.file("arff", "cpu.arff",
                             package = "RWeka"))
m3 <- M5P(class ~ ., data = DF3)
m3

## Logistic Model Tree.
DF4 <- read.arff(system.file("arff", "weather.arff",
                             package = "RWeka"))
m4 <- LMT(play ~ ., data = DF4)
m4
table(DF4$play, predict(m4))

---------------------------------------------------------------
################################################################
#
#   R talks to Weka about Data Mining
#   https://www.r-bloggers.com/r-talks-to-weka-about-data-mining/amp/
#
################################################################


iris_j48 <- J48(Species ~ ., data = iris)
iris_j48
summary(iris_j48)
plot(iris_j48)

# Evaluation in Weka

eval_j48 <- evaluate_Weka_classifier(iris_j48, numFolds = 10, complexity = FALSE,
    seed = 1, class = TRUE)
eval_j48

# Using Weka-controls

WOW("J48")

j48_control <- J48(Species ~ ., data = iris, control = Weka_control(M = 10))
j48_control

##############################################################
#
#   Non-Linear Classification in R with Decision Trees
#   https://machinelearningmastery.com/non-linear-classification-in-r-with-decision-trees/
#
#########################################################################

CART Method in R /////////////////////////

# load the package
library(rpart)
# load data
data(iris)
# fit model
fit <- rpart(Species~., data=iris)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
table(predictions, iris$Species)


C4.5 - J48 in Weka /////////////////////////////////


# load the package
library(RWeka)
# load data
data(iris)
# fit model
fit <- J48(Species~., data=iris)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)


PART method in RR /////////////////////

# load the package
library(RWeka)
# load data
data(iris)
# fit model
fit <- PART(Species~., data=iris)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)

Bagging CART /////////////////////////

# load the package
library(ipred)
# load data
data(iris)
# fit model
fit <- bagging(Species~., data=iris)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, iris[,1:4], type="class")
# summarize accuracy
table(predictions, iris$Species)


Random Forest in RR  ////////////////////////

# load the package
library(randomForest)
# load data
data(iris)
# fit model
fit <- randomForest(Species~., data=iris)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, iris[,1:4])
# summarize accuracy
table(predictions, iris$Species)



Gradient Boosted Machine ///////////////////////7

# load the package
library(gbm)
# load data
data(iris)
# fit model
fit <- gbm(Species~., data=iris, distribution="multinomial")
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris)
# summarize accuracy
table(predictions, iris$Species)


Boosted C5.0 ///////////////////////////////

# load the package
library(C50)
# load data
data(iris)
# fit model
fit <- C5.0(Species~., data=iris, trials=10)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, iris)
# summarize accuracy
table(predictions, iris$Species)


########################################################
#
#   A ten-minute introduction to sequence-to-sequence learning in Keras
#   https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
#
#########################################################


from keras.models import Model
from keras.layers import Input, LSTM, Dense

# Define an input sequence and process it.
encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(latent_dim, return_sequences=True)
decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
We train our model in two lines, while monitoring the loss on a held-out set of 20% of the samples.

# Run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)




Here's our inference setup:

encoder_model = Model(encoder_inputs, encoder_states)

decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs = decoder_lstm(decoder_inputs,
                               initial_state=decoder_states)
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model(
    [decoder_inputs] + decoder_states,
    decoder_outputs)
We use it to implement the inference loop described above:

def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index['\t']] = 1.

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if (sampled_char == '\n' or
           len(decoded_sentence) > max_decoder_seq_length):
            stop_condition = True

        # Add the sampled character to the sequence
        char_vector = np.zeros((1, 1, num_decoder_tokens))
        char_vector[0, 0, sampled_token_index] = 1.

        target_seq = np.concatenate([target_seq, char_vector], axis=1)

    return decoded_sentence


------------------


What if I want to use a GRU layer instead of a LSTM?

It's actually a bit simpler, because GRU has only one state, whereas LSTM has two states. Here's how to adapt the training model to use a GRU layer:

encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = GRU(latent_dim, return_state=True)
encoder_outputs, state_h = encoder(encoder_inputs)

decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_gru = GRU(latent_dim, return_sequences=True)
decoder_outputs = decoder_gru(decoder_inputs, initial_state=state_h)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
What if I want to use a word-level model with integer sequences?

What if your inputs are integer sequences (e.g. representing sequences of words, encoded by their index in a dictionary)? You can embed these integer tokens via an Embedding layer. Here's how:

# Define an input sequence and process it.
encoder_inputs = Input(shape=(None,))
x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)
x, state_h, state_c = LSTM(latent_dim,
                           return_state=True)(x)
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None,))
x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)
x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)

# Define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile & run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
# Note that `decoder_target_data` needs to be one-hot encoded,
# rather than sequences of integers like `decoder_input_data`!
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)


-------------


from keras.layers import Lambda
from keras import backend as K

# The first part is unchanged
encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
states = [state_h, state_c]

# Set up the decoder, which will only process one timestep at a time.
decoder_inputs = Input(shape=(1, num_decoder_tokens))
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')

all_outputs = []
inputs = decoder_inputs
for _ in range(max_decoder_seq_length):
    # Run the decoder on one timestep
    outputs, state_h, state_c = decoder_lstm(inputs,
                                             initial_state=states)
    outputs = decoder_dense(outputs)
    # Store the current prediction (we will concatenate all predictions later)
    all_outputs.append(outputs)
    # Reinject the outputs as inputs for the next loop iteration
    # as well as update the states
    inputs = outputs
    states = [state_h, state_c]

# Concatenate all predictions
decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)

# Define and compile model as previously
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

# Prepare decoder input data that just contains the start character
# Note that we could have made it a constant hard-coded in the model
decoder_input_data = np.zeros((num_samples, 1, num_decoder_tokens))
decoder_input_data[:, 0, target_token_index['\t']] = 1.

# Train model as previously
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)





########################################################
#
#   Using Weka via Jepp
#
########################################################

Accessing Weka classes within Jepp
Java classes are imported in one's Python script as follows:
 from <package> import <class>
E.g., importing J48 looks like this:
 from weka.classifiers.trees import J48
In the following a little example script for loading a dataset, cross-validating J48 with it and outputting the results of the cross-validation in the console:
 # import classes
 from weka.core import Instances
 from weka.classifiers import Evaluation
 from weka.classifiers.trees import J48

 from java.io import BufferedReader
 from java.io import FileReader
 from java.util import Random

 # load data
 reader = BufferedReader(FileReader('/some/where/file.arff'))
 data   = Instances(reader)
 data.setClassIndex(data.numAttributes() - 1)
 reader.close()

 # train classifier
 j48  = J48()
 eval = Evaluation(data)
 rand = Random(1)
 eval.crossValidateModel(j48, data, 10, rand)

 # output summary
 print eval.toSummaryString()
The script can be started like this (you will have to adjust the paths for the jars and the Python script):
 java -classpath jep.jar:weka.jar some_script.py

###############################################################
#
#   Decision trees in python with scikit-learn and pandas
#
###############################################################

from __future__ import print_function

import os
import subprocess

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, export_graphviz


def get_iris_data():
    """Get the iris data, from local csv or pandas repo."""
    if os.path.exists("iris.csv"):
        print("-- iris.csv found locally")
        df = pd.read_csv("iris.csv", index_col=0)
    else:
        print("-- trying to download from github")
        fn = "https://raw.githubusercontent.com/pydata/pandas/" + \
             "master/pandas/tests/data/iris.csv"
        try:
            df = pd.read_csv(fn)
        except:
            exit("-- Unable to download iris.csv")

        with open("iris.csv", 'w') as f:
            print("-- writing to local iris.csv file")
            df.to_csv(f)

    return df


df = get_iris_data()
-- iris.csv found locally

print("* df.head()", df.head(), sep="\n", end="\n\n")
print("* df.tail()", df.tail(), sep="\n", end="\n\n")

* df.head()
* df.tail()

print("* iris types:", df["Name"].unique(), sep="\n")

* iris types:
['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']

def encode_target(df, target_column):
    """Add column to df with integers for the target.

    Args
    ----
    df -- pandas DataFrame.
    target_column -- column to map to int, producing
                     new Target column.

    Returns
    -------
    df_mod -- modified DataFrame.
    targets -- list of target names.
    """
    df_mod = df.copy()
    targets = df_mod[target_column].unique()
    map_to_int = {name: n for n, name in enumerate(targets)}
    df_mod["Target"] = df_mod[target_column].replace(map_to_int)

    return (df_mod, targets)


df2, targets = encode_target(df, "Name")
print("* df2.head()", df2[["Target", "Name"]].head(),  sep="\n", end="\n\n")
print("* df2.tail()", df2[["Target", "Name"]].tail(),  sep="\n", end="\n\n")
print("* targets", targets, sep="\n", end="\n\n")
* df2.head()
* df2.tail()

* targets
['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']

features = list(df2.columns[:4])
print("* features:", features, sep="\n")

* features:
['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']

# fitting the decision tree with scikit-learn

y = df2["Target"]
X = df2[features]
dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)
dt.fit(X, y)

# visualizing the tree

def visualize_tree(tree, feature_names):
    """Create tree png using graphviz.

    Args
    ----
    tree -- scikit-learn DecsisionTree.
    feature_names -- list of feature names.
    """
    with open("dt.dot", 'w') as f:
        export_graphviz(tree, out_file=f,
                        feature_names=feature_names)

    command = ["dot", "-Tpng", "dt.dot", "-o", "dt.png"]
    try:
        subprocess.check_call(command)
    except:
        exit("Could not run dot, ie graphviz, to "
             "produce visualization")

visualize_tree(dt, features)


##############################################################
#
#   python-weka-wrapper 0.3.11 documentation »
#   http://fracpete.github.io/python-weka-wrapper/examples.html
#
##############################################################

#Start up JVM
import weka.core.jvm as jvm
jvm.start()

help(jvm.start)
help(jvm.stop)

#Location of the datasets
data_dir = "/my/datasets/"

#Load dataset and print it
from weka.core.converters import Loader
loader = Loader(classname="weka.core.converters.ArffLoader")
data = loader.load_file(data_dir + "iris.arff")
data.class_is_last()
print(data)

import weka.core.converters as converters
data = converters.load_any_file(data_dir + "iris.arff")
data.class_is_last()
print(data)

#Output help from underlying OptionHandler
from weka.classifiers import Classifier
cls = Classifier(classname="weka.classifiers.trees.J48")
print(cls.to_help())

#Option handling
from weka.classifiers import Classifier
cls = Classifier(classname="weka.classifiers.trees.J48")
cls.options = ["-C", "0.3"]

from weka.classifiers import Classifier
cls = Classifier(classname="weka.classifiers.trees.J48", options=["-C", "0.3"])

from weka.classifiers import Classifier
cls = Classifier(classname="weka.classifiers.trees.J48", options=["-C", "0.3"])
print(cls.options)

#Build classifier on dataset, output predictions
from weka.classifiers import Classifier
cls = Classifier(classname="weka.classifiers.trees.J48", options=["-C", "0.3"])
cls.build_classifier(data)

for index, inst in enumerate(data):
    pred = cls.classify_instance(inst)
    dist = cls.distribution_for_instance(inst)
    print(str(index+1) + ": label index=" + str(pred) + ", class distribution=" + str(dist))


#Build classifier on dataset, print model and draw graph
from weka.classifiers import Classifier
cls = Classifier(classname="weka.classifiers.trees.J48", options=["-C", "0.3"])
cls.build_classifier(data)

print(cls)

import weka.plot.graph as graph  # NB: pygraphviz and PIL are required
graph.plot_dot_graph(cls.graph)

#Build classifier incrementally with data and print model
loader = Loader(classname="weka.core.converters.ArffLoader")
iris_inc = loader.load_file(data_dir + "iris.arff", incremental=True)
iris_inc.class_is_last()

print(iris_inc)

cls = Classifier(classname="weka.classifiers.bayes.NaiveBayesUpdateable")
cls.build_classifier(iris_inc)
for inst in loader:
    cls.update_classifier(inst)

print(cls)

#Cross-validate filtered classifier and print evaluation and display ROC
data = loader.load_file(data_dir + "diabetes.arff")
data.class_is_last()

from weka.filters import Filter
remove = Filter(classname="weka.filters.unsupervised.attribute.Remove", options=["-R", "1-3"])

cls = Classifier(classname="weka.classifiers.bayes.NaiveBayes")

from weka.classifiers import FilteredClassifier
fc = FilteredClassifier()
fc.filter = remove
fc.classifier = cls

from weka.classifiers import Evaluation
from weka.core.classes import Random
evl = Evaluation(data)
evl.crossvalidate_model(fc, data, 10, Random(1))

print(evl.percent_correct)
print(evl.summary())
print(evl.class_details())

import weka.plot.classifiers as plcls  # NB: matplotlib is required
plcls.plot_roc(evl, class_index=[0, 1], wait=True)

#Cross-validate regressor, display classifier errors and predictions
from weka.classifiers import PredictionOutput, KernelClassifier, Kernel
data = loader.load_file(data_dir + "bolts.arff")
data.class_is_last()

cls = KernelClassifier(classname="weka.classifiers.functions.SMOreg", options=["-N", "0"])
kernel = Kernel(classname="weka.classifiers.functions.supportVector.RBFKernel", options=["-G", "0.1"])
cls.kernel = kernel
pout = PredictionOutput(classname="weka.classifiers.evaluation.output.prediction.PlainText")
evl = Evaluation(data)
evl.crossvalidate_model(cls, data, 10, Random(1), pout)

print(evl.summary())
print(pout.buffer_content())

import weka.plot.classifiers as plcls  # NB: matplotlib is required
plcls.plot_classifier_errors(evl.predictions, wait=True)

#Parameter optimization - GridSearch
from weka.classifiers import GridSearch
grid = GridSearch(options=["-sample-size", "100.0", "-traversal", "ROW-WISE", "-num-slots", "1", "-S", "1"])
grid.evaluation = "CC"
grid.y = {"property": "kernel.gamma", "min": -3.0, "max": 3.0, "step": 1.0, "base": 10.0, "expression": "pow(BASE,I)"}
grid.x = {"property": "C", "min": -3.0, "max": 3.0, "step": 1.0, "base": 10.0, "expression": "pow(BASE,I)"}
cls = Classifier(
    classname="weka.classifiers.functions.SMOreg",
    options=["-K", "weka.classifiers.functions.supportVector.RBFKernel"])
grid.classifier = cls
grid.build_classifier(train)
print("Model:\n" + str(grid))
print("\nBest setup:\n" + grid.best.to_commandline())

#Parameter optimization - MultiSearch
from weka.core.classes import ListParameter, MathParameter
multi = MultiSearch(
    options=["-sample-size", "100.0", "-initial-folds", "2", "-subsequent-folds", "2",
             "-num-slots", "1", "-S", "1"])
multi.evaluation = "CC"
mparam = MathParameter()
mparam.prop = "classifier.kernel.gamma"
mparam.minimum = -3.0
mparam.maximum = 3.0
mparam.step = 1.0
mparam.base = 10.0
mparam.expression = "pow(BASE,I)"
lparam = ListParameter()
lparam.prop = "classifier.C"
lparam.values = ["-2.0", "-1.0", "0.0", "1.0", "2.0"]
multi.parameters = [mparam, lparam]
cls = Classifier(
    classname="weka.classifiers.functions.SMOreg",
    options=["-K", "weka.classifiers.functions.supportVector.RBFKernel"])
multi.classifier = cls
multi.build_classifier(train)
print("Model:\n" + str(multi))
print("\nBest setup:\n" + multi.best.to_commandline())



#Experiments
datasets = [
    data_dir + "iris.arff",
    data_dir + "vote.arff",
    data_dir + "anneal.arff"
]
classifiers = [
    Classifier(classname="weka.classifiers.rules.ZeroR"),
    Classifier(classname="weka.classifiers.trees.J48"),
    Classifier(classname="weka.classifiers.trees.REPTree"),
]
result = "exp.arff"
from weka.experiments import SimpleCrossValidationExperiment
exp = SimpleCrossValidationExperiment(
    classification=True,
    runs=10,
    folds=10,
    datasets=datasets,
    classifiers=classifiers,
    result=result)
exp.setup()
exp.run()

import weka.core.converters
loader = weka.core.converters.loader_for_file(result)
data = loader.load_file(result)
from weka.experiments import Tester, ResultMatrix
matrix = ResultMatrix(classname="weka.experiment.ResultMatrixPlainText")
tester = Tester(classname="weka.experiment.PairedCorrectedTTester")
tester.resultmatrix = matrix
comparison_col = data.attribute_by_name("Percent_correct").index
tester.instances = data

print(tester.header(comparison_col))
print(tester.multi_resultset_full(0, comparison_col))
print(tester.multi_resultset_full(1, comparison_col))


#Clustering
data = loader.load_file(data_dir + "vote.arff")
data.delete_last_attribute()

from weka.clusterers import Clusterer
clusterer = Clusterer(classname="weka.clusterers.SimpleKMeans", options=["-N", "3"])
clusterer.build_clusterer(data)

print(clusterer)

# cluster the data
for inst in data:
    cl = clusterer.cluster_instance(inst)  # 0-based cluster index
    dist = clusterer.distribution_for_instance(inst)   # cluster membership distribution
    print("cluster=" + str(cl) + ", distribution=" + str(dist))



#Associations
data = loader.load_file(data_dir + "vote.arff")
data.class_is_last()

from weka.associations import Associator
associator = Associator(classname="weka.associations.Apriori", options=["-N", "9", "-I"])
associator.build_associations(data)

print(associator)


#Attribute selection
data = loader.load_file(data_dir + "vote.arff")
data.class_is_last()

from weka.attribute_selection import ASSearch, ASEvaluation, AttributeSelection
search = ASSearch(classname="weka.attributeSelection.BestFirst", options=["-D", "1", "-N", "5"])
evaluator = ASEvaluation(classname="weka.attributeSelection.CfsSubsetEval", options=["-P", "1", "-E", "1"])
attsel = AttributeSelection()
attsel.search(search)
attsel.evaluator(evaluator)
attsel.select_attributes(data)

print("# attributes: " + str(attsel.number_attributes_selected))
print("attributes: " + str(attsel.selected_attributes))
print("result string:\n" + attsel.results_string)


#Data generators
from weka.datagenerators import DataGenerator
generator = DataGenerator(classname="weka.datagenerators.classifiers.classification.Agrawal", options=["-B", "-P", "0.05"])
DataGenerator.make_data(generator, ["-o", data_dir + "generated.arff"])

generator = DataGenerator(classname="weka.datagenerators.classifiers.classification.Agrawal", options=["-n", "10", "-r", "agrawal"])
generator.dataset_format = generator.define_data_format()
print(generator.dataset_format)
if generator.single_mode_flag:
    for i in xrange(generator.num_examples_act):
        print(generator.generate_example())
else:
    print(generator.generate_examples())




#Filters
data = loader.load_file(data_dir + "vote.arff")

from weka.filters import Filter
remove = Filter(classname="weka.filters.unsupervised.attribute.Remove", options=["-R", "last"])
remove.inputformat(data)
filtered = remove.filter(data)
print(filtered)



#Partial classnames
cls = Classifier(classname="weka.classifiers.trees.J48", options=["-C", "0.3"])
cls = Classifier(classname=".J48", options=["-C", "0.3"])
cls = Filter(classname=".Discretize")




#Packages
import weka.core.packages as packages
items = packages.all_packages()
for item in items:
    if item.get_name() == "CLOPE":
        print(item.name + " " + item.url)

packages.install_package("CLOPE")
items = packages.installed_packages()
for item in items:
    print(item.name + " " + item.url)

packages.uninstall_package("CLOPE")
items = packages.installed_packages()
for item in items:
    print(item.name + " " + item.url)


import weka.core.packages as packages
packages.install_package("/some/where/funky-package-1.0.0.zip")

import weka.core.packages as packages
packages.install_package("http://some.server.com/funky-package-1.0.0.zip")

#Stop JVM
jvm.stop()


#Database access
jvm.start(class_path=["/some/where/mysql-connector-java-X.Y.Z-bin.jar"])

from weka.core.database import InstanceQuery
iquery = InstanceQuery()
iquery.db_url = "jdbc:mysql://dbserver:3306/mydb"
iquery.user = "me"
iquery.password = "verysecret"
iquery.query = "select * from lotsadata"
data = iquery.retrieve_instances()


##########################################################
#
#   python-weka-wrapper-examples
#   https://github.com/fracpete/python-weka-wrapper-examples/blob/master/src/wekaexamples/classifiers/classifiers.py
#
##########################################################


# classifiers.py
# Copyright (C) 2014-2017 Fracpete (pythonwekawrapper at gmail dot com)

import os
import traceback
import weka.core.jvm as jvm
import wekaexamples.helper as helper
from weka.core.converters import Loader
from weka.classifiers import Classifier, SingleClassifierEnhancer, MultipleClassifiersCombiner, FilteredClassifier, \
    PredictionOutput, Kernel, KernelClassifier
from weka.classifiers import Evaluation
from weka.filters import Filter
from weka.core.classes import Random, from_commandline
import weka.plot.classifiers as plot_cls
import weka.plot.graph as plot_graph
import weka.core.types as types


def main():
    """
    Just runs some example code.
    """

    # load a dataset
    iris_file = helper.get_data_dir() + os.sep + "iris.arff"
    helper.print_info("Loading dataset: " + iris_file)
    loader = Loader("weka.core.converters.ArffLoader")
    iris_data = loader.load_file(iris_file)
    iris_data.class_is_last()

    # classifier help
    helper.print_title("Creating help string")
    classifier = Classifier(classname="weka.classifiers.trees.J48")
    print(classifier.to_help())

    # partial classname
    helper.print_title("Creating classifier from partial classname")
    clsname = ".J48"
    classifier = Classifier(classname=clsname)
    print(clsname + " --> " + classifier.classname)

    # classifier from commandline
    helper.print_title("Creating SMO from command-line string")
    cmdline = 'weka.classifiers.functions.SMO -K "weka.classifiers.functions.supportVector.NormalizedPolyKernel -E 3.0"'
    classifier = from_commandline(cmdline, classname="weka.classifiers.Classifier")
    classifier.build_classifier(iris_data)
    print("input: " + cmdline)
    print("output: " + classifier.to_commandline())
    print("model:\n" + str(classifier))

    # kernel classifier
    helper.print_title("Creating SMO as KernelClassifier")
    kernel = Kernel(classname="weka.classifiers.functions.supportVector.RBFKernel", options=["-G", "0.001"])
    classifier = KernelClassifier(classname="weka.classifiers.functions.SMO", options=["-M"])
    classifier.kernel = kernel
    classifier.build_classifier(iris_data)
    print("classifier: " + classifier.to_commandline())
    print("model:\n" + str(classifier))

    # build a classifier and output model
    helper.print_title("Training J48 classifier on iris")
    classifier = Classifier(classname="weka.classifiers.trees.J48")
    # Instead of using 'options=["-C", "0.3"]' in the constructor, we can also set the "confidenceFactor"
    # property of the J48 classifier itself. However, being of type float rather than double, we need
    # to convert it to the correct type first using the double_to_float function:
    classifier.set_property("confidenceFactor", types.double_to_float(0.3))
    classifier.build_classifier(iris_data)
    print(classifier)
    print(classifier.graph)
    plot_graph.plot_dot_graph(classifier.graph)

    # evaluate model on test set
    helper.print_title("Evaluating J48 classifier on iris")
    evaluation = Evaluation(iris_data)
    evl = evaluation.test_model(classifier, iris_data)
    print(evl)
    print(evaluation.summary())

    # evaluate model on train/test split
    helper.print_title("Evaluating J48 classifier on iris (random split 66%)")
    classifier = Classifier(classname="weka.classifiers.trees.J48", options=["-C", "0.3"])
    evaluation = Evaluation(iris_data)
    evaluation.evaluate_train_test_split(classifier, iris_data, 66.0, Random(1))
    print(evaluation.summary())

    # load a dataset incrementally and build classifier incrementally
    helper.print_title("Build classifier incrementally on iris")
    helper.print_info("Loading dataset: " + iris_file)
    loader = Loader("weka.core.converters.ArffLoader")
    iris_inc = loader.load_file(iris_file, incremental=True)
    iris_inc.class_is_last()
    classifier = Classifier(classname="weka.classifiers.bayes.NaiveBayesUpdateable")
    classifier.build_classifier(iris_inc)
    for inst in loader:
        classifier.update_classifier(inst)
    print(classifier)

    # construct meta-classifiers
    helper.print_title("Meta classifiers")
    # generic FilteredClassifier instantiation
    print("generic FilteredClassifier instantiation")
    meta = SingleClassifierEnhancer(classname="weka.classifiers.meta.FilteredClassifier")
    meta.classifier = Classifier(classname="weka.classifiers.functions.LinearRegression")
    flter = Filter("weka.filters.unsupervised.attribute.Remove")
    flter.options = ["-R", "first"]
    meta.set_property("filter", flter.jobject)
    print(meta.to_commandline())
    # direct FilteredClassifier instantiation
    print("direct FilteredClassifier instantiation")
    meta = FilteredClassifier()
    meta.classifier = Classifier(classname="weka.classifiers.functions.LinearRegression")
    flter = Filter("weka.filters.unsupervised.attribute.Remove")
    flter.options = ["-R", "first"]
    meta.filter = flter
    print(meta.to_commandline())
    # generic Vote
    print("generic Vote instantiation")
    meta = MultipleClassifiersCombiner(classname="weka.classifiers.meta.Vote")
    classifiers = [
        Classifier(classname="weka.classifiers.functions.SMO"),
        Classifier(classname="weka.classifiers.trees.J48")
    ]
    meta.classifiers = classifiers
    print(meta.to_commandline())

    # cross-validate nominal classifier
    helper.print_title("Cross-validating NaiveBayes on diabetes")
    diabetes_file = helper.get_data_dir() + os.sep + "diabetes.arff"
    helper.print_info("Loading dataset: " + diabetes_file)
    loader = Loader("weka.core.converters.ArffLoader")
    diabetes_data = loader.load_file(diabetes_file)
    diabetes_data.class_is_last()
    classifier = Classifier(classname="weka.classifiers.bayes.NaiveBayes")
    pred_output = PredictionOutput(
        classname="weka.classifiers.evaluation.output.prediction.PlainText", options=["-distribution"])
    evaluation = Evaluation(diabetes_data)
    evaluation.crossvalidate_model(classifier, diabetes_data, 10, Random(42), output=pred_output)
    print(evaluation.summary())
    print(evaluation.class_details())
    print(evaluation.matrix())
    print("areaUnderPRC/0: " + str(evaluation.area_under_prc(0)))
    print("weightedAreaUnderPRC: " + str(evaluation.weighted_area_under_prc))
    print("areaUnderROC/1: " + str(evaluation.area_under_roc(1)))
    print("weightedAreaUnderROC: " + str(evaluation.weighted_area_under_roc))
    print("avgCost: " + str(evaluation.avg_cost))
    print("totalCost: " + str(evaluation.total_cost))
    print("confusionMatrix: " + str(evaluation.confusion_matrix))
    print("correct: " + str(evaluation.correct))
    print("pctCorrect: " + str(evaluation.percent_correct))
    print("incorrect: " + str(evaluation.incorrect))
    print("pctIncorrect: " + str(evaluation.percent_incorrect))
    print("unclassified: " + str(evaluation.unclassified))
    print("pctUnclassified: " + str(evaluation.percent_unclassified))
    print("coverageOfTestCasesByPredictedRegions: " + str(evaluation.coverage_of_test_cases_by_predicted_regions))
    print("sizeOfPredictedRegions: " + str(evaluation.size_of_predicted_regions))
    print("falseNegativeRate: " + str(evaluation.false_negative_rate(1)))
    print("weightedFalseNegativeRate: " + str(evaluation.weighted_false_negative_rate))
    print("numFalseNegatives: " + str(evaluation.num_false_negatives(1)))
    print("trueNegativeRate: " + str(evaluation.true_negative_rate(1)))
    print("weightedTrueNegativeRate: " + str(evaluation.weighted_true_negative_rate))
    print("numTrueNegatives: " + str(evaluation.num_true_negatives(1)))
    print("falsePositiveRate: " + str(evaluation.false_positive_rate(1)))
    print("weightedFalsePositiveRate: " + str(evaluation.weighted_false_positive_rate))
    print("numFalsePositives: " + str(evaluation.num_false_positives(1)))
    print("truePositiveRate: " + str(evaluation.true_positive_rate(1)))
    print("weightedTruePositiveRate: " + str(evaluation.weighted_true_positive_rate))
    print("numTruePositives: " + str(evaluation.num_true_positives(1)))
    print("fMeasure: " + str(evaluation.f_measure(1)))
    print("weightedFMeasure: " + str(evaluation.weighted_f_measure))
    print("unweightedMacroFmeasure: " + str(evaluation.unweighted_macro_f_measure))
    print("unweightedMicroFmeasure: " + str(evaluation.unweighted_micro_f_measure))
    print("precision: " + str(evaluation.precision(1)))
    print("weightedPrecision: " + str(evaluation.weighted_precision))
    print("recall: " + str(evaluation.recall(1)))
    print("weightedRecall: " + str(evaluation.weighted_recall))
    print("kappa: " + str(evaluation.kappa))
    print("KBInformation: " + str(evaluation.kb_information))
    print("KBMeanInformation: " + str(evaluation.kb_mean_information))
    print("KBRelativeInformation: " + str(evaluation.kb_relative_information))
    print("SFEntropyGain: " + str(evaluation.sf_entropy_gain))
    print("SFMeanEntropyGain: " + str(evaluation.sf_mean_entropy_gain))
    print("SFMeanPriorEntropy: " + str(evaluation.sf_mean_prior_entropy))
    print("SFMeanSchemeEntropy: " + str(evaluation.sf_mean_scheme_entropy))
    print("matthewsCorrelationCoefficient: " + str(evaluation.matthews_correlation_coefficient(1)))
    print("weightedMatthewsCorrelation: " + str(evaluation.weighted_matthews_correlation))
    print("class priors: " + str(evaluation.class_priors))
    print("numInstances: " + str(evaluation.num_instances))
    print("meanAbsoluteError: " + str(evaluation.mean_absolute_error))
    print("meanPriorAbsoluteError: " + str(evaluation.mean_prior_absolute_error))
    print("relativeAbsoluteError: " + str(evaluation.relative_absolute_error))
    print("rootMeanSquaredError: " + str(evaluation.root_mean_squared_error))
    print("rootMeanPriorSquaredError: " + str(evaluation.root_mean_prior_squared_error))
    print("rootRelativeSquaredError: " + str(evaluation.root_relative_squared_error))
    print("prediction output:\n" + str(pred_output))
    plot_cls.plot_roc(
        evaluation, title="ROC diabetes",
        class_index=range(0, diabetes_data.class_attribute.num_values), wait=False)
    plot_cls.plot_prc(
        evaluation, title="PRC diabetes",
        class_index=range(0, diabetes_data.class_attribute.num_values), wait=False)

    # load a numeric dataset
    bolts_file = helper.get_data_dir() + os.sep + "bolts.arff"
    helper.print_info("Loading dataset: " + bolts_file)
    loader = Loader("weka.core.converters.ArffLoader")
    bolts_data = loader.load_file(bolts_file)
    bolts_data.class_is_last()

    # build a classifier and output model
    helper.print_title("Training LinearRegression on bolts")
    classifier = Classifier(classname="weka.classifiers.functions.LinearRegression", options=["-S", "1", "-C"])
    classifier.build_classifier(bolts_data)
    print(classifier)

    # cross-validate numeric classifier
    helper.print_title("Cross-validating LinearRegression on bolts")
    classifier = Classifier(classname="weka.classifiers.functions.LinearRegression", options=["-S", "1", "-C"])
    evaluation = Evaluation(bolts_data)
    evaluation.crossvalidate_model(classifier, bolts_data, 10, Random(42))
    print(evaluation.summary())
    print("correlationCoefficient: " + str(evaluation.correlation_coefficient))
    print("errorRate: " + str(evaluation.error_rate))
    helper.print_title("Header - bolts")
    print(str(evaluation.header))
    helper.print_title("Predictions on bolts")
    for index, pred in enumerate(evaluation.predictions):
        print(str(index+1) + ": " + str(pred) + " -> error=" + str(pred.error))
    plot_cls.plot_classifier_errors(evaluation.predictions, wait=False)

    # learning curve
    cls = [
        Classifier(classname="weka.classifiers.trees.J48"),
        Classifier(classname="weka.classifiers.bayes.NaiveBayesUpdateable")]
    plot_cls.plot_learning_curve(
        cls, diabetes_data, increments=0.05, label_template="[#] !", metric="percent_correct", wait=True)

    # access classifier's Java API
    labor_file = helper.get_data_dir() + os.sep + "labor.arff"
    helper.print_info("Loading dataset: " + labor_file)
    loader = Loader("weka.core.converters.ArffLoader")
    labor_data = loader.load_file(labor_file)
    labor_data.class_is_last()

    helper.print_title("Using JRip's Java API to access rules")
    jrip = Classifier(classname="weka.classifiers.rules.JRip")
    jrip.build_classifier(labor_data)
    rset = jrip.jwrapper.getRuleset()
    for i in xrange(rset.size()):
        r = rset.get(i)
        print(str(r.toString(labor_data.class_attribute.jobject)))


if __name__ == "__main__":
    try:
        jvm.start()
        main()
    except Exception, e:
        print(traceback.format_exc())
    finally:
        jvm.stop()



###############################################################
#
#   Decision Trees
#   http://scikit-learn.org/stable/modules/tree.html
#
###############################################################


>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)
#After being fitted, the model can then be used to predict the class of samples:
>>>
>>> clf.predict([[2., 2.]])
array([1])
#Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:
>>>
>>> clf.predict_proba([[2., 2.]])
array([[ 0.,  1.]])
#DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, …, K-1]) classification.
#Using the Iris dataset, we can construct a tree as follows:
>>>
>>> from sklearn.datasets import load_iris
>>> from sklearn import tree
>>> iris = load_iris()
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(iris.data, iris.target)
#Once trained, we can export the tree in Graphviz format using the export_graphviz exporter. If you use the conda package manager, the graphviz binaries and the python package can be installed with
#conda install python-graphviz
#Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with pip install graphviz.
#Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file iris.pdf:
>>>
>>> import graphviz
>>> dot_data = tree.export_graphviz(clf, out_file=None)
>>> graph = graphviz.Source(dot_data)
>>> graph.render("iris")
#The export_graphviz exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:
>>>
>>> dot_data = tree.export_graphviz(clf, out_file=None,
                         feature_names=iris.feature_names,
                         class_names=iris.target_names,
                         filled=True, rounded=True,
                         special_characters=True)
>>> graph = graphviz.Source(dot_data)
>>> graph



#After being fitted, the model can then be used to predict the class of samples:
>>>
>>> clf.predict(iris.data[:1, :])
array([0])
#Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:
>>>
>>> clf.predict_proba(iris.data[:1, :])
array([[ 1.,  0.,  0.]])


###############################################################
#
#   Use WEKA in your Python code
#   http://dimitri-christodoulou.blogspot.de/2012/03/use-weka-in-your-python-code.html?m=1
#
###############################################################


# Initialize the specified JVM
from jpype import *options = [
"-Xmx4G",
"-Djava.class.path=./moa.jar",
"-Djava.class.path=./weka.jar",
"-Djavaagent:sizeofag.jar",
]
startJVM(getDefaultJVMPath(), *options)

# Import java/weka packages and classes
Trees = JPackage("weka.classifiers.trees")
Filter = JClass("weka.filters.Filter")
Attribute = JPackage("weka.filters.unsupervised.attribute")
Instance = JPackage("weka.filters.unsupervised.instance")
RemovePercentage = JClass("weka.filters.unsupervised.instance.RemovePercentage")
Remove = JClass("weka.filters.unsupervised.attribute.Remove")
Classifier = JClass("weka.classifiers.Classifier")
NaiveBayes = JClass("weka.classifiers.bayes.NaiveBayes")
Evaluation = JClass("weka.classifiers.Evaluation")
FilteredClassifier = JClass("weka.classifiers.meta.FilteredClassifier")
Instances = JClass("weka.core.Instances")
BufferedReader = JClass("java.io.BufferedReader")
FileReader = JClass("java.io.FileReader")
Random = JClass("java.util.Random")


#Reading from an ARFF file
reader = BufferedReader(FileReader("./iris.arff"))
data = Instances(reader)
reader.close()
data.setClassIndex(data.numAttributes() - 1) # setting class attribute

# Standardizes all numeric attributes in the given dataset to have zero mean and unit variance, apart from the class attribute.
standardizeFilter = Attribute.Standardize()
standardizeFilter.setInputFormat(data)
data = Filter.useFilter(data, standardizeFilter)

# Randomly shuffles the order of instances passed through it.
randomizeFilter = Instance.Randomize()
randomizeFilter.setInputFormat(data)
data = Filter.useFilter(data, randomizeFilter)

# Creating train set
removeFilter = RemovePercentage()
removeFilter.setInputFormat(data)
removeFilter.setPercentage(30.0)
removeFilter.setInvertSelection(False)
trainData = Filter.useFilter(data, removeFilter)

# Creating test set
removeFilter.setInputFormat(data)
removeFilter.setPercentage(30.0)
removeFilter.setInvertSelection(True)
testData = Filter.useFilter(data, removeFilter)

# Create classifier
j48 = Trees.J48()
j48.setUnpruned(True) # using an unpruned J48
j48.buildClassifier(trainData)

print "Number Training Data", trainData.numInstances(), data.numInstances()
print "Number Test Data", testData.numInstances()

# Test classifier
for i in range(testData.numInstances()):
    pred = j48.classifyInstance(testData.instance(i))
    print "ID:", testData.instance(i).value(0),
    print "actual:", testData.classAttribute().value(int(testData.instance(i).classValue())),
    print "predicted:", testData.classAttribute().value(int(pred))

shutdownJVM()


###############################################################
#
#   https://algorithmia.com/algorithms/weka/J48
#
###############################################################

{
"trainUrl":"data://weka/WekaSampleCollection/train.arff",
"cv":5,
"options":"",
"mode":"train",
"modelUrl":"data://.algo/temp/model.txt"
}

curl -X POST -d '{
"trainUrl":"data://weka/WekaSampleCollection/train.arff",
"cv":5,
"options":"",
"mode":"train",
"modelUrl":"data://.algo/temp/model.txt"
}' -H 'Content-Type: application/json' -H 'Authorization: Simple YOUR_API_KEY' https://api.algorithmia.com/v1/algo/weka/J48/0.1.1



###############################################################
#
#   Intro to Machine Learning & NLP with Python and Weka
#   https://www.codementor.io/python/tutorial/get-started-machine-learning-nlp-weka
#
###############################################################

def numwords(emailtext):
  splittext = emailtext.split(" ")
  return len(splittext)


import  os, re
import math, glob
import features
import inspect


def main():


   arff = open("spam.arff", "w")

   ben_functions = inspect.getmembers(features, inspect.isfunction)
   feature_funcitons = []
   feature_funcitons +=  list([f[1] for f in ben_functions])

   RELATION_NAME = "spam"
   arff.write("@RELATION " + RELATION_NAME + "\n")
   for feature in feature_funcitons:
      arff.write("@ATTRIBUTE " +\
            str(feature.__name__) + " REAL\n")  #change this if we
                              #have non-real number
                              #values

    ###PREDEFINED USER FEATURES#######
   arff.write("@ATTRIBUTE SPAM {True, False}\n")

   arff.write("@DATA\n")

   spam_directory = "is-spam"
   not_spam = "not-spam"
   os.chdir(spam_directory)
   for email in glob.glob("*"):#ITERATE THROUGH ALL DATA HERE
       extract_features(open(email).read(), feature_funcitons, arff, True)

   os.chdir('../'+not_spam)
   for email in glob.glob("*"):#ITERATE THROUGH ALL DATA HERE
       extract_features(open(email).read(), feature_funcitons, arff, False)



def numwords(emailtext):
    splittext = emailtext.split(" ")
    return len(splittext)



def extract_features(data, feature_funcitons, arff, spam):
    values = []
    buff = ""

    for feature in feature_funcitons:
    value = feature(data)
    values.append(value)
    if spam:
      buff += (",".join([str(x) for x in values]) + ', True' + "\n")
    else:
      buff += (",".join([str(x) for x in values]) + ', False' + "\n")

    arff.write(buff)

if __name__ == "__main__":
  main()


def has_html(emailtext):
  return 1 if "html" in emailtext.lower() else 0

#python feature_extract.py

def num_links(emailtext):
  return emailtext.count('http')

def spammy_words(emailtext):
  spam_words = ['join', 'free', 'buy', 'start', 'click', 'discount']
  splittext = emailtext.split(" ")
  total = 0
  for word in spam_words:
    total += splittext.count(word) #appends the count of each of these words to total
  return total




import os, glob
spamwords = {}
notspamwords = {}
spam_directory = "is-spam"
not_spam = "not-spam"
os.chdir(spam_directory)
for email in glob.glob("*"):#ITERATE THROUGH ALL DATA HERE
   text = open(email).read().lower().split(‘ ') #gets all the text of our emails and normalizes them to lower case
   for word in text:
    if word in spamwords:
      spamwords[word] += 1
    else:
      spamwords[word] = 1
print spamwords



keys = sorted(spamwords.keys(), key=lambda x: spamwords[x], reversed=True`)
for word in keys:
  print word, spamwords[word]



def spammy_words(emailtext):
  spam_words = ['helvetica', 'new', 'money', 'e-mail', 'receive', 'business']
  splittext = emailtext.split(" ")
  total = 0
  for word in spam_words:
    total += splittext.count(word) #appends the count of each of these words to total
  return total