http://pythonfiddle.com/
http://rextester.com/l/python_online_compiler
http://www.compileonline.com/execute_python_online.php
https://ideone.com/2mkVbD
https://repl.it/repls/AgreeableAntiqueMouse
https://repl.it/repls/ExperiencedInsidiousSection
https://www.jdoodle.com/python3-programming-online
https://www.onlinegdb.com/online_python_compiler
https://www.onlinegdb.com/online_python_interpreter
https://www.python.org/shell/
https://www.pythonanywhere.com/try-ipython/
https://www.tutorialspoint.com/execute_python_online.php
https://quintagroup.com/cms/python/online-interpreter
https://www.pythonanywhere.com/try-ipython/
------------------------------------------------

nltk
https://www.pythonanywhere.com/try-ipython/
https://repl.it/repls/YellowGrowingComputing
------------------------------------------------
http://www.nltk.org/api/nltk.html?highlight=freqdist
https://www.nltk.org/book/ch01.html
https://www.strehle.de/tim/weblog/archives/2015/09/03/1569
https://pythonspot.com/nltk-stop-words/
------------------------------------------------


##################################################################################################

Training and Testing the Naive Bayes Classifier
https://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/

##################################################################################################

import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews

def word_feats(words):
    return dict([(word, True) for word in words])

negids = movie_reviews.fileids('neg')
posids = movie_reviews.fileids('pos')

negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]

negcutoff = len(negfeats)*3/4
poscutoff = len(posfeats)*3/4

trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]
print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))

classifier = NaiveBayesClassifier.train(trainfeats)
print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
classifier.show_most_informative_features()



##################################################################################################

TEXT CLASSIFICATION FOR SENTIMENT ANALYSIS â€“ ELIMINATE LOW INFORMATION FEATURES
https://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/

##################################################################################################

import collections, itertools
import nltk.classify.util, nltk.metrics
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews, stopwords
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
from nltk.probability import FreqDist, ConditionalFreqDist

def evaluate_classifier(featx):
    negids = movie_reviews.fileids('neg')
    posids = movie_reviews.fileids('pos')

    negfeats = [(featx(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
    posfeats = [(featx(movie_reviews.words(fileids=[f])), 'pos') for f in posids]

    negcutoff = len(negfeats)*3/4
    poscutoff = len(posfeats)*3/4

    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]

    classifier = NaiveBayesClassifier.train(trainfeats)
    refsets = collections.defaultdict(set)
    testsets = collections.defaultdict(set)

    for i, (feats, label) in enumerate(testfeats):
            refsets[label].add(i)
            observed = classifier.classify(feats)
            testsets[observed].add(i)

    print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
    print 'pos precision:', nltk.metrics.precision(refsets['pos'], testsets['pos'])
    print 'pos recall:', nltk.metrics.recall(refsets['pos'], testsets['pos'])
    print 'neg precision:', nltk.metrics.precision(refsets['neg'], testsets['neg'])
    print 'neg recall:', nltk.metrics.recall(refsets['neg'], testsets['neg'])
    classifier.show_most_informative_features()

def word_feats(words):
    return dict([(word, True) for word in words])

print 'evaluating single word features'
evaluate_classifier(word_feats)

word_fd = FreqDist()
label_word_fd = ConditionalFreqDist()

for word in movie_reviews.words(categories=['pos']):
    word_fd.inc(word.lower())
    label_word_fd['pos'].inc(word.lower())

for word in movie_reviews.words(categories=['neg']):
    word_fd.inc(word.lower())
    label_word_fd['neg'].inc(word.lower())

# n_ii = label_word_fd[label][word]
# n_ix = word_fd[word]
# n_xi = label_word_fd[label].N()
# n_xx = label_word_fd.N()

pos_word_count = label_word_fd['pos'].N()
neg_word_count = label_word_fd['neg'].N()
total_word_count = pos_word_count + neg_word_count

word_scores = {}

for word, freq in word_fd.iteritems():
    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],
        (freq, pos_word_count), total_word_count)
    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],
        (freq, neg_word_count), total_word_count)
    word_scores[word] = pos_score + neg_score

best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]
bestwords = set([w for w, s in best])

def best_word_feats(words):
    return dict([(word, True) for word in words if word in bestwords])

print 'evaluating best word features'
evaluate_classifier(best_word_feats)

def best_bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):
    bigram_finder = BigramCollocationFinder.from_words(words)
    bigrams = bigram_finder.nbest(score_fn, n)
    d = dict([(bigram, True) for bigram in bigrams])
    d.update(best_word_feats(words))
    return d

print 'evaluating best words + bigram chi_sq word features'
evaluate_classifier(best_bigram_word_feats)




https://bitbucket.org/japerk/nltk-trainer/src
https://github.com/japerk/nltk-trainer
http://www.cs.cornell.edu/people/pabo/movie-review-data/
http://text-processing.com/demo/sentiment/
each method is throttled to 1000 calls per day per IP
http://text-processing.com/docs/sentiment.html

curl -d "text=terrible" http://text-processing.com/api/sentiment/
curl -d "text=great" http://text-processing.com/api/sentiment/
curl -d "text=hi friend" http://text-processing.com/api/sentiment/
curl -d "language=dutch&text=goed boek" http://text-processing.com/api/sentiment/













###################################################

SentimentAnalyzerSample

###################################################

https://repl.it/languages

https://www.nltk.org/book/ch05.html
https://www.kaggle.com/ngyptr/python-nltk-sentiment-analysis
https://www.programcreek.com/python/example/91264/nltk.sentiment
https://www.learndatasci.com/tutorials/sentiment-analysis-reddit-headlines-pythons-nltk/
https://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/
https://github.com/nagypeterjob/Sentiment-Analysis-NLTK-ML-LSTM/blob/master/Sentiment.ipynb
https://opensourceforu.com/2016/12/analysing-sentiments-nltk/
https://www.nltk.org/api/nltk.sentiment.html


https://github.com/Delvison/Sentiment-Analysis/blob/master/sentiment_analysis.py
https://github.com/sangeeth96/twitter-sentiment-analysis/blob/master/twitter.py
https://github.com/Jordan396/Basic_Twitter_Sentiment_Analysis/blob/master/main_NLTK.py
https://github.com/nisarg64/Sentiment-Analysis-NLTK/blob/master/imdb_sentiment.py
https://github.com/Bharat123rox/sentiment-analysis-nltk/blob/master/SentimentAnalysis/SentimentAnalyzerSample.py
https://github.com/alokkumary2j/Sentiment-Analysis-Using-Python-NLTK/blob/master/BayesSemanticAnalyzer.py
https://github.com/uditkumawat/Sentiment-Analysis-In-Python-using-nltk/blob/master/main.py
https://github.com/Kamaldeep-j/Sentiment-Analysis-Python-NLTK/blob/master/sentiment-analysis/test.py
https://github.com/PatGaston3/Sentiment-Analysis/blob/master/AnalyzeSentiment.py
https://github.com/maranemil/howto/blob/master/fullstack/python/nltk/nltk_replit2.py


###################################################




https://www.tensorflow.org/tutorials/representation/word2vec
https://nlp.stanford.edu/projects/glove/
https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html
https://scikit-learn.org/stable/modules/feature_extraction.html
https://stackabuse.com/text-classification-with-python-and-scikit-learn/
https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f
https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a
https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub
https://www.tensorflow.org/tutorials/keras/basic_text_classification
https://www.tensorflow.org/beta/tutorials/text/text_classification_rnn
https://medium.com/tensorflow/building-a-text-classification-model-with-tensorflow-hub-and-estimators-3169e7aa568
https://medium.com/tensorflow/text-classification-using-tensorflow-js-an-example-of-detecting-offensive-language-in-browser-e2b94e3565ce
https://becominghuman.ai/deep-learning-using-tensorflow-and-nltk-analyzing-corpuss-sentiments-part-1-bec9d6c1051
https://towardsdatascience.com/tagged/text-mining
https://github.com/Hvass-Labs/TensorFlow-Tutorials

from nltk.stem import WordNetLemmatizer

stemmer = WordNetLemmatizer()

for sen in range(0, len(X)):
    # Remove all the special characters
    document = re.sub(r'\W', ' ', str(X[sen]))

    # remove all single characters
    document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)

    # Remove single characters from the start
    document = re.sub(r'\^[a-zA-Z]\s+', ' ', document)

    # Substituting multiple spaces with single space
    document = re.sub(r'\s+', ' ', document, flags=re.I)

    # Removing prefixed 'b'
    document = re.sub(r'^b\s+', '', document)

    # Converting to Lowercase
    document = document.lower()

    # Lemmatization
    document = document.split()

    document = [stemmer.lemmatize(word) for word in document]
    document = ' '.join(document)

    documents.append(document)

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
X = vectorizer.fit_transform(documents).toarray()


from sklearn.feature_extraction.text import TfidfVectorizer
tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
X = tfidfconverter.fit_transform(documents).toarray()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
classifier = RandomForestClassifier(n_estimators=1000, random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

with open('text_classifier', 'wb') as picklefile:
    pickle.dump(classifier,picklefile)
with open('text_classifier', 'rb') as training_model:
    model = pickle.load(training_model)

y_pred2 = model.predict(X_test)
print(confusion_matrix(y_test, y_pred2))
print(classification_report(y_test, y_pred2))
print(accuracy_score(y_test, y_pred2))
