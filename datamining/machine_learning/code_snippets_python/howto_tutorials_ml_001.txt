
###############################################
#
#   Keras Tutorial: The Ultimate Beginner’s Guide to Deep Learning in Python
#    https://elitedatascience.com/keras-tutorial-deep-learning-in-python
#
###############################################

# 3. Import libraries and modules
import numpy as np
np.random.seed(123)  # for reproducibility

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.utils import np_utils
from keras.datasets import mnist

# 4. Load pre-shuffled MNIST data into train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# 5. Preprocess input data
X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)
X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

# 6. Preprocess class labels
Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)

# 7. Define model architecture
model = Sequential()

model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))
model.add(Convolution2D(32, 3, 3, activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

# 8. Compile model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# 9. Fit model on training data
model.fit(X_train, Y_train,
          batch_size=32, nb_epoch=10, verbose=1)

# 10. Evaluate model on test data
score = model.evaluate(X_test, Y_test, verbose=0)

---------------------------------------------------------------------------


----------------------------------------------------------

###############################################
#
#   https://github.com/Lemmah/tensorflow
#
###############################################

import numpy as np
import tensorflow as tf

# Model parameters
W = tf.Variable([.3], tf.float32)
b = tf.Variable([-.3], tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
linear_model = W * x + b
y = tf.placeholder(tf.float32)
# loss
loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
# training data
x_train = [1,2,3,4]
y_train = [0,-1,-2,-3]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x:x_train, y:y_train})

# evaluate training accuracy
curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})
print("W: %s b: %s loss: %s"%(curr_W, curr_b, curr_loss))

-----------------------------------------------------------


###############################################
#
#   Analysing Cryptocurrency Market in R
#
###############################################

library(coinmarketcapr)
plot_top_5_currencies()

market_today <- get_marketcap_ticker_all()
head(market_today[,1:8])
              id         name symbol rank price_usd  price_btc X24h_volume_usd market_cap_usd
1      bitcoin      Bitcoin    BTC    1   5568.99        1.0    2040540000.0  92700221345.0
2     ethereum     Ethereum    ETH    2   297.408  0.0537022     372802000.0  28347433482.0
3       ripple       Ripple    XRP    3  0.204698 0.00003696     100183000.0   7887328954.0
4 bitcoin-cash Bitcoin Cash    BCH    4   329.862  0.0595624     156369000.0   5512868154.0
5     litecoin     Litecoin    LTC    5    55.431   0.010009     124636000.0   2967255097.0
6         dash         Dash   DASH    6   287.488  0.0519109      46342600.0   2197137527.0

library(treemap)
df1 <- na.omit(market_today[,c('id','market_cap_usd')])
df1$market_cap_usd <- as.numeric(df1$market_cap_usd)
df1$formatted_market_cap <-  paste0(df1$id,'\n','$',format(df1$market_cap_usd,big.mark = ',',scientific = F, trim = T))
treemap(df1, index = 'formatted_market_cap', vSize = 'market_cap_usd', title = 'Cryptocurrency Market Cap', size.labels=c(12, 8), palette='RdYlGn')

---







###############################################
#
# https://github.com/biolab/orange3
# https://orange.biolab.si/download/
#
###############################################

python3 --version
sudo apt install  git build-essential python3-dev
sudo apt-get install python-pip
sudo apt-get install python3-pip
sudo pip install orange3
sudo pip3 install orange3
sudo pip install numpy
sudo apt install python3-numpy python3-pandas

###############################################
#
# http://jupyter.org/install.html
#
###############################################

python3 -m pip install --upgrade pip
python3 -m pip install jupyter

python -m pip install --upgrade pip
python -m pip install jupyter


# jupyter notebook
http://localhost:8888/notebooks/Dokumente/MLearning/
http://localhost:8888/tree/Dokumente/MLearning

sudo -H  pip install pandas

https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks











###############################################
#
# numpy pandas test
#
###############################################

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#a = np.matrix('1 2; 3 4')
#b = numpy.zeros(shape=(5,2))
#c = np.full((2, 2), 10)
d = np.random.rand(6,4)
d

#plt.plot(d)
#plt.axis([0, 2, 0, 2])
#plt.show()


n = 15
x = np.random.randn(n)
y = x * np.random.randn(n)
x

#fig, ax = plt.subplots()
#fit = np.polyfit(x, y, deg=1)
#ax.plot(x, fit[0] * x + fit[1], color='red')
#ax.scatter(x, y)
#fig.show()

# Set the data
data = pd.DataFrame ({
'length' : [94,74,147,58,86,94,63,86,69,72,128,85,82,86,88,72,74,61,90,89,68,76,114,90,78],
'weight' : [130,51,640,28,80,110,33,90,36,38,366,84,80,83,70,61,54,44,106,84,39,42,197,102,57]
})

# create another data frame of log values
data_log = np.log(data)


http://stamfordresearch.com/linear-regression-using-pandas-python/
http://matthiaseisen.com/pp/patterns/p0170/
https://matplotlib.org/users/pyplot_tutorial.html
https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html
https://matplotlib.org/api/pyplot_api.html

https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html
https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.matrix.html
https://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html#numpy.full





###################################################################################
#
#	Introducing TensorFlow.js: Machine Learning in Javascript
#	https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db
#
###################################################################################

import * as tf from ‘@tensorflow/tfjs’;
const model = tf.sequential();
model.add(tf.layers.dense({inputShape: [4], units: 100}));
model.add(tf.layers.dense({units: 4}));
model.compile({loss: ‘categoricalCrossentropy’, optimizer: ‘sgd’});

await model.fit(
  xData, yData, {
    batchSize: batchSize,
    epochs: epochs
});

# The model is now ready to use to make predictions

// Get measurements for a new flower to generate a prediction
// The first argument is the data, and the second is the shape.
const inputData = tf.tensor2d([[4.8, 3.0, 1.4, 0.1]], [1, 4]);

// Get the highest confidence prediction from our model
const result = model.predict(inputData);
const winner = irisClasses[result.argMax().dataSync()[0]];

// Display the winner
console.log(winner);

###################################################################################
#
#	Analysing 1.4 billion rows with python Using pytubes, numpy and matplotlib
#	by Steve Stagg
#	https://hackernoon.com/analysing-1-4-billion-rows-with-python-6cec86ca9d73
#
###################################################################################


import tubes

FILES = glob.glob(path.expanduser("~/src/data/ngrams/1gram/googlebooks*"))
WORD = "Python"

# Set up the data load pipeline
one_grams_tube = (tubes.Each(FILES)
    .read_files()
    .split()
    .tsv(headers=False)
    .multi(lambda row: (
        row.get(0).equals(WORD.encode('utf-8')),
        row.get(1).to(int),
        row.get(2).to(int)
    ))
)

# Load the data into a numpy array.  By setting a roughly-accurate
# estimated_rows count, pytubes optimizes the allocation pattern.
# fields=True here is redundant, but ensures that the returned ndarray
# uses fields, rather than a single multidimentional array
one_grams = one_grams_tube.ndarray(estimated_rows=500_000_000, fields=True)



last_year = 2008
YEAR_COL = '1'
COUNT_COL = '2'

year_totals, bins = np.histogram(
    one_grams[YEAR_COL],
    density=False,
    range=(0, last_year+1),
    bins=last_year + 1,
    weights=one_grams[COUNT_COL]
)

one_grams_tube = (tubes.Each(FILES)
    .read_files()
    .split()
    .tsv(headers=False)
    .skip_unless(lambda row: row.get(1).to(int).gt(1799))
    .multi(lambda row: (
        row.get(0).equals(word.encode('utf-8')),
        row.get(1).to(int),
        row.get(2).to(int)
    ))
)

# Find the matching rows (where the first column is True)
word_rows = one_grams[IS_WORD_COL]
# Create an empty array to hold the per-year % values
word_counts = np.zeros(last_year+1)
# Iterate over each matching row (for a matching word, there should just be 1,000s of rows)
for _, year, count in one_grams[word_rows]:
    # Set the relevant word_counts row to the calculated value
    word_counts[year] += (100*count) / year_totals[year]