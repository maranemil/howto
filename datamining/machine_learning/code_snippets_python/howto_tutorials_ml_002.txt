
####################################################
#
#	kdnuggets tutorials Juni 2018
#
####################################################
---------------------------------------------------
https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score as acc
from mlxtend.feature_selection import SequentialFeatureSelector as sfs

# Read data
df = pd.read_csv('winequality-white.csv', sep=';')

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    df.values[:,:-1],
    df.values[:,-1:],
    test_size=0.25,
    random_state=42)

y_train = y_train.ravel()
y_test = y_test.ravel()

print('Training dataset shape:', X_train.shape, y_train.shape)
print('Testing dataset shape:', X_test.shape, y_test.shape)

# Build RF classifier to use in feature selection
clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)

# Build step forward feature selection
sfs1 = sfs(clf,
           k_features=5,
           forward=True,
           floating=False,
           verbose=2,
           scoring='accuracy',
           cv=5)

# Perform SFFS
sfs1 = sfs1.fit(X_train, y_train)

# Which features?
feat_cols = list(sfs1.k_feature_idx_)
print(feat_cols)

# Build full model with selected features
clf = RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=4)
clf.fit(X_train[:, feat_cols], y_train)

y_train_pred = clf.predict(X_train[:, feat_cols])
print('Training accuracy on selected features: %.3f' % acc(y_train, y_train_pred))

y_test_pred = clf.predict(X_test[:, feat_cols])
print('Testing accuracy on selected features: %.3f' % acc(y_test, y_test_pred))

# Build full model on ALL features, for comparison
clf = RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=4)
clf.fit(X_train, y_train)

y_train_pred = clf.predict(X_train)
print('Training accuracy on all features: %.3f' % acc(y_train, y_train_pred))

y_test_pred = clf.predict(X_test)
print('Testing accuracy on all features: %.3f' % acc(y_test, y_test_pred))


* * * * * * * * * * * * * * * * * * * *

https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html
https://gist.github.com/williamFalcon/f75b01b47bf1be8b1e80be173ebdf39c#file-bilstm_net_design-py
https://gist.github.com/williamFalcon/b0dc6d25b39e7da0d05e5713ef0a57af#file-pytorch_lstm_forward-py
https://gist.github.com/williamFalcon/42da07d5cea5d00151f9cfde30f092b6#file-pytorch_lstm_loss-py
https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd#file-pytorch_lstm_variable_mini_batches-py
* * * * * * * * * * * * * * * * * * * *
https://www.kdnuggets.com/2018/06/generating-text-rnn-4-lines-code.html
https://github.com/minimaxir/textgenrnn
http://www.trumptwitterarchive.com/archive

from textgenrnn import textgenrnn
textgen = textgenrnn()
textgen.train_from_file('trump-tweets.txt', num_epochs=10)
textgen.generate(5)

textgen.generate(5, temperature=0.9)
textgen.generate(5, temperature=0.1)

* * * * * * * * * * * * * * * * * * * *

https://www.kdnuggets.com/2018/06/natural-language-semantic-search-arbitrary-objects-deep-learning.html
https://www.kdnuggets.com/2018/06/better-stats-101.html


# 1. Live Loss Plot

from livelossplot import PlotLossesKeras
model.fit(X_train, Y_train,
          epochs=10,
          validation_data=(X_test, Y_test),
          callbacks=[PlotLossesKeras()],
          verbose=0)


# 2. Parfit
from parfit import bestFit # Necessary if you wish to use bestFit
# Necessary if you wish to run each step sequentially
from parfit.fit import *
from parfit.score import *
from parfit.plot import *
from parfit.crossval import *
grid = {
    'min_samples_leaf': [1, 5, 10, 15, 20, 25],
    'max_features': ['sqrt', 'log2', 0.5, 0.6, 0.7],
    'n_estimators': [60],
    'n_jobs': [-1],
    'random_state': [42]
}
paramGrid = ParameterGrid(grid)
best_model, best_score, all_models, all_scores = bestFit(RandomForestClassifier(), paramGrid,
                                                    X_train, y_train, X_val, y_val, # nfolds=5 [optional, instead of validation set]
                                                    metric=roc_auc_score, greater_is_better=True,
                                                    scoreLabel='AUC')

print(best_model, best_score)

# 4. textgenrnn
from textgenrnn import textgenrnn
textgen = textgenrnn()
textgen.train_from_file('hacker-news-2000.txt', num_epochs=1)
textgen.generate()

# 5. Magnitude
from pymagnitude import *
vectors = Magnitude("/path/to/vectors.magnitude")

* * * * * * * * * * * * * * * * * * * *

https://www.kdnuggets.com/2018/06/packaging-distributing-python-project-pypi-pip.html

1. Creating a Simple Python Project.
2. How Python Locates Libraries?
3. Manual Installation by Copying Project Files to site-packages.
4. How Python Installers Locate Libraries?
5. Preparing the Package and its Files (__init__.py and setup.py).
6. Distributing the Package.
7. Uploading the Distribution Files Online to Test PyPI.
8. Installing the Distributed Package from Test PyPI.
9. Importing and Using the Installed Package.
10. Using PyPI rather than Test PyPI.

* * * * * * * * * * * * * * * * * * * *

https://www.kdnuggets.com/2018/06/command-line-tricks-data-scientists.html

ICONV
HEAD
TR
WC
SPLIT
SORT & UNIQ
CUT
PASTE
JOIN
GREP
SED
AWK

# Converting -f (from) latin1 (ISO-8859-1)
# -t (to) standard UTF_8
iconv -f ISO-8859-1 -t UTF-8 < input.txt > output.txt

# Print first 3 lines
head -n 3 filename.csv

# Converting a tab delimited file into commas
cat tab_delimited.txt | tr "\\t" "," comma_delimited.csv
cat README.md | tr "[:punct:][:space:]" "\n" | tr "[:upper:]" "[:lower:]" | grep . | sort | uniq -c | sort -nr
# Converting all upper case letters to lower case
cat filename.csv | tr '[A-Z]' '[a-z]'


Useful options:
tr -d delete characters
tr -s squeeze characters
\b backspace
\f form feed
\v vertical tab
\NNN character with octal value NNN

# Will return number of lines in CSV
wc -l gigantic_comma.csv

# We will split our CSV into new_filename every 500 line
split -l 500 filename.csv new_filename_
find . -type f -exec mv '{}' '{}'.csv \;

Useful options:
split -b split by certain byte size
split -a generate suffixes of length N
split -x split using hex suffixes

SORT & UNIQ
# Sorting a CSV file by the second column alphabetically
sort -t, -k2 filename.csv
# Numerically
sort -t, -k2n filename.csv
# Reverse order
sort -t, -k2nr filename.csv

Useful options:
sort -f ignore case
sort -r reverse sort order
sort -R scramble order
uniq -c count number of occurrences
uniq -d only print duplicate lines

CUT
# get  the first and third columns
cut -d, -f 1,3 filename.csv

# select every column other than the first.
cut -d, -f 2- filename.csv

# Print first 10 lines of column 1 and 3, where "some_string_value" is present
head filename.csv | grep "some_string_value" | cut -d, -f 1,3


Finding out the number of unique values within the second column.
cat filename.csv | cut -d, -f 2 | sort | uniq | wc -l

# Count occurences of unique values, limiting to first 10 results
cat filename.csv | cut -d, -f 2 | sort | uniq -c | head

PASTE

# Join the two into a CSV
paste -d ',' names.txt jobs.txt > person_data.txt

# Output
adam,lawyer
john,youtuber
zach,developer


JOIN
# Join the first file (-1) by the second column
# and the second file (-2) by the first
join -t, -1 2 -2 1 first_file.txt second_file.txt

# Outer join, replace blanks with NULL in columns 1 and 2
# -o which fields to substitute - 0 is key, 1.1 is first column, etc...
join -t, -1 2 -a 1 -a2 -e ' NULL' -o '0,1.1,2.2' first_file.txt second_file.txt

Useful options:
join -a print unpairable lines
join -e replace missing input fields
join -j equivalent to -1 FIELD -2 FIELD

GREP
# Recursively search and list all files in directory containing 'word'
grep -lr 'word' .

# List number of files containing word
grep -lr 'word' . | wc -l
grep -c 'some_value' filename.csv

# Same thing, but in all files in current directory by file name
grep -c 'some_value' *
grep "first_value\|second_value" filename.csv

Useful options

alias grep="grep --color=auto" make grep colorful
grep -E use extended regexps
grep -w only match whole words
grep -l print name of files with match
grep -v inverted matching

SED
sed -i '' 's/\$//g' data.txt
sed -i '' 's/\([0-9]\),\([0-9]\)/\1\2/g' data.txt
sed -i '' '/jack/d' data.txt

AWK
awk '/word/' filename.csv
awk -F, '/word/ { print $3 "\t" $4 }' filename.csv
awk -F, 'NR == 53' filename.csv
awk -F, ' $1 == "string" { print NR, $0 } ' filename.csv

# Filter based off of numerical value in second column
awk -F, ' $2 == 1000 { print NR, $0 } ' filename.csv

# Print line number and columns where column three greater
# than 2005 and column five less than one thousand
awk -F, ' $3 >= 2005 && $5 <= 1000 { print NR, $0 } ' filename.csv

# Sum the third column:
awk -F, '{ x+=$3 } END { print x }' filename.csv

# sum of the third column, for values where the first column equals “something”.
awk -F, '$1 == "something" { x+=$3 } END { print x }' filename.csv

# Get the dimensions of a file:
awk -F, 'END { print NF, NR }' filename.csv

# Prettier version
awk -F, 'BEGIN { print "COLUMNS", "ROWS" }; END { print NF, NR }' filename.csv

# Print lines appearing twice:
awk -F, '++seen[$0] == 2' filename.csv

Remove duplicate lines:
# Consecutive lines
awk 'a !~ $0; {a=$0}']
# Nonconsecutive lines
awk '! a[$0]++' filename.csv
# More efficient
awk '!($0 in a) {a[$0];print}

# Substitute multiple values using built-in function gsub().
awk '{gsub(/scarlet|ruby|puce/, "red"); print}'

This awk command will combine multiple CSV files, ignoring the header and then append it at the end.
awk 'FNR==1 && NR!=1{next;}{print}' *.csv > final_file.csv


Need to downsize a massive file? Welp, awk can handle that with help from sed. Specifically, this command breaks one big file into multiple smaller ones based on a line count. This one-liner will also add an extension.
sed '1d;$d' filename.csv | awk 'NR%NUMBER_OF_LINES==1{x="filename-"++i".csv";}{print > x}'
# Example: splitting big_data.csv into data_(n).csv every 100,000 lines
sed '1d;$d' big_data.csv | awk 'NR%100000==1{x="data_"++i".csv";}{print > x}'


* * * * * * * * * * * * * * * * * * * *

https://www.kdnuggets.com/2018/06/audience-segmentation.html

* * * * * * * * * * * * * * * * * * * *

https://www.kdnuggets.com/2018/06/keras-4-step-workflow.html

# Define the training data
import numpy as np
X_train = np.random.random((5000, 32))
y_train = np.random.random((5000, 5))
# Define the neural network model
from keras import models
from keras import layers
INPUT_DIM = X_train.shape[1]
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_dim=INPUT_DIM))
model.add(layers.Dense(5, activation='softmax'))
# Configure the learning process
from keras import optimizers
from keras import metrics
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Train the model
model.fit(X_train, y_train,
          batch_size=128,
          epochs=10)

* * * * * * * * * * * * * * * * * * * *

https://www.kdnuggets.com/2018/06/linear-regression-predictive-modeling-r.html

data(trees) ## access the data from R’s datasets package
head(trees) ## look at the first several rows of the data
str(trees) ## look at the structure of the variables
ggpairs(data=trees, columns=1:3, title="trees data")

