https://raw.githubusercontent.com/Shiza1aas/quran/master/quran/quran.txt
http://www.betania-deva.ro/bible/bible/bible.txt
https://raw.githubusercontent.com/mxw/grmr/master/src/finaltests/bible.txt

https://timothyrenner.github.io/datascience/2015/12/02/violence-in-religious-text.html
https://timothyrenner.github.io/datascience/2015/12/02/violence-in-religious-text.html

Natural Language Toolkit â€” NLTK 3.3 documentation
https://www.nltk.org/book/
https://www.nltk.org/install.html
https://pypi.org/project/nltk/
https://update.hanser-fachbuch.de/2013/09/artikelreihe-python-3-nltk-natural-language-toolkit/
https://www.nltk.org/book/ch03.html
https://www.nltk.org/book/ch04.html


http://snowball.tartarus.org/algorithms/romanian/stemmer.html
https://text-processing.com/demo/stem/
https://github.com/stopwords-iso/stopwords-ro




sudo pip install -U nltk
sudo pip install -U numpy
import nltk

import nltk, re, pprint
from nltk import word_tokenize

##################################################################################################
------------------------------------
Getting Started with NLTK
------------------------------------
##################################################################################################

>>> import nltk
>>> nltk.download()
>>> from nltk.book import *
>>> text1.concordance("monstrous")
>>> text1.similar("monstrous")
>>> text2.similar("monstrous")
>>> text2.common_contexts(["monstrous", "very"])
>>> text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])
>>> text3.generate()
>>> len(text3)
>>> sorted(set(text3))
>>> len(set(text3))
>>> len(set(text3)) / len(text3)
>>> text3.count("smote")
>>> 100 * text4.count('a') / len(text4)
>>> lexical_diversity(text3)
>>> lexical_diversity(text5)
>>> percentage(4, 5)
>>> percentage(text4.count('a'), len(text4))

>>> sent1.append("Some")
text4.index('awaken')
>>> wOrDs = sorted(noun_phrase)





>>> fdist1 = FreqDist(text1)
>>> print(fdist1)
>>> fdist1.most_common(50)
>>> fdist1['whale']





>>> V = set(text1)
>>> long_words = [w for w in V if len(w) > 15]
>>> sorted(long_words)


>>> fdist5 = FreqDist(text5)
>>> sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)



Functions Defined for NLTK's Frequency Distributions

Example 	Description
fdist = FreqDist(samples) 	create a frequency distribution containing the given samples
fdist[sample] += 1 	increment the count for this sample
fdist['monstrous'] 	count of the number of times a given sample occurred
fdist.freq('monstrous') 	frequency of a given sample
fdist.N() 	total number of samples
fdist.most_common(n) 	the n most common samples and their frequencies
for sample in fdist: 	iterate over the samples
fdist.max() 	sample with the greatest count
fdist.tabulate() 	tabulate the frequency distribution
fdist.plot() 	graphical plot of the frequency distribution
fdist.plot(cumulative=True) 	cumulative plot of the frequency distribution
fdist1 |= fdist2 	update fdist1 with counts from fdist2
fdist1 < fdist2 	test if samples in fdist1 occur less frequently than in fdist2




Some Word Comparison Operators

Function 	Meaning
s.startswith(t) 	test if s starts with t
s.endswith(t) 	test if s ends with t
t in s 	test if t is a substring of s
s.islower() 	test if s contains cased characters and all are lowercase
s.isupper() 	test if s contains cased characters and all are uppercase
s.isalpha() 	test if s is non-empty and all characters in s are alphabetic
s.isalnum() 	test if s is non-empty and all characters in s are alphanumeric
s.isdigit() 	test if s is non-empty and all characters in s are digits
s.istitle() 	test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)



>>> sorted(w for w in set(text1) if w.endswith('ableness'))
>>> sorted(term for term in set(text4) if 'gnt' in term)
>>> sorted(item for item in set(text6) if item.istitle())
>>> sorted(item for item in set(sent7) if item.isdigit())

>>> sorted(w for w in set(text7) if '-' in w and 'index' in w)
>>> sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10)
>>> sorted(w for w in set(sent7) if not w.islower())
>>> sorted(t for t in set(text2) if 'cie' in t or 'cei' in t)



>>> [len(w) for w in text1]
>>> [w.upper() for w in text1]



>>> len(text1)
>>> len(set(text1))
>>> len(set(word.lower() for word in text1))

------------------------------------
Gutenberg Corpus
------------------------------------

>>> import nltk
>>> nltk.corpus.gutenberg.fileids()


>>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')
>>> len(emma)
>>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))
>>> emma.concordance("surprize")


>>> from nltk.corpus import gutenberg
>>> gutenberg.fileids()
['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]
>>> emma = gutenberg.words('austen-emma.txt')





>>> for fileid in gutenberg.fileids():
...     num_chars = len(gutenberg.raw(fileid)) [1]
...     num_words = len(gutenberg.words(fileid))
...     num_sents = len(gutenberg.sents(fileid))
...     num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))
...     print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)


ID 	File 	Genre 	Description
A16 	ca16 	news 	Chicago Tribune: Society Reportage
B02 	cb02 	editorial 	Christian Science Monitor: Editorials
C17 	cc17 	reviews 	Time Magazine: Reviews
D12 	cd12 	religion 	Underwood: Probing the Ethics of Realtors
E36 	ce36 	hobbies 	Norling: Renting a Car in Europe
F25 	cf25 	lore 	Boroff: Jewish Teenage Culture
G22 	cg22 	belles_lettres 	Reiner: Coping with Runaway Technology
H15 	ch15 	government 	US Office of Civil and Defence Mobilization: The Family Fallout Shelter
J17 	cj19 	learned 	Mosteller: Probability with Statistical Applications
K04 	ck04 	fiction 	W.E.B. Du Bois: Worlds of Color
L13 	cl13 	mystery 	Hitchens: Footsteps in the Night
M01 	cm01 	science_fiction 	Heinlein: Stranger in a Strange Land
N14 	cn15 	adventure 	Field: Rattlesnake Ridge
P12 	cp12 	romance 	Callaghan: A Passion in Rome
R06 	cr06 	humor 	Thurber: The Future, If Any, of Comedy



>>> from nltk.corpus import brown
>>> brown.categories()
>>> brown.words(categories='news')
>>> brown.sents(categories=['news', 'editorial', 'reviews'])



>>> from nltk.corpus import brown
>>> news_text = brown.words(categories='news')
>>> fdist = nltk.FreqDist(w.lower() for w in news_text)
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> for m in modals:
...     print(m + ':', fdist[m], end=' ')


>>> cfd = nltk.ConditionalFreqDist(
...           (genre, word)
...           for genre in brown.categories()
...           for word in brown.words(categories=genre))
>>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> cfd.tabulate(conditions=genres, samples=modals)



>>> cfd = nltk.ConditionalFreqDist(
...           (target, fileid[:4])
...           for fileid in inaugural.fileids()
...           for w in inaugural.words(fileid)
...           for target in ['america', 'citizen']
...           if w.lower().startswith(target)) [1]
>>> cfd.plot()





>>> from nltk.corpus import udhr
>>> languages = ['Chickasaw', 'English', 'German_Deutsch',
...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
>>> cfd = nltk.ConditionalFreqDist(
...           (lang, len(word))
...           for lang in languages
...           for word in udhr.words(lang + '-Latin1'))
>>> cfd.plot(cumulative=True)





fileids() 	the files of the corpus
fileids([categories]) 	the files of the corpus corresponding to these categories
categories() 	the categories of the corpus
categories([fileids]) 	the categories of the corpus corresponding to these files
raw() 	the raw content of the corpus
raw(fileids=[f1,f2,f3]) 	the raw content of the specified files
raw(categories=[c1,c2]) 	the raw content of the specified categories
words() 	the words of the whole corpus
words(fileids=[f1,f2,f3]) 	the words of the specified fileids
words(categories=[c1,c2]) 	the words of the specified categories
sents() 	the sentences of the whole corpus
sents(fileids=[f1,f2,f3]) 	the sentences of the specified fileids
sents(categories=[c1,c2]) 	the sentences of the specified categories
abspath(fileid) 	the location of the given file on disk
encoding(fileid) 	the encoding of the file (if known)
open(fileid) 	open a stream for reading the given corpus file
root 	if the path to the root of locally installed corpus
readme() 	the contents of the README file of the corpus



>>> raw = gutenberg.raw("burgess-busterbrown.txt")
>>> raw[1:20]
'The Adventures of B'
>>> words = gutenberg.words("burgess-busterbrown.txt")
>>> words[1:20]
['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.',
'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster',
'Bear']
>>> sents = gutenberg.sents("burgess-busterbrown.txt")
>>> sents[1:20]


>>> from nltk.corpus import inaugural
>>> cfd = nltk.ConditionalFreqDist(
...           (target, fileid[:4]) [1]
...           for fileid in inaugural.fileids()
...           for w in inaugural.words(fileid)
...           for target in ['america', 'citizen'] [2]
...           if w.lower().startswith(target))




>>> from nltk.corpus import udhr
>>> languages = ['Chickasaw', 'English', 'German_Deutsch',
...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
>>> cfd = nltk.ConditionalFreqDist(
...           (lang, len(word)) [1]
...           for lang in languages
...           for word in udhr.words(lang + '-Latin1'))

>>> cfd.tabulate(conditions=['English', 'German_Deutsch'],
...              samples=range(10), cumulative=True)





>>> from __future__ import division
>>> def lexical_diversity(text):
...     return len(text) / len(set(text))






>>> from text_proc import plural
>>> plural('wish')
wishes
>>> plural('fan')
fen




>>> from nltk.corpus import stopwords
>>> stopwords.words('english')





>>> def content_fraction(text):
...     stopwords = nltk.corpus.stopwords.words('english')
...     content = [w for w in text if w.lower() not in stopwords]
...     return len(content) / len(text)
...
>>> content_fraction(nltk.corpus.reuters.words())





>>> names = nltk.corpus.names
>>> names.fileids()
['female.txt', 'male.txt']
>>> male_names = names.words('male.txt')
>>> female_names = names.words('female.txt')
>>> [w for w in male_names if w in female_names]
>>> cfd = nltk.ConditionalFreqDist(
...           (fileid, name[-1])
...           for fileid in names.fileids()
...           for name in names.words(fileid))
>>> cfd.plot()



>>> languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']
>>> for i in [139, 140, 141, 142]:
...     print(swadesh.entries(languages)[i])





------------------------------------
Dealing with text
------------------------------------

from urllib import request
url = "http://www.gutenberg.org/files/2554/2554.txt"
response = request.urlopen(url)
raw = response.read().decode('utf8')
type(raw)
len(raw)


tokens = word_tokenize(raw)
type(tokens)
len(tokens)
tokens[:10]

text = nltk.Text(tokens)
type(text)
text[1024:1062]
text.collocations()


raw.find("PART I")
raw.rfind("End of Project Gutenberg's Crime")
raw = raw[5338:1157743]
raw.find("PART I")

------------------------------------
Dealing with HTML
------------------------------------

url = "http://news.bbc.co.uk/2/hi/health/2284783.stm"
html = request.urlopen(url).read().decode('utf8')
html[:60]

from bs4 import BeautifulSoup
raw = BeautifulSoup(html).get_text()
tokens = word_tokenize(raw)
tokens

tokens = tokens[110:390]
text = nltk.Text(tokens)
text.concordance('gene')

------------------------------------
Processing RSS Feeds
------------------------------------

import feedparser
llog = feedparser.parse("http://languagelog.ldc.upenn.edu/nll/?feed=atom")
llog['feed']['title']
len(llog.entries)
post = llog.entries[2]
post.title
content = post.content[0].value
content[:70]
raw = BeautifulSoup(content).get_text()
word_tokenize(raw)

------------------------------------
Reading Local Files
------------------------------------
f = open('document.txt')
raw = f.read()
f = open('document.txt', 'rU')

import os
os.listdir('.')
f.read()

f = open('document.txt', 'rU')
for line in f:
	print(line.strip())

path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')
raw = open(path, 'rU').read()

------------------------------------
Extracting Text from PDF, MSWord and other Binary Formats
------------------------------------

s = input("Enter some text: ")
print("You typed", len(word_tokenize(s)), "words.")

------------------------------------
The NLP Pipeline
------------------------------------

raw = open('document.txt').read()
type(raw)

tokens = word_tokenize(raw)
type(tokens)
words = [w.lower() for w in tokens]
type(words)
vocab = sorted(set(words))
type(vocab)

vocab.append('blog')
raw.append('blog')

query = 'Who knows?'
beatles = ['john', 'paul', 'george', 'ringo']
query + beatles

------------------------------------
Strings: Text Processing at the Lowest Level
------------------------------------
sent = 'colorless green ideas sleep furiously'
for char in sent:
    print(char, end=' ')


from nltk.corpus import gutenberg
raw = gutenberg.raw('melville-moby_dick.txt')
fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())
fdist.most_common(5)
[char for (char, count) in fdist.most_common()]


monty[6:10]
monty[-12:-7]
monty[:5]
monty[6:]


phrase = 'And now for something completely different'
if 'thing' in phrase:
	print('found "thing"')

monty.find('Python')



Method 	Functionality
s.find(t) 	index of first instance of string t inside s (-1 if not found)
s.rfind(t) 	index of last instance of string t inside s (-1 if not found)
s.index(t) 	like s.find(t) except it raises ValueError if not found
s.rindex(t) 	like s.rfind(t) except it raises ValueError if not found
s.join(text) 	combine the words of the text into a string using s as the glue
s.split(t) 	split s into a list wherever a t is found (whitespace by default)
s.splitlines() 	split s into a list of strings, one per line
s.lower() 	a lowercased version of the string s
s.upper() 	an uppercased version of the string s
s.title() 	a titlecased version of the string s
s.strip() 	a copy of s without leading or trailing whitespace
s.replace(t, u) 	replace instances of t with u inside s


------------------------------------
Extracting encoded text from files
------------------------------------

path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')

f = open(path, encoding='latin2')
for line in f:
   line = line.strip()
   print(line)

f = open(path, encoding='latin2')
for line in f:
    line = line.strip()
    print(line.encode('unicode_escape'))

------------------------------------
Regular Expressions for Detecting Word Patterns
------------------------------------

import re
wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]

[w for w in wordlist if re.search('ed$', w)]
[w for w in wordlist if re.search('^..j..t..$', w)]
[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]

chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))
[w for w in chat_words if re.search('^m+i+n+e+$', w)]
[w for w in chat_words if re.search('^[ha]+$', w)]

wsj = sorted(set(nltk.corpus.treebank.words()))
[w for w in wsj if re.search('^[0-9]+\.[0-9]+$', w)]

[w for w in wsj if re.search('^[A-Z]+\$$', w)]
[w for w in wsj if re.search('^[0-9]{4}$', w)]
[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]
[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]
[w for w in wsj if re.search('(ed|ing)$', w)]


Operator 	Behavior
. 	Wildcard, matches any character
^abc 	Matches some pattern abc at the start of a string
abc$ 	Matches some pattern abc at the end of a string
[abc] 	Matches one of a set of characters
[A-Z0-9] 	Matches one of a range of characters
ed|ing|s 	Matches one of the specified strings (disjunction)
* 	Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)
+ 	One or more of previous item, e.g. a+, [a-z]+
? 	Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?
{n} 	Exactly n repeats where n is a non-negative integer
{n,} 	At least n repeats
{,n} 	No more than n repeats
{m,n} 	At least m and no more than n repeats
a(b|c)+ 	Parentheses that indicate the scope of the operators



------------------------------------
Useful Applications of Regular Expressions
------------------------------------

Extracting Word Pieces

word = 'supercalifragilisticexpialidocious'
re.findall(r'[aeiou]', word)
len(re.findall(r'[aeiou]', word))


wsj = sorted(set(nltk.corpus.treebank.words()))
fd = nltk.FreqDist(vs for word in wsj
   for vs in re.findall(r'[aeiou]{2,}', word))
fd.most_common(12)



regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'
def compress(word):
    pieces = re.findall(regexp, word)
    return ''.join(pieces)

english_udhr = nltk.corpus.udhr.words('English-Latin1')
print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))



rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')
cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]
cfd = nltk.ConditionalFreqDist(cvs)
cfd.tabulate()



cv_word_pairs = [(cv, w) for w in rotokas_words
                          for cv in re.findall(r'[ptksvr][aeiou]', w)]
cv_index = nltk.Index(cv_word_pairs)
cv_index['su']
cv_index['po']


Finding Word Stems

re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')
re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')
re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')



from nltk.corpus import gutenberg, nps_chat
moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))
moby.findall(r"<a> (<.*>) <man>")
chat = nltk.Text(nps_chat.words())
chat.findall(r"<.*> <.*> <bro>")
chat.findall(r"<l.*>{3,}")



from nltk.corpus import brown
hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))
hobbies_learned.findall(r"<\w*> <and> <other> <\w*s>")



raw = """DENNIS: Listen, strange women lying in ponds distributing swords
... is no basis for a system of government.  Supreme executive power derives from
... a mandate from the masses, not from some farcical aquatic ceremony."""
tokens = word_tokenize(raw)



re.split(r'\W+', raw)
re.findall(r'\w+|\S\w*', raw)


Symbol 	Function
\b 	Word boundary (zero width)
\d 	Any decimal digit (equivalent to [0-9])
\D 	Any non-digit character (equivalent to [^0-9])
\s 	Any whitespace character (equivalent to [ \t\n\r\f\v])
\S 	Any non-whitespace character (equivalent to [^ \t\n\r\f\v])
\w 	Any alphanumeric character (equivalent to [a-zA-Z0-9_])
\W 	Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])
\t 	The tab character
\n 	The newline character


NLTK's Regular Expression Tokenizer
Sentence Segmentation

len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())

text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
sents = nltk.sent_tokenize(text)
pprint.pprint(sents[79:89])



Strings and Formats
# frequency

fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])
for word in sorted(fdist):
	print(word, '->', fdist[word], end='; ')

for word in sorted(fdist):
    print('{}->{};'.format(word, fdist[word]), end=' ')


template = 'Lee wants a {} right now'
menu = ['sandwich', 'spam fritter', 'pancake']
for snack in menu:
    print(template.format(snack))





def tabulate(cfdist, words, categories):
    print('{:16}'.format('Category'), end=' ')                    # column headings
    for word in words:
        print('{:>6}'.format(word), end=' ')
    print()
    for category in categories:
        print('{:16}'.format(category), end=' ')                  # row heading
        for word in words:                                        # for each word
            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell
        print()                                                   # end the row

from nltk.corpus import brown
cfd = nltk.ConditionalFreqDist(
          (genre, word)
           for genre in brown.categories()
           for word in brown.words(categories=genre))
genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
modals = ['can', 'could', 'may', 'might', 'must', 'will']
tabulate(cfd, modals, genres)


------------------------------------
Writing Results to a File
------------------------------------

output_file = open('output.txt', 'w')
words = set(nltk.corpus.genesis.words('english-kjv.txt'))
for word in sorted(words):
    print(word, file=output_file)

len(words)
str(len(words))
print(str(len(words)), file=output_file)







Python Expression 	Comment
for item in s 	iterate over the items of s
for item in sorted(s) 	iterate over the items of s in order
for item in set(s) 	iterate over unique elements of s
for item in reversed(s) 	iterate over elements of s in reverse
for item in set(s).difference(t) 	iterate over elements of s not in t





raw = 'Red lorry, yellow lorry, red lorry, yellow lorry.'
text = word_tokenize(raw)
fdist = nltk.FreqDist(text)
sorted(fdist)
for key in fdist:
    print(key + ':', fdist[key], end='; ')

------------------------------------
Procedural vs Declarative Style
------------------------------------


>>> tokens = nltk.corpus.brown.words(categories='news')
>>> count = 0
>>> total = 0
>>> for token in tokens:
...     count += 1
...     total += len(token)
>>> total / count



>>> total = sum(len(t) for t in tokens)
>>> print(total / len(tokens))



>>> word_list = []
>>> i = 0
>>> while i < len(tokens):
...     j = 0
...     while j < len(word_list) and word_list[j] <= tokens[i]:
...         j += 1
...     if j == 0 or tokens[i] != word_list[j-1]:
...         word_list.insert(j, tokens[i])
...     i += 1

>>> word_list = sorted(set(tokens))






>>> fd = nltk.FreqDist(nltk.corpus.brown.words())
>>> cumulative = 0.0
>>> most_common_words = [word for (word, count) in fd.most_common()]
>>> for rank, word in enumerate(most_common_words):
...     cumulative += fd.freq(word)
...     print("%3d %6.2f%% %s" % (rank + 1, cumulative * 100, word))
...     if cumulative > 0.25:
...         break
...


>>> text = nltk.corpus.gutenberg.words('milton-paradise.txt')
>>> longest = ''
>>> for word in text:
...     if len(word) > len(longest):
...         longest = word
>>> longest



>>> maxlen = max(len(word) for word in text)
>>> [word for word in text if len(word) == maxlen]



------------------------------------
Functions: The Foundation of Structured Programming
------------------------------------

import re
def get_text(file):
    """Read text from a file, normalizing whitespace and stripping HTML markup."""
    text = open(file).read()
    text = re.sub(r'<.*?>', ' ', text)
    text = re.sub('\s+', ' ', text)
    return text

>>> def my_sort1(mylist):      # good: modifies its argument, no return value
...     mylist.sort()
>>> def my_sort2(mylist):      # good: doesn't touch its argument, returns value
...     return sorted(mylist)



>>> for item in search1('zz', nltk.corpus.brown.words()):
...     print(item, end=" ")

>>> lengths = list(map(len, nltk.corpus.brown.sents(categories='news')))
>>> sum(lengths) / len(lengths)




>>> def freq_words(file, min=1, num=10):
...     text = open(file).read()
...     tokens = word_tokenize(text)
...     freqdist = nltk.FreqDist(t for t in tokens if len(t) >= min)
...     return freqdist.most_common(num)
>>> fw = freq_words('ch01.rst', 4, 10)
>>> fw = freq_words('ch01.rst', min=4, num=10)
>>> fw = freq_words('ch01.rst', num=10, min=4)



work with a lot of files
>>> with open("lexicon.txt") as f:
...     data = f.read()
...     # process the data













------------------------------------
Sample of Python Libraries
------------------------------------

from numpy import arange
from matplotlib import pyplot

colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black

def bar_chart(categories, words, counts):
    "Plot a bar chart showing counts for each word by category"
    ind = arange(len(words))
    width = 1 / (len(categories) + 1)
    bar_groups = []
    for c in range(len(categories)):
        bars = pyplot.bar(ind+c*width, counts[categories[c]], width,
                         color=colors[c % len(colors)])
        bar_groups.append(bars)
    pyplot.xticks(ind+width, words)
    pyplot.legend([b[0] for b in bar_groups], categories, loc='upper left')
    pyplot.ylabel('Frequency')
    pyplot.title('Frequency of Six Modal Verbs by Genre')
    pyplot.show()



>>> genres = ['news', 'religion', 'hobbies', 'government', 'adventure']
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> cfdist = nltk.ConditionalFreqDist(
...              (genre, word)
...              for genre in genres
...              for word in nltk.corpus.brown.words(categories=genre)
...              if word in modals)
...
>>> counts = {}
>>> for genre in genres:
...     counts[genre] = [cfdist[genre][word] for word in modals]
>>> bar_chart(genres, modals, counts)




>>> from matplotlib import use, pyplot
>>> use('Agg') [1]
>>> pyplot.savefig('modals.png') [2]
>>> print('Content-Type: text/html')
>>> print()
>>> print('<html><body>')
>>> print('<img src="modals.png"/>')
>>> print('</body></html>')



------------------------------------
NetworkX
------------------------------------
import networkx as nx
import matplotlib
from nltk.corpus import wordnet as wn

def traverse(graph, start, node):
    graph.depth[node.name] = node.shortest_path_distance(start)
    for child in node.hyponyms():
        graph.add_edge(node.name, child.name) [1]
        traverse(graph, start, child) [2]

def hyponym_graph(start):
    G = nx.Graph() [3]
    G.depth = {}
    traverse(G, start, start)
    return G

def graph_draw(graph):
    nx.draw_graphviz(graph,
         node_size = [16 * graph.degree(n) for n in graph],
         node_color = [graph.depth[n] for n in graph],
         with_labels = False)
    matplotlib.pyplot.show()



>>> dog = wn.synset('dog.n.01')
>>> graph = hyponym_graph(dog)
>>> graph_draw(graph)





>>> import csv
>>> input_file = open("lexicon.csv", "rb") [1]
>>> for row in csv.reader(input_file): [2]
...     print(row)




>>> from numpy import array
>>> cube = array([ [[0,0,0], [1,1,1], [2,2,2]],
...                [[3,3,3], [4,4,4], [5,5,5]],
...                [[6,6,6], [7,7,7], [8,8,8]] ])
>>> cube[1,1,1]
>>> cube[2].transpose()
>>> cube[2,1:]




>>> from numpy import linalg
>>> a=array([[4,0], [3,-5]])
>>> u,s,vt = linalg.svd(a)
>>> u
>>> s
>>> vt



------------------------------------
Categorizing and Tagging Words
------------------------------------

Using a Tagger

>>> text = word_tokenize("And now for something completely different")
>>> nltk.pos_tag(text)

>>> text = word_tokenize("They refuse to permit us to obtain the refuse permit")
>>> nltk.pos_tag(text)




>>> text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())
>>> text.similar('woman')
>>> text.similar('bought')
>>> text.similar('over')
>>> text.similar('the')

------------------------------------
Tagged Corpora
------------------------------------
>>> tagged_token = nltk.tag.str2tuple('fly/NN')
>>> tagged_token
>>> tagged_token[0]
>>> tagged_token[1]



>>> sent = '''Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS'''
>>> [nltk.tag.str2tuple(t) for t in sent.split()]


Reading Tagged Corpora

>>> nltk.corpus.brown.tagged_words()
>>> nltk.corpus.brown.tagged_words(tagset='universal')


>>> print(nltk.corpus.nps_chat.tagged_words())
>>> nltk.corpus.conll2000.tagged_words()
>>> nltk.corpus.treebank.tagged_words()
>>> nltk.corpus.brown.tagged_words(tagset='universal')
>>> nltk.corpus.treebank.tagged_words(tagset='universal')





>>> from nltk.corpus import brown
>>> brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')
>>> tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)
>>> tag_fd.most_common()


------------------------------------
Verbs
------------------------------------
wsj = nltk.corpus.treebank.tagged_words(tagset='universal')
>>> word_tag_fd = nltk.FreqDist(wsj)
>>> [wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == 'VERB']





>>> cfd1 = nltk.ConditionalFreqDist(wsj)
>>> cfd1['yield'].most_common()
>>> cfd1['cut'].most_common()




>>> wsj = nltk.corpus.treebank.tagged_words()
>>> cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)
>>> list(cfd2['VBN'])



def findtags(tag_prefix, tagged_text):
    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text
                                  if tag.startswith(tag_prefix))
    return dict((tag, cfd[tag].most_common(5)) for tag in cfd.conditions())
>>> tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news'))
>>> for tag in sorted(tagdict):
...     print(tag, tagdict[tag])



------------------------------------
 Indexing Lists vs Dictionaries
 ------------------------------------


>>> pos = {}
>>> pos['colorless'] = 'ADJ'
>>> pos['ideas'] = 'N'
>>> pos['sleep'] = 'V'
>>> pos['furiously'] = 'ADV'












>>> alice = nltk.corpus.gutenberg.words('carroll-alice.txt')
>>> vocab = nltk.FreqDist(alice)
>>> v1000 = [word for (word, _) in vocab.most_common(1000)]
>>> mapping = defaultdict(lambda: 'UNK')
>>> for v in v1000:
...     mapping[v] = v
...
>>> alice2 = [mapping[v] for v in alice]
>>> alice2[:100]
>>> len(set(alice2))





>>> counts = defaultdict(int)
>>> for word in nltk.corpus.gutenberg.words('milton-paradise.txt'):
...     counts[word] += 1
...
>>> [key for (key, value) in counts.items() if value == 32]


------------------------------------
Storing Taggers
------------------------------------

>>> from pickle import dump
>>> output = open('t2.pkl', 'wb')
>>> dump(t2, output, -1)
>>> output.close()


>>> from pickle import load
>>> input = open('t2.pkl', 'rb')
>>> tagger = load(input)
>>> input.close()



------------------------------------
Learning to Classify Text
------------------------------------

Supervised Classification


Gender Identification #####

def gender_features(word):
...     return {'last_letter': word[-1]}
>>> gender_features('Shrek')



>>> from nltk.corpus import names
>>> labeled_names = ([(name, 'male') for name in names.words('male.txt')] +
... [(name, 'female') for name in names.words('female.txt')])
>>> import random
>>> random.shuffle(labeled_names)



>>> featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]
>>> train_set, test_set = featuresets[500:], featuresets[:500]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)

>>> classifier.classify(gender_features('Neo'))
>>> classifier.classify(gender_features('Trinity'))
>>> print(nltk.classify.accuracy(classifier, test_set))
>>> classifier.show_most_informative_features(5)



>>> from nltk.classify import apply_features
>>> train_set = apply_features(gender_features, labeled_names[500:])
>>> test_set = apply_features(gender_features, labeled_names[:500])


Choosing The Right Features


>>> featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]
>>> train_set, test_set = featuresets[500:], featuresets[:500]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> print(nltk.classify.accuracy(classifier, test_set))

>>> train_names = labeled_names[1500:]
>>> devtest_names = labeled_names[500:1500]
>>> test_names = labeled_names[:500]


>>> train_set = [(gender_features(n), gender) for (n, gender) in train_names]
>>> devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]
>>> test_set = [(gender_features(n), gender) for (n, gender) in test_names]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> print(nltk.classify.accuracy(classifier, devtest_set))



Document Classification

>>> from nltk.corpus import movie_reviews
>>> documents = [(list(movie_reviews.words(fileid)), category)
...              for category in movie_reviews.categories()
...              for fileid in movie_reviews.fileids(category)]
>>> random.shuffle(documents)





all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000] [1]

def document_features(document): [2]
    document_words = set(document) [3]
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features

>>> print(document_features(movie_reviews.words('pos/cv957_8737.txt')))





featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = nltk.NaiveBayesClassifier.train(train_set)

>>> print(nltk.classify.accuracy(classifier, test_set))
>>> classifier.show_most_informative_features(5)


------------------------------------
7. Extracting Information from Text
------------------------------------



>>> sentence = [("the", "DT"), ("little", "JJ"), ("yellow", "JJ"), [1]
... ("dog", "NN"), ("barked", "VBD"), ("at", "IN"),  ("the", "DT"), ("cat", "NN")]
>>> grammar = "NP: {<DT>?<JJ>*<NN>}"

>>> cp = nltk.RegexpParser(grammar)
>>> result = cp.parse(sentence)
>>> print(result)
>>> result.draw()




>>> text = '''
... he PRP B-NP
... accepted VBD B-VP
... the DT B-NP
... position NN I-NP
... of IN B-PP
... vice NN B-NP
... chairman NN I-NP
... of IN B-PP
... Carlyle NNP B-NP
... Group NNP I-NP
... , , O
... a DT B-NP
... merchant NN I-NP
... banking NN I-NP
... concern NN I-NP
... . . O
... '''
>>> nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()





>>> from nltk.corpus import conll2000
>>> print(conll2000.chunked_sents('train.txt')[99])
>>> print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])

Simple Evaluation and Baselines


>>> from nltk.corpus import conll2000
>>> cp = nltk.RegexpParser("")
>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])
>>> print(cp.evaluate(test_sents))


>>> grammar = r"NP: {<[CDJNP].*>+}"
>>> cp = nltk.RegexpParser(grammar)
>>> print(cp.evaluate(test_sents))



Trees


>>> tree1 = nltk.Tree('NP', ['Alice'])
>>> print(tree1)
>>> tree2 = nltk.Tree('NP', ['the', 'rabbit'])
>>> print(tree2)
>>> tree3 = nltk.Tree('VP', ['chased', tree2])
>>> tree4 = nltk.Tree('S', [tree1, tree3])
>>> print(tree4)
>>> print(tree4[1])
>>> tree4[1].label()
>>> tree4.leaves()
>>> tree4[1][1][1]
>>> tree3.draw()




Relation Extraction

>>> IN = re.compile(r'.*\bin\b(?!\b.+ing)')
>>> for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):
...     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,
...                                      corpus='ieer', pattern = IN):
...         print(nltk.sem.rtuple(rel))



>>> rd_parser = nltk.RecursiveDescentParser(grammar1)
>>> sent = 'Mary saw a dog'.split()
>>> for tree in rd_parser.parse(sent):
...     print(tree)


