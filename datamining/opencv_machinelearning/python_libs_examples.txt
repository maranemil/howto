#####################################################
#
# Python Libs Examples
#
#####################################################

https://www.tutorialspoint.com/python/index.htm

online ester
https://www.pythonanywhere.com/try-ipython/
https://www.jdoodle.com/python3-programming-online
https://trinket.io/python  # Put Interactive Python Anywhere on the Web
https://repl.it/languages/python3

# ///// Packages //////

https://www.scipy.org/about.html
https://www.scipy.org
http://scikit-learn.org/stable/tutorial/index.html
http://pandas.pydata.org
http://www.nltk.org
http://www.nltk.org/howto/
http://www.nltk.org/howto/classify.html
http://www.nltk.org/_modules/nltk/classify/decisiontree.html
http://www.nltk.org/api/nltk.classify.html

# ///// Wiki Scientific Packages  //////

https://wiki.python.org/moin/NumericAndScientific

NumPy - http://www.numpy.org/
SciPy - http://www.scipy.org/
SymPy - http://www.sympy.org/
PyGSL - http://pygsl.sourceforge.net/
PyGTS - http://pygts.sourceforge.net/
scikit-learn - http://scikit-learn.sourceforge.net/
mlpy - https://mlpy.fbk.eu/
graph-tool - http://graph-tool.skewed.de

///// 10 Best Python Frameworks for Web Developers /////

http://www.cubicweb.org/
https://pypi.python.org/pypi/Zope2
http://www.web2py.com/
http://www.turbogears.org/
http://www.pylonsproject.org/
http://grok.zope.org/
http://webpy.org/
http://www.pylonsproject.org/projects/pyramid/about
http://www.cherrypy.org/
http://flask.pocoo.org/

-------

http://nullege.com/codes/search/nltk.classify.DecisionTreeClassifier
http://nullege.com/codes/show/src%40v%40a%40vaccine-sentiment-HEAD%40analysis%40sentiment-calculation%40classifier%40decision-tree.py/146/nltk.classify.DecisionTreeClassifier.train/python
http://www.markhneedham.com/blog/2015/02/24/pythonnltk-naive-vs-naive-bayes-vs-decision-tree/
http://www.nltk.org/book/ch06.html

>>> classifier = nltk.classify.NaiveBayesClassifier.train(train)  # note the use of NaiveBayesClassifier here
>>> for pdist in classifier.batch_prob_classify(test):
      print('%.4f %.4f' % (pdist.prob('x'), pdist.prob('y')))


0.3104 0.6896
0.5746 0.4254
0.3685 0.6315
0.6365 0.3635


#####################################################
#
#   generating-permutations-in-python
#   https://codereview.stackexchange.com/questions/176283/generating-permutations-in-python
#
#####################################################
itertools.permutations:

import itertools
#next_permutation([1,2,3]) == list(itertools.permutations([1,2,3]))[0]

for permutation in itertools.permutations([1,2,3]):
 print(permutation)

(1, 2, 3)
(1, 3, 2)
(2, 1, 3)
(2, 3, 1)
(3, 1, 2)
(3, 2, 1)

https://docs.python.org/3/library/random.html#random.choices
https://docs.python.org/2/library/itertools.html
https://docs.python.org/3/library/itertools.html
https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.compress.html


>>> print list(itertools.permutations([1,2,3], 2))
[(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]

>>> print list(itertools.combinations([1,2,3], 2))
[(1, 2), (1, 3), (2, 3)]

>>> print list(itertools.product([0,1], repeat=3))
[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0),
(1, 0, 1), (1, 1, 0), (1, 1, 1)]


from itertools import chain
print list(chain([1, 2, 3], ['a', 'b', 'c']))
# [1, 2, 3, 'a', 'b', 'c']

from itertools import izip
print list(izip([1, 2, 3], ['a', 'b', 'c']))
# [(1, 'a'), (2, 'b'), (3, 'c')]


for i in islice(range(100), 0, 100, 10):
    print i
# 0
# 10
# 20


from itertools import imap
print list(imap(lambda x: x * x, xrange(10)))
# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]


from itertools import starmap
print list(starmap(lambda x,y: (x, y, x * y), [(1, 4), (2, 4), (3, 4), (4, 4)]))
# [(1, 4, 4), (2, 4, 8), (3, 4, 12), (4, 4, 16)]


from itertools import cycle
for number, letter in izip(cycle(range(2)), ['a', 'b', 'c', 'd', 'e']):
    print '{0}: {1}'.format(number, letter)
# 0: a
# 1: b
# 0: c
# 1: d
# 0: e


from itertools import repeat
print list(repeat('Hello, world!', 3))
# ['Hello, world!', 'Hello, world!', 'Hello, world!']


from itertools import dropwhile
print list(dropwhile(lambda x: x < 10, [1, 4, 6, 7, 11, 34, 66, 100, 1]))
# [11, 34, 66, 100, 1]


from itertools import takewhile
print list(takewhile(lambda x: x < 10, [1, 4, 6, 7, 11, 34, 66, 100, 1]))
# [1, 4, 6, 7]


from itertools import ifilter
print list(ifilter(lambda x: x < 10, [1, 4, 6, 7, 11, 34, 66, 100, 1]))
# [1, 4, 6, 7, 1]


from collections import defaultdict
counts = defaultdict(list)
attempts = [('dan', 87), ('erik', 95), ('jason', 79), ('erik', 97), ('dan', 100)]
for (name, score) in attempts:
    counts[name].append(score)
print counts
# defaultdict(<type 'list'>, {'dan': [87, 100], 'jason': [79], 'erik': [95, 97]})

http://programeveryday.com/post/using-python-itertools-to-save-memory/
https://www.blog.pythonlibrary.org/2016/04/20/python-201-an-intro-to-itertools/
http://naiquevin.github.io/a-look-at-some-of-pythons-useful-itertools.html

dice = range(2, 7)
random.choice([p for p in itertools.product(dice, repeat=2)])

# output:
(5, 2)
(2, 5)
(2, 5)
(6, 5)


friends = 'bob tim julian fred'.split()
# as these are "Combinatoric generators" I consume them here for example purposes using list()
list(itertools.combinations(friends, 2))

# output:
[('bob', 'tim'),
('bob', 'julian'),
('bob', 'fred'),
('tim', 'julian'),
('tim', 'fred'),
('julian', 'fred')]

#####################################################

https://pybit.es/itertools-examples.html
https://people.rit.edu/blbgse/pythonNotes/itertools.html
https://krother.gitbooks.io/python-3-module-examples/content/itertools.html
http://www.diveintopython3.net/porting-code-to-python-3-with-2to3.html
https://kite.com/docs/python/itertools.islice
https://pymotw.com/2/itertools/

~/PycharmProjects/untitled$ python test5.py

#####################################################
#
#  scikit-learn examples
#
#####################################################

http://scikit-learn.org/stable/modules/preprocessing.html
http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html
http://scikit-learn.org/stable/auto_examples/index.html

http://scikit-learn.org/stable/auto_examples/plot_multilabel.html#sphx-glr-auto-examples-plot-multilabel-py
http://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py
http://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py
http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py
http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py
http://scikit-learn.org/stable/auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py

http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html#sphx-glr-auto-examples-cluster-plot-cluster-iris-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py

http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_regression_multioutput.html#sphx-glr-auto-examples-ensemble-plot-random-forest-regression-multioutput-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html#sphx-glr-auto-examples-ensemble-plot-voting-probas-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html#sphx-glr-auto-examples-linear-model-plot-logistic-path-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py
http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py
http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py
http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py
http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py
http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py

http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py

http://pandas.pydata.org
http://pandas.pydata.org/pandas-docs/stable/timeseries.html

http://deeplearning.net/software/theano/tutorial/index.html
http://deeplearning.net/software/theano/tutorial/examples.html#logistic-function

#####################################################
#
#    http://www.nltk.org
#    http://www.nltk.org/howto/tree.html
#
#####################################################

Tokenize and tag some text:

>>> import nltk
>>> sentence = """At eight o'clock on Thursday morning
... Arthur didn't feel very good."""
>>> tokens = nltk.word_tokenize(sentence)
>>> tokens
['At', 'eight', "o'clock", 'on', 'Thursday', 'morning',
'Arthur', 'did', "n't", 'feel', 'very', 'good', '.']
>>> tagged = nltk.pos_tag(tokens)
>>> tagged[0:6]
[('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'),
('Thursday', 'NNP'), ('morning', 'NN')]
Identify named entities:

>>> entities = nltk.chunk.ne_chunk(tagged)
>>> entities
Tree('S', [('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'),
           ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),
       Tree('PERSON', [('Arthur', 'NNP')]),
           ('did', 'VBD'), ("n't", 'RB'), ('feel', 'VB'),
           ('very', 'RB'), ('good', 'JJ'), ('.', '.')])
Display a parse tree:

>>> from nltk.corpus import treebank
>>> t = treebank.parsed_sents('wsj_0001.mrg')[0]
>>> t.draw()


#####################################################
# Oder resources
#####################################################

https://github.com/lmjohns3/theanets
https://github.com/lmjohns3/theanets/blob/master/examples/weighted-classification.py
http://caffe.berkeleyvision.org/tutorial/forward_backward.html

http://pybrain.org/docs/#tutorials
http://pybrain.org/docs/tutorial/fnn.html
http://pybrain.org/docs/tutorial/netmodcon.html#using-recurrent-networks
http://pybrain.org/docs/tutorial/datasets.html#superviseddataset
http://pybrain.org/docs/tutorial/datasets.html#classificationdataset
http://pybrain.org/docs/tutorial/reinforcement-learning.html

http://www.csie.ntu.edu.tw/~cjlin/liblinear/#LIBLINEAR -- A Library for Large Linear Classification
http://www.csie.ntu.edu.tw/~cjlin/libsvm/

% time libsvm-2.85/svm-train -c 4 -t 0 -e 0.1 -m 800 -v 5 rcv1_train.binary
Cross Validation Accuracy = 96.8136%
345.569s
% time liblinear-1.21/train -c 4 -e 0.1 -v 5 rcv1_train.binary
Cross Validation Accuracy = 97.0161%
2.944s

https://pythonhosted.org/milk/
https://pythonhosted.org/milk/supervised.html
http://www.cs.waikato.ac.nz/ml/weka/datasets.html
https://www.knime.com/knime-applications/credit-scoring
https://rapidminer.com/products/studio/
https://dbaumgartel.wordpress.com/2014/03/14/machine-learning-examples-scikit-learn-versus-tmva-cern-root/
http://www.erogol.com/broad-view-machine-learning-libraries/

#####################################################
#
#   python machine learning tutorial
#
#####################################################

https://elitedatascience.com/datasets#time-series
https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn

# 2. Import libraries and modules
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib

# 3. Load red wine data.
dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
data = pd.read_csv(dataset_url, sep=';')

# 4. Split data into training and test sets
y = data.quality
X = data.drop('quality', axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=123,
                                                    stratify=y)

# 5. Declare data preprocessing steps
pipeline = make_pipeline(preprocessing.StandardScaler(),
                         RandomForestRegressor(n_estimators=100))

# 6. Declare hyperparameters to tune
hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],
                  'randomforestregressor__max_depth': [None, 5, 3, 1]}

# 7. Tune model using cross-validation pipeline
clf = GridSearchCV(pipeline, hyperparameters, cv=10)
clf.fit(X_train, y_train)
# 8. Refit on the entire training set
# No additional code needed if clf.refit == True (default is True)
# 9. Evaluate model pipeline on test data
pred = clf.predict(X_test)
print r2_score(y_test, pred)
print mean_squared_error(y_test, pred)
# 10. Save model for future use
joblib.dump(clf, 'rf_regressor.pkl')
# To load: clf2 = joblib.load('rf_regressor.pkl')

#####################################################

https://elitedatascience.com/imbalanced-classes


import pandas as pd
import numpy as np

# Read dataset
df = pd.read_csv('balance-scale.data',
                 names=['balance', 'var1', 'var2', 'var3', 'var4'])

# Display example observations
df.head()
df['balance'].value_counts()
# R    288
# L    288
# B     49
# Name: balance, dtype: int64
# Transform into binary classification
df['balance'] = [1 if b=='B' else 0 for b in df.balance]
df['balance'].value_counts()
# 0    576
# 1     49
# Name: balance, dtype: int64
# About 8% were balanced




from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Separate input features (X) and target variable (y)
y = df.balance
X = df.drop('balance', axis=1)
# Train model
clf_0 = LogisticRegression().fit(X, y)
# Predict on training set
pred_y_0 = clf_0.predict(X)
# How's the accuracy?
print( accuracy_score(pred_y_0, y) )
# 0.9216
# Should we be excited?
print( np.unique( pred_y_0 ) )
# [0]




from sklearn.utils import resample

# Separate majority and minority classes
df_majority = df[df.balance==0]
df_minority = df[df.balance==1]
# Upsample minority class
df_minority_upsampled = resample(df_minority,
                                 replace=True,     # sample with replacement
                                 n_samples=576,    # to match majority class
                                 random_state=123) # reproducible results

# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_majority, df_minority_upsampled])
# Display new class counts
df_upsampled.balance.value_counts()
# 1    576
# 0    576
# Name: balance, dtype: int64
# Separate input features (X) and target variable (y)
y = df_upsampled.balance
X = df_upsampled.drop('balance', axis=1)
# Train model
clf_1 = LogisticRegression().fit(X, y)
# Predict on training set
pred_y_1 = clf_1.predict(X)
# Is our model still predicting just one class?
print( np.unique( pred_y_1 ) )
# [0 1]
# How's our accuracy?
print( accuracy_score(y, pred_y_1) )
# 0.513888888889
# Separate majority and minority classes
df_majority = df[df.balance==0]
df_minority = df[df.balance==1]
# Downsample majority class
df_majority_downsampled = resample(df_majority,
                                 replace=False,    # sample without replacement
                                 n_samples=49,     # to match minority class
                                 random_state=123) # reproducible results

# Combine minority class with downsampled majority class
df_downsampled = pd.concat([df_majority_downsampled, df_minority])
# Display new class counts
df_downsampled.balance.value_counts()
# 1    49
# 0    49
# Name: balance, dtype: int64
# Separate input features (X) and target variable (y)
y = df_downsampled.balance
X = df_downsampled.drop('balance', axis=1)
# Train model
clf_2 = LogisticRegression().fit(X, y)
# Predict on training set
pred_y_2 = clf_2.predict(X)
# Is our model still predicting just one class?
print( np.unique( pred_y_2 ) )
# [0 1]
# How's our accuracy?
print( accuracy_score(y, pred_y_2) )
# 0.581632653061

#####################################################

from sklearn.metrics import roc_auc_score

# Predict class probabilities
prob_y_2 = clf_2.predict_proba(X)
# Keep only the positive class
prob_y_2 = [p[1] for p in prob_y_2]
prob_y_2[:5] # Example
# [0.45419197226479618,
#  0.48205962213283882,
#  0.46862327066392456,
#  0.47868378832689096,
#  0.58143856820159667]
print( roc_auc_score(y, prob_y_2) )
# 0.568096626406
prob_y_0 = clf_0.predict_proba(X)
prob_y_0 = [p[1] for p in prob_y_0]
print( roc_auc_score(y, prob_y_0) )
# 0.530718537415

#####################################################

from sklearn.svm import SVC

# Separate input features (X) and target variable (y)
y = df.balance
X = df.drop('balance', axis=1)
# Train model
clf_3 = SVC(kernel='linear',
            class_weight='balanced', # penalize
            probability=True)
clf_3.fit(X, y)
# Predict on training set
pred_y_3 = clf_3.predict(X)
# Is our model still predicting just one class?
print( np.unique( pred_y_3 ) )
# [0 1]
# How's our accuracy?
print( accuracy_score(y, pred_y_3) )
# 0.688
# What about AUROC?
prob_y_3 = clf_3.predict_proba(X)
prob_y_3 = [p[1] for p in prob_y_3]
print( roc_auc_score(y, prob_y_3) )
# 0.5305236678


#####################################################


from sklearn.ensemble import RandomForestClassifier

# Separate input features (X) and target variable (y)
y = df.balance
X = df.drop('balance', axis=1)

# Train model
clf_4 = RandomForestClassifier()
clf_4.fit(X, y)

# Predict on training set
pred_y_4 = clf_4.predict(X)

# Is our model still predicting just one class?
print( np.unique( pred_y_4 ) )
# [0 1]

# How's our accuracy?
print( accuracy_score(y, pred_y_4) )
# 0.9744

# What about AUROC?
prob_y_4 = clf_4.predict_proba(X)
prob_y_4 = [p[1] for p in prob_y_4]
print( roc_auc_score(y, prob_y_4) )
# 0.999078798186




#####################################################
#
#   Getting Started with NLTK
#
#####################################################

http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk

Install Setuptools: http://pypi.python.org/pypi/setuptools
Install Pip: run sudo easy_install pip
Install Numpy (optional): run sudo pip install -U numpy
Install PyYAML and NLTK: run sudo pip install -U pyyaml nltk
Test installation: run python then type import nltk

>>> import nltk
>>> nltk.download()

Hit Enter to continue:
....
Downloader> d
Identifier> all
Downloader> u

# https://github.com/nltk/nltk_data
# https://github.com/nltk/nltk_data/tree/master

Test1 NLTK

>> from nltk.corpus import brown
>>> brown.words()[0:10]
['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']
>>> brown.tagged_words()[0:10]
[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]
>>> len(brown.words())
1161192
>>> dir(brown)


Test2 NLTK Book Resources:

>>> from nltk.book import *
>>> dir(text1)
>>> len(text1)


Test3) Tokenize , Word Tokenize and Pos Tagging

>>> from nltk import sent_tokenize, word_tokenize, pos_tag
>>> text = "Machine learning is the science of getting computers to act without being explicitly programmed."
>>> sents = sent_tokenize(text)
>>> sents
>>> len(sents)
>>> tokens = word_tokenize(text)
>>> tokens
>>> len(tokens)
>>> tagged_tokens = pos_tag(tokens)
>>> tagged_tokens


#####################################################
#
https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk
https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk
#
#####################################################

>>> python -c "import nltk"
>>> python -c "import nltk; print(nltk.__version__)"
>>> pip install nltk
>>> python -m nltk.downloader twitter_samples
>>> python -m nltk.downloader averaged_perceptron_tagger

#twitter_samples.fileids()
#twitter_samples.strings('tweets.20150430-223406.json')

# Import data and tagger
from nltk.corpus import twitter_samples
from nltk.tag import pos_tag_sents

# Load tokenized tweets
tweets_tokens = twitter_samples.tokenized('positive_tweets.json')

# Tag tagged tweets
tweets_tagged = pos_tag_sents(tweets_tokens)

# Set accumulators
JJ_count = 0
NN_count = 0

# Loop through list of tweets
for tweet in tweets_tagged:
    for pair in tweet:
        tag = pair[1]
        if tag == 'JJ':
            JJ_count += 1
        elif tag == 'NN':
            NN_count += 1

# Print total numbers for each adjectives and nouns
print('Total number of adjectives = ', JJ_count)
print('Total number of nouns = ', NN_count)


#####################################################
#
http://update.hanser-fachbuch.de/2013/09/artikelreihe-python-3-nltk-natural-language-toolkit/
#
#####################################################

>>> import nltk
>>> nltk.download()
>>> from nltk.book import *
>>> text1
>>> len(text1)
>>> words = set(text1)
>>> len(words)
>>> words = [word for word in words if word.isalpha()] # # removing words containing special characters
>>> len(words)
>>> words = set(text1)
>>> non_words = [word for word in words if not word.isalpha()]
>>> non_words
>>> len(non_words)


>>> words = list(text1)
>>> words = [word for word in words if word.isalpha()]
>>> diff_words = set(words)
>>> diversity = len(diff_words) / float(len(words))
>>> diversity

# //////////  All in a function //////////////

from __future__ import division
from nltk.book import *

for t in range(1,10):
    name = "text" + str(t)
    words = list(eval(name))
    # removing words containing or consisting of special characters
    words = [word for word in words if word.isalpha()]

    diff_words = set(words)

    num_words = len(words)
    num_diff_words = len(diff_words)

    diversity = num_diff_words / num_words

    print(name + ":", eval(name))
    print("different words:    {0:8d}".format(num_diff_words))
    print("words:              {0:8d}".format(num_words))
    print("lexical diversity:  {0:8.2f}".format(diversity))



>>> text1.generate()
>>> text1.generate(50)
>>> text4.generate()

>>> fdist1 = FreqDist(text3)
>>> frequencies = fdist1.items()
>>> frequencies[:30]
>>> frequencies[-30:]

>>> [str(w[0]) for w in frequencies[:30]]
>>> [str(w[0]) for w in frequencies[-30:]]
>>> fdist1.plot(50, cumulative=True)


# /////////////// classify ///////////////////

import nltk
import random

def gender_features(word):
    return {'last_letter': word[-1]}

def classify(name):
    return classifier.classify(gender_features(name))

male_names     = nltk.corpus.names.words('male.txt')
female_names   = nltk.corpus.names.words('female.txt')
labelled_names = ([(name, 'male') for name in male_names] +
[(name, 'female') for name in female_names])

random.shuffle(labelled_names)

featuresets = [(gender_features(n), g) for (n,g) in labelled_names]
train_set  = featuresets[500:]
test_set   = featuresets[:500]

classifier = nltk.NaiveBayesClassifier.train(train_set)





>>> from names_classifier import *
>>> for name in ["Charlotte", "Amelia", "Benjamin", "Lucas", "Obama"]:
...      print(name, classify(name))
...
('Charlotte', 'female')
('Amelia', 'female')
('Benjamin', 'male')
('Lucas', 'male')
('Obama', 'female')
>>> print(nltk.classify.accuracy(classifier, test_set))
0.764
>>> classifier.show_most_informative_features(7)





#####################################################
#
#   https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/
#
#####################################################


from nltk.tokenize import sent_tokenize, word_tokenize

EXAMPLE_TEXT = "Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard."

print(sent_tokenize(EXAMPLE_TEXT))
print(word_tokenize(EXAMPLE_TEXT))

Now our output is: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', "n't", 'eat', 'cardboard', '.']

#####################################################
#
#   https://dzone.com/articles/natural-language-toolkit-nltk
#
#####################################################

from nltk.book import *
from __future__ import division

 from nltk.book import *

 #Enter their names to find out about these texts
 print text3
 #Length of a text from start to finish, in terms of the words and punctuation symbols that appear.
 print 'Length of Text: '+str(len(text3))
 #Text is just the set of tokens
 #print sorted(set(text3))
 print 'Length of Token: '+str(len(set(text3)))
 #lexical richness of the text
 def lexical_richness(text):
     return len(set(text)) / len(text)

 #percentage of the text is taken up by a specific word
 def percentage(word, text):
     return (100 * text.count(word) / len(text))

 print 'Lexical richness of the text: '+str(lexical_richness(text3))
 print 'Percentage: '+ str(percentage('God',text3));

 #count the word in the Text
 print "===Count==="
 print text3.count("Adam")
 #'concordance()' view shows us every occurrence of a given word, together with some context.
 #Here 'Adam' search in 'The Book of Genesis'
 print "===Concordance==="
 print text3.concordance("Adam")
 #Appending the term similar to the name of the text
 print "===Similar==="
 print text3.similar("Adam")
 #Contexts are shared by two or more words
 print "===Common Contexts==="
 text3.common_contexts(["Adam", "Noah"])
text3.dispersion_plot(["God","Adam", "Eve", "Noah", "Abram","Sarah", "Joseph", "Shem", "Isaac"])

#####################################################
#
#   https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words
#
#####################################################

# Import the pandas package, then use the "read_csv" function to read
# the labeled training data
import pandas as pd
train = pd.read_csv("labeledTrainData.tsv", header=0, \
                    delimiter="\t", quoting=3)

>>> train.shape
(25000, 3)

>>> train.columns.values
array([id, sentiment, review], dtype=object)
print train["review"][0]


sudo pip install BeautifulSoup4

# Import BeautifulSoup into your workspace
from bs4 import BeautifulSoup

# Initialize the BeautifulSoup object on a single movie review
example1 = BeautifulSoup(train["review"][0])

# Print the raw review and then the output of get_text(), for
# comparison
print train["review"][0]
print example1.get_text()

import re
# Use regular expressions to do a find-and-replace
letters_only = re.sub("[^a-zA-Z]",           # The pattern to search for
                      " ",                   # The pattern to replace it with
                      example1.get_text() )  # The text to search
print letters_only

lower_case = letters_only.lower()        # Convert to lower case
words = lower_case.split()               # Split into words

import nltk
nltk.download()  # Download text data sets, including stop words

from nltk.corpus import stopwords # Import the stop word list
print stopwords.words("english"

# Remove stop words from "words"
words = [w for w in words if not w in stopwords.words("english")]
print words



# ////////////// Putting it all together ///////////////

def review_to_words( raw_review ):
    # Function to convert a raw review to a string of words
    # The input is a single string (a raw movie review), and
    # the output is a single string (a preprocessed movie review)
    #
    # 1. Remove HTML
    review_text = BeautifulSoup(raw_review).get_text()
    #
    # 2. Remove non-letters
    letters_only = re.sub("[^a-zA-Z]", " ", review_text)
    #
    # 3. Convert to lower case, split into individual words
    words = letters_only.lower().split()
    #
    # 4. In Python, searching a set is much faster than searching
    #   a list, so convert the stop words to a set
    stops = set(stopwords.words("english"))
    #
    # 5. Remove stop words
    meaningful_words = [w for w in words if not w in stops]
    #
    # 6. Join the words back into one string separated by space,
    # and return the result.
    return( " ".join( meaningful_words ))


clean_review = review_to_words( train["review"][0] )
print clean_review
# Get the number of reviews based on the dataframe column size
num_reviews = train["review"].size

# Initialize an empty list to hold the clean reviews
clean_train_reviews = []

# Loop over each review; create an index i that goes from 0 to the length
# of the movie review list
for i in xrange( 0, num_reviews ):
    # Call our function for each one, and add the result to the list of
    # clean reviews
    clean_train_reviews.append( review_to_words( train["review"][i] ) )


print "Cleaning and parsing the training set movie reviews...\n"
clean_train_reviews = []
for i in xrange( 0, num_reviews ):
    # If the index is evenly divisible by 1000, print a message
    if( (i+1)%1000 == 0 ):
        print "Review %d of %d\n" % ( i+1, num_reviews )
    clean_train_reviews.append( review_to_words( train["review"][i] ))


print "Creating the bag of words...\n"
from sklearn.feature_extraction.text import CountVectorizer

# Initialize the "CountVectorizer" object, which is scikit-learn's
# bag of words tool.
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 5000)

# fit_transform() does two functions: First, it fits the model
# and learns the vocabulary; second, it transforms our training data
# into feature vectors. The input to fit_transform should be a list of
# strings.
train_data_features = vectorizer.fit_transform(clean_train_reviews)

# Numpy arrays are easy to work with, so convert the result to an
# array
train_data_features = train_data_features.toarray()

>>> print train_data_features.shape
(25000, 5000)

# Take a look at the words in the vocabulary
vocab = vectorizer.get_feature_names()
print vocab

import numpy as np

# Sum up the counts of each vocabulary word
dist = np.sum(train_data_features, axis=0)

# For each, print the vocabulary word and the number of times it
# appears in the training set
for tag, count in zip(vocab, dist):
    print count, tag


print "Training the random forest..."
from sklearn.ensemble import RandomForestClassifier

# Initialize a Random Forest classifier with 100 trees
forest = RandomForestClassifier(n_estimators = 100)

# Fit the forest to the training set, using the bag of words as
# features and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit( train_data_features, train["sentiment"] )


#####################################################
#
# Creating a Submission
#
#####################################################

# Read the test data
test = pd.read_csv("testData.tsv", header=0, delimiter="\t", \
                   quoting=3 )

# Verify that there are 25,000 rows and 2 columns
print test.shape

# Create an empty list and append the clean reviews one by one
num_reviews = len(test["review"])
clean_test_reviews = []

print "Cleaning and parsing the test set movie reviews...\n"
for i in xrange(0,num_reviews):
    if( (i+1) % 1000 == 0 ):
        print "Review %d of %d\n" % (i+1, num_reviews)
    clean_review = review_to_words( test["review"][i] )
    clean_test_reviews.append( clean_review )

# Get a bag of words for the test set, and convert to a numpy array
test_data_features = vectorizer.transform(clean_test_reviews)
test_data_features = test_data_features.toarray()

# Use the random forest to make sentiment label predictions
result = forest.predict(test_data_features)

# Copy the results to a pandas dataframe with an "id" column and
# a "sentiment" column
output = pd.DataFrame( data={"id":test["id"], "sentiment":result} )

# Use pandas to write the comma-separated output file
output.to_csv( "Bag_of_Words_model.csv", index=False, quoting=3 )

----------------------------
http://www.nltk.org/howto/classify.html

>>> train = [
...     (dict(a=1,b=1,c=1), 'y'),
...     (dict(a=1,b=1,c=1), 'x'),
...     (dict(a=1,b=1,c=0), 'y'),
...     (dict(a=0,b=1,c=1), 'x'),
...     (dict(a=0,b=1,c=1), 'y'),
...     (dict(a=0,b=0,c=1), 'y'),
...     (dict(a=0,b=1,c=0), 'x'),
...     (dict(a=0,b=0,c=0), 'x'),
...     (dict(a=0,b=1,c=1), 'y'),
...     ]
>>> test = [
...     (dict(a=1,b=0,c=1)), # unseen
...     (dict(a=1,b=0,c=0)), # unseen
...     (dict(a=0,b=1,c=1)), # seen 3 times, labels=y,y,x
...     (dict(a=0,b=1,c=0)), # seen 1 time, label=x
...     ]


>>> classifier = nltk.classify.NaiveBayesClassifier.train(train)
>>> sorted(classifier.labels())
['x', 'y']
>>> classifier.classify_many(test)
['y', 'x', 'y', 'x']
>>> for pdist in classifier.prob_classify_many(test):
...     print('%.4f %.4f' % (pdist.prob('x'), pdist.prob('y')))
0.3203 0.6797
0.5857 0.4143
0.3792 0.6208


>>> from nltk.classify import SklearnClassifier
>>> from sklearn.naive_bayes import BernoulliNB
>>> from sklearn.svm import SVC
>>> train_data = [({"a": 4, "b": 1, "c": 0}, "ham"),
...               ({"a": 5, "b": 2, "c": 1}, "ham"),
...               ({"a": 0, "b": 3, "c": 4}, "spam"),
...               ({"a": 5, "b": 1, "c": 1}, "ham"),
...               ({"a": 1, "b": 4, "c": 3}, "spam")]
>>> classif = SklearnClassifier(BernoulliNB()).train(train_data)
>>> test_data = [{"a": 3, "b": 2, "c": 1},
...              {"a": 0, "b": 3, "c": 7}]
>>> classif.classify_many(test_data)
['ham', 'spam']
>>> classif = SklearnClassifier(SVC(), sparse=False).train(train_data)
>>> classif.classify_many(test_data)
['ham', 'spam']




http://www.nltk.org/book/ch01.html
1. Language Processing and Python

http://www.nltk.org/book/ch02.html
2. Accessing Text Corpora and Lexical Resources

http://www.nltk.org/book/ch03.html
3   Processing Raw Text

>>> from urllib import request
>>> url = "http://www.gutenberg.org/files/2554/2554.txt"
>>> response = request.urlopen(url)
>>> raw = response.read().decode('utf8')
>>> type(raw)
<class 'str'>
>>> len(raw)
1176893
>>> raw[:75]
>>> tokens = word_tokenize(raw)
>>> type(tokens)
<class 'list'>
>>> len(tokens)
254354
>>> tokens[:10]
>>> text = nltk.Text(tokens)
>>> type(text)
<class 'nltk.text.Text'>
>>> text[1024:1062]
>>> text.collocations()
>>> raw.find("PART I")
5338
>>> raw.rfind("End of Project Gutenberg's Crime")
1157743
>>> raw = raw[5338:1157743] [1]
>>> raw.find("PART I")
0

############## Dealing with HTML

>>> url = "http://news.bbc.co.uk/2/hi/health/2284783.stm"
>>> html = request.urlopen(url).read().decode('utf8')
>>> html[:60]
>>> from bs4 import BeautifulSoup
>>> raw = BeautifulSoup(html).get_text()
>>> tokens = word_tokenize(raw)
>>> tokens
['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', "'to", 'die', 'out', ...]
>>> tokens = tokens[110:390]
>>> text = nltk.Text(tokens)
>>> text.concordance('gene')

######################## Processing RSS Feeds



>>> import feedparser
>>> llog = feedparser.parse("http://languagelog.ldc.upenn.edu/nll/?feed=atom")
>>> llog['feed']['title']
'Language Log'
>>> len(llog.entries)
15
>>> post = llog.entries[2]
>>> post.title
"He's My BF"
>>> content = post.content[0].value
>>> content[:70]
'<p>Today I was chatting with three of our visiting graduate students f'
>>> raw = BeautifulSoup(content).get_text()
>>> word_tokenize(raw)

################## Reading Local Files

>>> f = open('document.txt')
>>> raw = f.read()

>>> import os
>>> os.listdir('.')
>>> f.read()
>>> f = open('document.txt', 'rU')
>>> for line in f:
...     print(line.strip())
Time flies like an arrow.
>>> path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')
>>> raw = open(path, 'rU').read()




>>> from nltk.corpus import gutenberg
>>> raw = gutenberg.raw('melville-moby_dick.txt')
>>> fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())
>>> fdist.most_common(5)
[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]
>>> [char for (char, count) in fdist.most_common()]
['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',
'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']




Method 	Functionality
s.find(t) 	index of first instance of string t inside s (-1 if not found)
s.rfind(t) 	index of last instance of string t inside s (-1 if not found)
s.index(t) 	like s.find(t) except it raises ValueError if not found
s.rindex(t) 	like s.rfind(t) except it raises ValueError if not found
s.join(text) 	combine the words of the text into a string using s as the glue
s.split(t) 	split s into a list wherever a t is found (whitespace by default)
s.splitlines() 	split s into a list of strings, one per line
s.lower() 	a lowercased version of the string s
s.upper() 	an uppercased version of the string s
s.title() 	a titlecased version of the string s
s.strip() 	a copy of s without leading or trailing whitespace
s.replace(t, u) 	replace instances of t with u inside s



################# Encoding

>>> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')
>>> f = open(path, encoding='latin2')
>>> for line in f:
...    line = line.strip()
...    print(line)
...    print(line.encode('unicode_escape'))

####################  Regex

>>> from nltk.corpus import gutenberg, nps_chat
>>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))
>>> moby.findall(r"<a> (<.*>) <man>") [1]
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> chat = nltk.Text(nps_chat.words())
>>> chat.findall(r"<.*> <.*> <bro>") [2]
you rule bro; telling you bro; u twizted bro
>>> chat.findall(r"<l.*>{3,}") [3]
lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la
la la; lovely lol lol love; lol lol lol.; la la la; la la la




>>> from nltk.corpus import brown
>>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))
>>> hobbies_learned.findall(r"<\w*> <and> <other> <\w*s>")

################### Lining Things Up

def tabulate(cfdist, words, categories):
    print('{:16}'.format('Category'), end=' ')                    # column headings
    for word in words:
        print('{:>6}'.format(word), end=' ')
    print()
    for category in categories:
        print('{:16}'.format(category), end=' ')                  # row heading
        for word in words:                                        # for each word
            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell
        print()                                                   # end the row

>>> from nltk.corpus import brown
>>> cfd = nltk.ConditionalFreqDist(
...           (genre, word)
...           for genre in brown.categories()
...           for word in brown.words(categories=genre))
>>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> tabulate(cfd, modals, genres)
Category            can  could    may  might   must   will
news                 93     86     66     38     50    389
religion             82     59     78     12     54     71
hobbies             268     58    131     22     83    264
science_fiction      16     49      4     12      8     16
romance              74    193     11     51     45     43
humor                16     30      8      8      9     13

/////////////////////// Writing Results to a File ///////////////////////


>>> output_file = open('output.txt', 'w')
>>> words = set(nltk.corpus.genesis.words('english-kjv.txt'))
>>> for word in sorted(words):
...     print(word, file=output_file)

http://www.nltk.org/book/ch04.html

/////////////////////// Generator Expressions ///////////////////////

>>> text = '''"When I use a word," Humpty Dumpty said in rather a scornful tone,
... "it means just what I choose it to mean - neither more nor less."'''
>>> [w.lower() for w in word_tokenize(text)]
['``', 'when', 'i', 'use', 'a', 'word', ',', "''", 'humpty', 'dumpty', 'said', ...]

/////////////////////////////////////////////////////////////////////

>>> fd = nltk.FreqDist(nltk.corpus.brown.words())
>>> cumulative = 0.0
>>> most_common_words = [word for (word, count) in fd.most_common()]
>>> for rank, word in enumerate(most_common_words):
...     cumulative += fd.freq(word)
...     print("%3d %6.2f%% %s" % (rank + 1, cumulative * 100, word))
...     if cumulative > 0.25:
...         break
...
  1   5.40% the
  2  10.42% ,
  3  14.67% .
  4  17.78% of
  5  20.19% and
  6  22.40% to
  7  24.29% a
  8  25.97% in




>>> text = nltk.corpus.gutenberg.words('milton-paradise.txt')
>>> longest = ''
>>> for word in text:
...     if len(word) > len(longest):
...         longest = word
>>> longest
'unextinguishable'


>>> maxlen = max(len(word) for word in text)
>>> [word for word in text if len(word) == maxlen]
['unextinguishable', 'transubstantiate', 'inextinguishable', 'incomprehensible']


############ Some Legitimate Uses for Counters


>>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
>>> n = 3
>>> [sent[i:i+n] for i in range(len(sent)-n+1)]
[['The', 'dog', 'gave'],
 ['dog', 'gave', 'John'],
 ['gave', 'John', 'the'],
 ['John', 'the', 'newspaper']]


>>> m, n = 3, 7
>>> array = [[set() for i in range(n)] for j in range(m)]
>>> array[2][5].add('Alice')
>>> pprint.pprint(array)
[[set(), set(), set(), set(), set(), set(), set()],
 [set(), set(), set(), set(), set(), set(), set()],
 [set(), set(), set(), set(), set(), {'Alice'}, set()]]


>>> array = [[set()] * n] * m
>>> array[2][5].add(7)
>>> pprint.pprint(array)
[[{7}, {7}, {7}, {7}, {7}, {7}, {7}],
 [{7}, {7}, {7}, {7}, {7}, {7}, {7}],
 [{7}, {7}, {7}, {7}, {7}, {7}, {7}]]

/////////////////////// freq words ///////////////////////

from urllib import request
from bs4 import BeautifulSoup

def freq_words(url, freqdist, n):
    html = request.urlopen(url).read().decode('utf8')
    raw = BeautifulSoup(html).get_text()
    for word in word_tokenize(raw):
        freqdist[word.lower()] += 1
    result = []
    for word, count in freqdist.most_common(n):
        result = result + [word]
    print(result)



>>> constitution = "http://www.archives.gov/exhibits/charters/constitution_transcript.html"
>>> fd = nltk.FreqDist()
>>> freq_words(constitution, fd, 30)
['the', ',', 'of', 'and', 'shall', '.', 'be', 'to', ';', 'in', 'states','or', 'united', 'a', 'state', 'by', 'for', 'any', '=', 'which', 'president','all', 'on', 'may', 'such', 'as', 'have', ')', '(', 'congress']

/////////////////////// freq words 2 ///////////////////////

from urllib import request
from bs4 import BeautifulSoup

def freq_words(url, n):
    html = request.urlopen(url).read().decode('utf8')
    text = BeautifulSoup(html).get_text()
    freqdist = nltk.FreqDist(word.lower() for word in word_tokenize(text))
    return [word for (word, _) in fd.most_common(n)]



>>> freq_words(constitution, 30)
['the', ',', 'of', 'and', 'shall', '.', 'be', 'to', ';', 'in', 'states','or', 'united', 'a', 'state', 'by', 'for', 'any', '=', 'which', 'president','all', 'on', 'may', 'such', 'as', 'have', ')', '(', 'congress']







>>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the','sounds', 'will', 'take', 'care', 'of', 'themselves', '.']
>>> sorted(sent, lambda x, y: cmp(len(y), len(x)))
['themselves', 'sounds', 'sense', 'Take', 'care', 'will', 'take', 'care',
'the', 'and', 'the', 'of', 'of', ',', '.']


/////////////////////// permutations ///////////////////////

>>> def permutations(seq):
...     if len(seq) <= 1:
...         yield seq
...     else:
...         for perm in permutations(seq[1:]):
...             for i in range(len(perm)+1):
...                 yield perm[:i] + seq[0:1] + perm[i:]
...
>>> list(permutations(['police', 'fish', 'buffalo'])) [1]
[['police', 'fish', 'buffalo'], ['fish', 'police', 'buffalo'],
 ['fish', 'buffalo', 'police'], ['police', 'buffalo', 'fish'],
 ['buffalo', 'police', 'fish'], ['buffalo', 'fish', 'police']]



>>> with open("lexicon.txt") as f:
...     data = f.read()
...     # process the data



/////////////////////// insert elements ///////////////////////

def insert(trie, key, value):
    if key:
        first, rest = key[0], key[1:]
        if first not in trie:
            trie[first] = {}
        insert(trie[first], rest, value)
    else:
        trie['value'] = value



>>> trie = {}
>>> insert(trie, 'chat', 'cat')
>>> insert(trie, 'chien', 'dog')
>>> insert(trie, 'chair', 'flesh')
>>> insert(trie, 'chic', 'stylish')
>>> trie = dict(trie)               # for nicer printing
>>> trie['c']['h']['a']['t']['value']
'cat'
>>> pprint.pprint(trie, width=40)
{'c': {'h': {'a': {'t': {'value': 'cat'}},
                  {'i': {'r': {'value': 'flesh'}}},
             'i': {'e': {'n': {'value': 'dog'}}}
                  {'c': {'value': 'stylish'}}}}}



/////////////////////// Charts ///////////////////////

from numpy import arange
from matplotlib import pyplot

colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black

def bar_chart(categories, words, counts):
    "Plot a bar chart showing counts for each word by category"
    ind = arange(len(words))
    width = 1 / (len(categories) + 1)
    bar_groups = []
    for c in range(len(categories)):
        bars = pyplot.bar(ind+c*width, counts[categories[c]], width,
                         color=colors[c % len(colors)])
        bar_groups.append(bars)
    pyplot.xticks(ind+width, words)
    pyplot.legend([b[0] for b in bar_groups], categories, loc='upper left')
    pyplot.ylabel('Frequency')
    pyplot.title('Frequency of Six Modal Verbs by Genre')
    pyplot.show()



>>> genres = ['news', 'religion', 'hobbies', 'government', 'adventure']
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> cfdist = nltk.ConditionalFreqDist(
...              (genre, word)
...              for genre in genres
...              for word in nltk.corpus.brown.words(categories=genre)
...              if word in modals)
...
>>> counts = {}
>>> for genre in genres:
...     counts[genre] = [cfdist[genre][word] for word in modals]
>>> bar_chart(genres, modals, counts)

/////////////////////// NetworkX ///////////////////////

import networkx as nx
import matplotlib
from nltk.corpus import wordnet as wn

def traverse(graph, start, node):
    graph.depth[node.name] = node.shortest_path_distance(start)
    for child in node.hyponyms():
        graph.add_edge(node.name, child.name) [1]
        traverse(graph, start, child) [2]

def hyponym_graph(start):
    G = nx.Graph() [3]
    G.depth = {}
    traverse(G, start, start)
    return G

def graph_draw(graph):
    nx.draw_graphviz(graph,
         node_size = [16 * graph.degree(n) for n in graph],
         node_color = [graph.depth[n] for n in graph],
         with_labels = False)
    matplotlib.pyplot.show()



>>> dog = wn.synset('dog.n.01')
>>> graph = hyponym_graph(dog)
>>> graph_draw(graph)


/////////////////////// csv ///////////////////////

>>> import csv
>>> input_file = open("lexicon.csv", "rb") [1]
>>> for row in csv.reader(input_file): [2]
...     print(row)
['sleep', 'sli:p', 'v.i', 'a condition of body and mind ...']
['walk', 'wo:k', 'v.intr', 'progress by lifting and setting down each foot ...']
['wake', 'weik', 'intrans', 'cease to sleep']


http://www.nltk.org/book/ch05.html

/////////////////////// Using a Tagger ///////////////////////

>>> text = word_tokenize("And now for something completely different")
>>> nltk.pos_tag(text)
[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),
('completely', 'RB'), ('different', 'JJ')]

/////////////////////// similar ///////////////////////

>>> text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())
>>> text.similar('woman')
Building word-context index...
man day time year car moment world family house boy child country job
state girl place war way case question
>>> text.similar('bought')
made done put said found had seen given left heard been brought got
set was called felt in that told
>>> text.similar('over')
in on to of and for with from at by that into as up out down through
about all is
>>> text.similar('the')
a his this their its her an that our any all one these my in your no
some other and


/////////////////////// sort ///////////////////////

>>> list(pos) [1]
['ideas', 'furiously', 'colorless', 'sleep']
>>> sorted(pos) [2]
['colorless', 'furiously', 'ideas', 'sleep']
>>> [w for w in pos if w.endswith('s')] [3]
['colorless', 'ideas']

/////////////////////// shuffle ///////////////////////

>>> from nltk.corpus import names
>>> labeled_names = ([(name, 'male') for name in names.words('male.txt')] +
... [(name, 'female') for name in names.words('female.txt')])
>>> import random
>>> random.shuffle(labeled_names)


///////////////////////  Document Classification ///////////////////////


>>> from nltk.corpus import movie_reviews
>>> documents = [(list(movie_reviews.words(fileid)), category)
...              for category in movie_reviews.categories()
...              for fileid in movie_reviews.fileids(category)]
>>> random.shuffle(documents)

all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000] [1]

def document_features(document): [2]
    document_words = set(document) [3]
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features



>>> print(document_features(movie_reviews.words('pos/cv957_8737.txt')))
{'contains(waste)': False, 'contains(lot)': False, ...}



featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = nltk.NaiveBayesClassifier.train(train_set)



>>> print(nltk.classify.accuracy(classifier, test_set)) [1]
0.81
>>> classifier.show_most_informative_features(5) [2]
Most Informative Features
   contains(outstanding) = True              pos : neg    =     11.1 : 1.0
        contains(seagal) = True              neg : pos    =      7.7 : 1.0
   contains(wonderfully) = True              pos : neg    =      6.8 : 1.0
         contains(damon) = True              pos : neg    =      5.9 : 1.0
        contains(wasted) = True              neg : pos    =      5.8 : 1.0




http://www.nltk.org/book/ch06.html



import math
def entropy(labels):
    freqdist = nltk.FreqDist(labels)
    probs = [freqdist.freq(l) for l in freqdist]
    return -sum(p * math.log(p,2) for p in probs)
>>> print(entropy(['male', 'male', 'male', 'male']))
0.0
>>> print(entropy(['male', 'female', 'male', 'male']))
0.811...
>>> print(entropy(['female', 'male', 'female', 'male']))
1.0
>>> print(entropy(['female', 'female', 'male', 'female']))
0.811...
>>> print(entropy(['female', 'female', 'female', 'female']))
0.0


/////////////////////// print tree ///////////////////////

>>> sentence = [("the", "DT"), ("little", "JJ"), ("yellow", "JJ"), [1]
... ("dog", "NN"), ("barked", "VBD"), ("at", "IN"),  ("the", "DT"), ("cat", "NN")]

>>> grammar = "NP: {<DT>?<JJ>*<NN>}" [2]

>>> cp = nltk.RegexpParser(grammar) [3]
>>> result = cp.parse(sentence) [4]
>>> print(result) [5]
(S
  (NP the/DT little/JJ yellow/JJ dog/NN)
  barked/VBD
  at/IN
  (NP the/DT cat/NN))
>>> result.draw() [6]


http://www.nltk.org/book/ch07.html

Recursion in Linguistic Structure

grammar = r"""
  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN
  PP: {<IN><NP>}               # Chunk prepositions followed by NP
  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments
  CLAUSE: {<NP><VP>}           # Chunk NP, VP
  """
cp = nltk.RegexpParser(grammar)
sentence = [("Mary", "NN"), ("saw", "VBD"), ("the", "DT"), ("cat", "NN"),
    ("sit", "VB"), ("on", "IN"), ("the", "DT"), ("mat", "NN")]



>>> print(cp.parse(sentence))

/////////////////////// Trees ///////////////////////


>>> tree1 = nltk.Tree('NP', ['Alice'])
>>> print(tree1)
(NP Alice)
>>> tree2 = nltk.Tree('NP', ['the', 'rabbit'])
>>> print(tree2)
(NP the rabbit)
>>> tree3 = nltk.Tree('VP', ['chased', tree2])
>>> tree4 = nltk.Tree('S', [tree1, tree3])
>>> print(tree4)
(S (NP Alice) (VP chased (NP the rabbit)))
>>> print(tree4[1])
(VP chased (NP the rabbit))
>>> tree4[1].label()
'VP'
>>> tree4.leaves()
['Alice', 'chased', 'the', 'rabbit']
>>> tree4[1][1][1]
'rabbit'
>>> tree3.draw()



>>> sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']
>>> parser = nltk.ChartParser(groucho_grammar)
>>> for tree in parser.parse(sent):
...     print(tree)

http://www.nltk.org/book/ch08.html

>>> rd_parser = nltk.RecursiveDescentParser(grammar1)
>>> sent = 'Mary saw a dog'.split()
>>> for tree in rd_parser.parse(sent):
...     print(tree)
(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))


>>> sr_parser = nltk.ShiftReduceParser(grammar1)
>>> sent = 'Mary saw a dog'.split()
>>> for tree in sr_parser.parse(sent):
...     print(tree)
  (S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))


>>> from nltk.corpus import treebank
>>> t = treebank.parsed_sents('wsj_0001.mrg')[0]
>>> print(t)


////////////////////////////////////////////////////////////////////////////////////////////

>>> from nltk import load_parser
>>> cp = load_parser('grammars/book_grammars/sql0.fcfg')
>>> query = 'What cities are located in China'
>>> trees = list(cp.parse(query.split()))
>>> answer = trees[0].label()['SEM']
>>> answer = [s for s in answer if s]
>>> q = ' '.join(answer)
>>> print(q)
SELECT City FROM city_table WHERE Country="china"
>>> from nltk.sem import chat80
>>> rows = chat80.sql_query('corpora/city_database/city.db', q)
>>> for r in rows: print(r[0], end=" ") [1]
canton chungking dairen harbin kowloon mukden peking shanghai sian tientsin


#####################################################
#
#   datasets
#
#####################################################

https://grouplens.org/datasets/movielens/
https://www.kaggle.com/c/msdchallenge#description
http://www.ieor.berkeley.edu/~goldberg/jester-data/
https://gist.github.com/entaroadun/1653794
https://dev.twitter.com/streaming/overview
https://stocktwits.com/developers/docs
https://www.wunderground.com/weather/api/
https://github.com/niderhoff/nlp-datasets
http://qwone.com/~jason/20Newsgroups/
https://snap.stanford.edu/data/web-Amazon.html
https://www.cs.cmu.edu/~./enron/
http://deeplearning.net/datasets/
https://deeplearning4j.org/opendata

http://yann.lecun.com/exdb/mnist/
http://image-net.org
https://research.google.com/youtube8m/explore.html
https://archive.ics.uci.edu/ml/datasets/Wine+Quality
https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients
https://archive.ics.uci.edu/ml/datasets/US+Census+Data+%281990%29
https://www.kaggle.com/datasets
https://www.reddit.com/r/datasets/top/?sort=top&t=all
https://www.reddit.com/r/datasets/comments/3rih8y/i_have_listed_every_publicly_available_open_data/
https://www.opendatasoft.com/a-comprehensive-list-of-all-open-data-portals-around-the-world/
https://www.opendatasoft.com/2015/11/02/how-we-put-together-a-list-of-1600-open-data-portals-around-the-world-to-help-open-data-community/
https://www.crowdbabble.com/blog/the-11-best-tweets-of-all-time-by-donald-trump/
https://www.crowdbabble.com/wp-content/uploads/2016/11/Crowdbabble_Social-Media-Analytics_Twitter-Download_Donald-Trump_7375-Tweets.csv
https://www.reddit.com/r/datasets/comments/63spoc/19gb_of_urban_dictionary_definitions_1999_may_2016/
https://aws.amazon.com/de/datasets/
https://www.dataquest.io/blog/free-datasets-for-projects/
https://medium.com/startup-grind/fueling-the-ai-gold-rush-7ae438505bc2
https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset
https://www.kaggle.com/mylesoneill/game-of-thrones

https://gist.github.com/fabianp/2020955
-------------------