
#####################################################
ollama Llama Gemma
#####################################################
https://www.youtube.com/watch?v=DYhC7nFRL5I
https://ollama.com/download
https://ollama.com/library/dolphin-llama3
https://www.llama.com/
https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/
https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md
https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
https://github.com/ollama
https://ollama.com/library
https://mistral.ai/news/mixtral-of-experts/
https://erichartford.com/uncensored-models
https://github.com/ollama/ollama

curl -fsSL https://ollama.com/install.sh | sh
ollama serve
ollama pull llama3.1:latest
ollama list
ollama run llama3.1:latest
ollama run gemma:2b

sudo docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data -name open-webui -restart always ghcr.io/open-webui/open-webui:ollama


mistral
https://mistral.ai/news/mixtral-of-experts/




dolphin-llama
https://ollama.com/hillct/dolphin-llama-3.1
https://huggingface.co/cognitivecomputations/dolphin-2.9.4-llama3.1-8b
https://www.promptlayer.com/models/dolphin-294-llama31-8b-3ac7
https://ollama.com/CognitiveComputations/dolphin-llama3.1:latest/blobs/c4e04968e3ca
https://huggingface.co/llmware/dolphin-2.9.4-llama3.1-8b-ov
https://github.com/ggerganov/llama.cpp/issues/8895

ollama pull rjmalagon/dolphin-2.9.4-llama3.1-8b:q8_0



cpu
ollama run llama2:13b
ollama run llama3:8b
ollama run mistral:7b
ollama run gemma2:9b
ollama run llava:9b
ollama run phi3:3.8b
ollama run gwen2:7b


######################################
Download and run Ollama
######################################
https://ollama.com/download
https://hub.docker.com/r/ollama/ollama
https://github.com/ollama/ollama

docker pull ollama/ollama

# CPU only
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
# Start the container
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
# Run model locally, Now you can run a model:
docker exec -it ollama ollama run llama3

https://github.com/ollama/ollama/blob/main/docs/linux.md
# Install
curl -fsSL https://ollama.com/install.sh | sh

# Manual install
curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
sudo tar -C /usr -xzf ollama-linux-amd64.tgz

ollama serve
ollama -v

sudo systemctl start ollama
sudo systemctl status ollama

ollama run deepseek-r1
solve for x:2x + 10 = 0

https://ollama.com/library
https://ollama.com/library/deepseek-r1		5gb
https://ollama.com/library/llama3.3		    43gb
https://ollama.com/library/llama3.2		    2gb
https://ollama.com/library/llama3.1		    5gb
https://ollama.com/library/llama3		    5gb
https://ollama.com/library/qwen			    2gb
https://ollama.com/library/qwen2		    4gb
https://ollama.com/library/qwen2.5		    5gb
https://ollama.com/library/phi3			    2gb
https://ollama.com/library/llama2		    4gb
https://ollama.com/library/gemma2		    5gb
https://ollama.com/library/qwen2.5-coder	5gb
https://ollama.com/library/tinyllama		600mb
https://ollama.com/library/codegemma		5gb
https://ollama.com/library/deepseek-coder	800mb
https://ollama.com/library/orca-mini		2gb
https://ollama.com/library/starcoder		2gb
https://ollama.com/library/stable-code		2gb
https://ollama.com/library/tinydolphin		600mb
https://ollama.com/library/qwen2-math		4gb
https://ollama.com/library/sqlcoder		    4gb
https://ollama.com/library/dolphin-phi		2gb
https://ollama.com/library/granite3-dense	2gb
https://ollama.com/library/granite3.1-dense	5gb
https://ollama.com/library/dolphin3		    5gb
https://ollama.com/library/smallthinker		3gb
https://ollama.com/library/reader-lm		1gb
https://ollama.com/library/falcon3		    4gb
https://ollama.com/library/granite-embedding    63mb
https://ollama.com/library/granite3-guardian   	3gb
https://ollama.com/library/sailor2  		5gb

ollama run deepseek-r1
ollama run gemma2
ollama run qwen
ollama run qwen tinyllama


#####################################################
LM Studio
#####################################################
https://www.youtube.com/watch?v=KtSdNwVkpWc

https://lmstudio.ai/
LM_Studio-0.3.5.AppImage
https://lmstudio.ai/docs
https://lmstudio.ai/download?os=win32

#####################################################
Open WebUI Channels : v0.5.3 Complete Guide
#####################################################

https://www.youtube.com/watch?v=pfxxZ7rQUJg

docker run -d -p 3000:8080 --env ENABLE_CHANNELS=true --env ENABLE_ADMIN_CHAT_ACCESS=true --env BYPASS_MODEL_ACCESS_CONTROL=true -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main


#####################################################
Desktop LLMs
#####################################################

https://anythingllm.com/desktop
https://docs.anythingllm.com/installation-desktop/linux#install-using-the-installer-script

#####################################################
Run LLMs locally - 5 Must-Know Frameworks
#####################################################

https://www.youtube.com/watch?v=5WCvGyPpWwg
Ollama
GPT4All
PrivateGPT
llama.cpp
LangChain


#####################################################
Best ChatGPT Alternatives for Everyday Use
#####################################################

https://www.youtube.com/watch?v=wwcneSVMK-A

https://www.meta.ai/
https://pi.ai/
https://copilot.microsoft.com/
https://www.phind.com/search
https://huggingface.co/cognitivecompu...
https://chat.mistral.ai/
https://poe.com/
https://gemini.google.com/
https://thegigabrain.com/
https://blogs.microsoft.com/blog/2023...
https://aistudio.google.com/app
https://chatgpt.com/
https://claude.ai/
https://www.perplexity.ai/


https://character.ai/
https://www.joyland.ai/

#####################################################
The Self-hosted AI Starter Kit
#####################################################

https://github.com/n8n-io/self-hosted-ai-starter-kit
https://docs.n8n.io/hosting/starter-kits/ai-starter-kit/
https://github.com/n8n-io/self-hosted-ai-starter-kit/blob/main/docker-compose.yml
https://hub.docker.com/r/localai/localai
https://hub.docker.com/r/elestio/flowiseai
https://community.n8n.io/c/tutorials/28

https://github.com/coleam00/ai-agents-masterclass/tree/main/local-ai-packaged
https://github.com/n8n-io/self-hosted-ai-starter-kit


#####################################################
LLM Everywhere: Docker for Local and Hugging Face Hosting
#####################################################

https://hub.docker.com/
https://www.docker.com/blog/llm-docker-for-local-and-hugging-face-hosting/
https://hub.docker.com/r/ollama/ollama

https://hub.docker.com/r/pytorch/pytorch
https://hub.docker.com/r/tensorflow/tensorflow


#####################################################
Pinokio AI Browser
#####################################################

https://www.youtube.com/watch?v=A1qzxDxfMZQ

https://pinokio.computer/

https://pinokio.computer/item?uri=https://github.com/pinokiofactory/MMAudio
https://pinokio.computer/item?uri=https://github.com/pinokiofactory/ai-video-composer
https://pinokio.computer/item?uri=https://github.com/pinokiofactory/comfy
https://pinokio.computer/item?uri=https://github.com/pinokiofactory/open-webui
https://pinokio.computer/item?uri=https://github.com/pinokiofactory/whisper-webui
https://pinokio.computer/item?uri=https://github.com/pinokiofactory/hallo
https://pinokio.computer/item?uri=https://github.com/pinokiofactory/stableaudio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/openvoice2
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/customnet
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/differential-diffusion-ui
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/chatbot-ollama
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/automatic1111
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/sdxl-turbo
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/diffusers-sdxl-turbo
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/svd.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/illusion.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/audio-webui.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/AudioLDM2.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/audiogradio.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/animatediff.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/llamacpp.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/tutorial.pinokio
https://pinokio.computer/item?uri=https://github.com/SUP3RMASS1VE/SD-Next
https://pinokio.computer/item?uri=https://github.com/6Morpheus6/illusion-diffusion-HQ
https://pinokio.computer/item?uri=https://github.com/tazztone/whisper-large-v3


###########################################
Pinokio
###########################################

https://pinokio.computer/
https://program.pinokio.computer/#/?id=install
https://program.pinokio.computer/#/?id=linux
https://github.com/pinokiocomputer/pinokio/releases/tag/1.3.4
https://github.com/pinokiocomputer/pinokio/releases/download/1.3.4/Pinokio_1.3.4_arm64.deb

https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/fooocus
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/differential-diffusion-ui
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/open-webui
https://pinokio.computer/item?uri=https://github.com/facefusion/facefusion-pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/magnet
https://pinokio.computer/item?uri=https://github.com/cocktailpeanutlabs/automatic1111
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/sdxl-turbo
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/svd.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/audiocraft_plus.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/audio-webui.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/audiogradio.pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/tutorial.pinokio
https://pinokio.computer/item?uri=https://github.com/6Morpheus6/pinokio-wiki
https://pinokio.computer/item?uri=https://github.com/facefusion/facefusion-pinokio
https://pinokio.computer/item?uri=https://github.com/Feedjer/stable-diffusion-webui-ux.pinokio
https://pinokio.computer/item?uri=https://github.com/supersonic13/sdxs-pinokio
https://pinokio.computer/item?uri=https://github.com/cocktailpeanut/AudioLDM2.pinokio


#####################################################
8 AI Tools
#####################################################

Napkin AI - https://www.napkin.ai/
NotebookLM - https://notebooklm.google.com/
Pinokio - https://pinokio.computer/
Replicate - https://replicate.com/
Flux Dev Lora Trainer - https://replicate.com/ostris/flux-dev...
Spotter Studio - https://partner.spotterstudio.com/fut...
Guidde - https://www.guidde.com/
Suno - https://suno.com/


#####################################################
Open NoteBookLM
#####################################################

https://www.youtube.com/watch?v=M3wue0dw6tw
https://github.com/gabrielchua/open-notebooklm/tree/main

git clone https://github.com/gabrielchua/open-notebooklm.git
cd open-notebooklm

python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt

python app.py

#####################################################
PodCastfy
#####################################################

https://www.youtube.com/watch?v=q26rqFg--84

https://github.com/souzatharsis/podcastfy
https://github.com/souzatharsis/podcastfy-demo

pip install ffmpeg
pip install podcastfy
Set up your API keys

from podcastfy.client import generate_podcast

audio_file = generate_podcast(urls=["<url1>", "<url2>"])
python -m podcastfy.client --url <url1> --url <url2>




#####################################################
#####################################################
https://scalastic.io/en/mixtral-ollama-llamaindex-llm/
https://peterfalkingham.com/2024/04/26/my-experience-training-a-local-llm-ai-chatbot-on-local-data/

chatDocs
privateGPT
phi-1
TinyStories
GPT4All
OPENAI
Lora
Ollama
FAST- GPT-LLM-Trainer






ollama run mixtral
conda create --name mixtral_ollama python=3.11
conda activate mixtral_ollama
# llama-index
pip install -r requirements.txt
python reference_test.py



#####################################################
FemtoGPT
#####################################################

https://www.youtube.com/watch?v=jEyPQUyNhD0
https://github.com/keyvank/femtoGPT
https://github.com/cthiriet/femtoGPT
https://docs.rs/femto-gpt/latest/femto_gpt/
https://crates.io/crates/femto-gpt/reverse_dependencies



#####################################################
LLMs locally
#####################################################

pip install llm
llm install llm-gpt4all
llm -m the-model-name "Your query"
llm -m ggml-model-gpt4all-falcon-q4_0 "Tell me a joke about computer programming"
llm aliases set falcon ggml-model-gpt4all-falcon-q4_0


https://github.com/ollama/ollama
https://scalastic.io/en/mixtral-ollama-llamaindex-llm/
https://www.restack.io/p/llm-orchestration-answer-train-own-data-cat-ai

ollama run model-name


h2oGPT
PrivateGPT

#####################################################
llamafile
#####################################################

https://github.com/Mozilla-Ocho/llamafile

curl -L https://github.com/Mozilla-Ocho/llamafile/releases/download/0.1/llamafile-server-0.1 > llamafile
chmod +x llamafile

./llamafile --model ./zephyr-7b-alpha.Q4_0.gguf
http://127.0.0.1:8080.

 GitHub repository: mistral-7b-instruct, llava-v1.5-7b-server, or wizardcoder-python-13b.

#####################################################
LocalGPT
#####################################################

https://github.com/PromtEngineer/localGPT
https://www.youtube.com/watch?v=MlyoObdIHyo

LM Studio
https://lmstudio.ai/
https://huggingface.co/models
https://www.infoworld.com/article/2338922/5-easy-ways-to-run-an-llm-locally.html
https://huggingface.co/docs/trl/en/learning_tools


#####################################################
LLMs can be used to label data and support analyses.
#####################################################

https://github.com/thu-vu92/local-llms-analyse-finance#
https://www.youtube.com/watch?v=h_GTxRFYETY



Text generation web UI
https://github.com/oobabooga/text-generation-webui

How to install
Clone or download the repository.
Run the script that matches your OS: start_linux.sh, start_windows.bat, start_macos.sh, or start_wsl.bat.
Select your GPU vendor when asked.
Once the installation ends, browse to http://localhost:7860.
Have fun!

ComfyUI-HunyuanVideoWrapper
https://github.com/kijai/ComfyUI-HunyuanVideoWrapper


#####################################################
deepseek
#####################################################

ollama run deepseek-coder 	# 1.3 billion parameter model
ollama run deepseek-coder:6.7b	# 6.7 billion parameter model
ollama run deepseek-r1:1.5b	# DeepSeek-R1-Distill-Qwen-1.5B
ollama run deepseek-r1:7b	# DeepSeek-R1-Distill-Qwen-7B
ollama run deepseek-r1:8b

https://ollama.com/library/deepseek-coder
https://ollama.com/library/deepseek-r1
https://ollama.com/download


API
Example using curl:

curl -X POST http://localhost:11434/api/generate -d '{
  "model": "deepseek-coder",
  "prompt":"Why is the sky blue?"
 }'

curl -fsSL https://ollama.com/install.sh | sh
ollama run deepseek-r1:1.5b

docker run -d -p 9783:8080 -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main

http://localhost:9783/

####################################################
ollama
####################################################


https://www.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/
https://ollama.com/library/llama2:7b-chat-q4_0
https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B
https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview
https://huggingface.co/meta-llama/Llama-3.2-11B-Vision
https://github.com/shinhyo/OllamaTalk


# first build it:
git clone --depth 1 --branch v0.7.2 https://github.com/vllm-project/vllm.git && \
cd vllm && \
sudo docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g . && \
# then run the inference:
docker run -it --rm --network=host \
  vllm-cpu-env \
  --model your-huggingface-model-name \ # adapt this!
  --served-model-name my-api-model-name \ # optional: what name should be used in the API
  --port 8000 \ # port of the API
  --load-format safetensors # or whatever your model is


llm models working on cpu
https://huggingface.co/spaces/Nymbo/Hermes-3-Llama-3.1-8B

Several large language models (LLMs) have been developed or optimized to run on CPUs, although they may not be as performant as their GPU-accelerated counterparts. Here are a few notable examples:

BLOOM (BigScience Large Open-science Open-access Multilingual) - Developed by the BigScience consortium, this 17-billion parameter model can run on CPUs.

PaLM (Pathways Language Model) - Developed by Google, this 540-billion parameter model has been fine-tuned to run efficiently on CPUs.

OPT (Optimus) - Developed by Meta AI, this 125-billion parameter model is designed to run on CPUs and has shown strong performance on various natural language tasks.

GLaM (Generalist Language Model) - Developed by UC Berkeley and Stanford, this 352-billion parameter model is optimized for CPU inference.

CTRL (Causal Transformer Language model) - Developed by Microsoft, this 7-billion parameter model is designed to run on CPUs and has been used for various natural language tasks.

Camembert - A French language variant of the BERT model developed by Facebook AI, which can run on CPUs.

Distilm - Developed by DeepMind, this model is designed to run efficiently on CPUs and has been used for various natural language tasks.

Keep in mind that while these models can run on CPUs, they may not achieve the same level of performance as their GPU-accelerated counterparts. The choice of model depends on factors such as the specific task, available computational resources, and desired performance trade-offs.