#############################################
Run an unlocked LLM on your desktop in 15 minutes - 6GB RAM 6 CPU
#############################################

sudo apt install curl -y
curl -fsSL https://ollama.com/install.sh | sh

ollama run qwen3:0.6b

https://docs.docker.com/engine/install/linux-postinstall/

sudo apt install docker.io -y
sudo groupadd docker
sudo usermod -aG docker $USER
exec su -l $USER
docker run hello-world

ollama list

check
http://localhost:11434/
http://127.0.0.1:11434/

systemctl daemon-reload
systemctl restart ollama

docker run -d -p 3000:8080 -e OLLAMA_API_BASE_URL=http://host.docker.internal:11434/api -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
Open your browser and go to: http://localhost:3000

sudo apt install python3-pip
pip install ollama


##########################################
codellama
##########################################


https://ollama.com/library/qwen3:0.6b
https://ollama.com/library/qwen3:1.7b
https://ollama.com/library/qwen3:4b
https://ollama.com/library/qwen3
https://ollama.com/library/qwen2.5
https://ollama.com/library/mistral   4GB
https://ollama.com/library/deepseek-r1:1.5b
https://ollama.com/library/qwen2.5vl:3b
https://ollama.com/library/llama3.1:8b
https://ollama.com/library/qwen2.5:1.5b
https://ollama.com/library/qwen2.5-coder:1.5b
https://ollama.com/library/gemma2:2b
https://ollama.com/library/qwen:1.8b
https://ollama.com/library/codellama:7b
https://ollama.com/library/tinyllama:latest
https://ollama.com/library/deepseek-coder:6.7b
https://ollama.com/library/phi3.5:latest
https://ollama.com/library/phi4-mini:latest
https://ollama.com/library/tinydolphin:latest
https://ollama.com/library/qwen2-math:1.5b
https://ollama.com/library/deepcoder:1.5b
https://ollama.com/library/sqlcoder:7b
https://ollama.com/library/yi-coder:1.5b
https://ollama.com/library/mistrallite:latest

...

https://blog.codegpt.co/create-your-own-and-custom-copilot-in-vscode-with-ollama-and-codegpt-736277a60298

https://ollama.com/library/codellama
ollama pull codellama
ollama list
ollama run codellama


Modelfile
~~~
FROM codellama

# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# sets the context window size to 1500, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 1500

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are expert Code Assistant
~~~


ollama create codegpt-codellama -f ./Codellama/Modelfile
ollama run codegpt-codellama

CodeGPT extension
https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt
https://marketplace.visualstudio.com/items?itemName=AnikGhosh.ollama-copilot

