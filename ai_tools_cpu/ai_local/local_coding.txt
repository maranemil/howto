


docs-mcp-server
https://github.com/arabold/docs-mcp-server

This is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the correct syntax for your version of language implementation, and avoid hallucinations.

You should also be able to run localhost:6281 to open web UI for the docs-mcp-server, however web UI doesn't seem to be working for me, which I can ignore because AI is managing that anyway.

You can implement this MCP server as following -

Docker version (needs Docker Installed)

{
  "mcpServers": {
    "docs-mcp-server": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-p",
        "6280:6280",
        "-p",
        "6281:6281",
        "-e",
        "OPENAI_API_KEY",
        "-e",
        "OPENAI_API_BASE",
        "-e",
        "DOCS_MCP_EMBEDDING_MODEL",
        "-v",
        "docs-mcp-data:/data",
        "ghcr.io/arabold/docs-mcp-server:latest"
      ],
      "env": {
        "OPENAI_API_KEY": "ollama",
        "OPENAI_API_BASE": "http://host.docker.internal:11434/v1",
        "DOCS_MCP_EMBEDDING_MODEL": "snowflake-arctic-embed2"
      }
    }
  }
}
NPX version (needs NodeJS installed)

{
  "mcpServers": {
    "docs-mcp-server": {
      "command": "npx",
      "args": [
        "@arabold/docs-mcp-server@latest"
      ],
      "env": {
        "OPENAI_API_KEY": "ollama",
        "OPENAI_API_BASE": "http://host.docker.internal:11434/v1",
        "DOCS_MCP_EMBEDDING_MODEL": "snowflake-arctic-embed2"
      }
    }
  }
}


Adding documentation for your language

Ask AI to use the scrape_docs tool with:

url (link to the documentation),

library (name of the documentation/programming language),

version (version of the documentation)

you can also provide (optional):

maxPages (maximum number of pages to scrape, default is 1000).

maxDepth (maximum navigation depth, default is 3).

scope (crawling boundary, which can be 'subpages', 'hostname', or 'domain', default is 'subpages').

followRedirects (whether to follow HTTP 3xx redirects, default is true).

You can ask AI to use search_docs tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.

This stack isnâ€™t limited to coding, Devstral handles logical, non-coding tasks well too.
The MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.

https://www.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/

https://github.com/likelovewant/ollama-for-amd
https://github.com/mostlygeek/llama-swap
https://github.com/arabold/docs-mcp-server
https://martech.org/how-to-run-deepseek-locally-on-your-computer/

https://github.com/ItzCrazyKns/Perplexica
https://github.com/NPC-Worldwide/npcpy
https://github.com/NPC-Worldwide/npc-studio


-----------------------------
########################################
VSCode Ollama Integration
########################################


Integrating Ollama with Visual Studio Code (VSCode) allows developers to leverage local Large Language Models (LLMs) directly within their code editor for enhanced productivity, privacy, and customization. This integration enables features like code completion, natural language queries for explanations and code snippets, and interactive chat panels for real-time assistance.

To set up the integration, first install Ollama on your machine using the appropriate command for your operating system, such as curl -fsSL https://ollama.com/install.sh | sh for macOS and Linux, or using WSL2 for Windows.
 After installation, confirm Ollama is running by checking its version.
 Next, install a compatible VSCode extension from the marketplace. Popular options include the official "VSCode Ollama" extension 
 , "ollama-vscode-integration" 
 , "Ollama connection" 
 , and "Ollama Chat".
 Each extension provides a way to connect to the locally running Ollama server, typically accessible at http://127.0.0.1:11434.
 
 
https://ollama.com/blog/continue-code-assistant

https://marketplace.visualstudio.com/items?itemName=warm3snow.vscode-ollama
https://marketplace.visualstudio.com/items?itemName=Continue.continue
https://plugins.jetbrains.com/plugin/22707-continue
https://code.visualstudio.com/docs/intelligentapps/models
https://marketplace.visualstudio.com/items?itemName=10nates.ollama-autocoder
https://hacktobeer.eu/posts/local-llms/
https://marketplace.visualstudio.com/items?itemName=ekbanasolutions.codellm
https://github.com/ex3ndr/llama-coder
https://marketplace.visualstudio.com/items?itemName=AnikGhosh.ollama-copilot
https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt
https://marketplace.visualstudio.com/items?itemName=Continue.continue




ollama run codestral

Click on the VSCode gear icon in the bottom right corner of Continue to open your config.json and add

{
  "models": [
    {
      "title": "Codestral",
      "provider": "ollama",
      "model": "codestral"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Codestral",
    "provider": "ollama",
    "model": "codestral"
  }
}

ollama run llama3:8b

open your config.json and add

{
  "models": [
    {
      "title": "Llama 3 8B",
      "provider": "ollama",
      "model": "llama3:8b"
    }
  ],
  "tabAutocompleteModel": {
    "title": "DeepSeek Coder 6.7B",
    "provider": "ollama",
    "model": "deepseek-coder:6.7b-base"
  }
}

https://docs.codegpt.co/docs/tutorial-ai-providers/ollama


ollama run deepseek-r1
For localhost: htttp://localhost:11434

Autocomplete models availables:
codestral:latest
qwen2.5-coder:7b
deepseek-coder:base
qwen2.5-coder:1.5b
codegemma:code
codellama:code


