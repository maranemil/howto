


docs-mcp-server
https://github.com/arabold/docs-mcp-server

This is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the correct syntax for your version of language implementation, and avoid hallucinations.

You should also be able to run localhost:6281 to open web UI for the docs-mcp-server, however web UI doesn't seem to be working for me, which I can ignore because AI is managing that anyway.

You can implement this MCP server as following -

Docker version (needs Docker Installed)

{
  "mcpServers": {
    "docs-mcp-server": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-p",
        "6280:6280",
        "-p",
        "6281:6281",
        "-e",
        "OPENAI_API_KEY",
        "-e",
        "OPENAI_API_BASE",
        "-e",
        "DOCS_MCP_EMBEDDING_MODEL",
        "-v",
        "docs-mcp-data:/data",
        "ghcr.io/arabold/docs-mcp-server:latest"
      ],
      "env": {
        "OPENAI_API_KEY": "ollama",
        "OPENAI_API_BASE": "http://host.docker.internal:11434/v1",
        "DOCS_MCP_EMBEDDING_MODEL": "snowflake-arctic-embed2"
      }
    }
  }
}
NPX version (needs NodeJS installed)

{
  "mcpServers": {
    "docs-mcp-server": {
      "command": "npx",
      "args": [
        "@arabold/docs-mcp-server@latest"
      ],
      "env": {
        "OPENAI_API_KEY": "ollama",
        "OPENAI_API_BASE": "http://host.docker.internal:11434/v1",
        "DOCS_MCP_EMBEDDING_MODEL": "snowflake-arctic-embed2"
      }
    }
  }
}


Adding documentation for your language

Ask AI to use the scrape_docs tool with:

url (link to the documentation),

library (name of the documentation/programming language),

version (version of the documentation)

you can also provide (optional):

maxPages (maximum number of pages to scrape, default is 1000).

maxDepth (maximum navigation depth, default is 3).

scope (crawling boundary, which can be 'subpages', 'hostname', or 'domain', default is 'subpages').

followRedirects (whether to follow HTTP 3xx redirects, default is true).

You can ask AI to use search_docs tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.

This stack isn’t limited to coding, Devstral handles logical, non-coding tasks well too.
The MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.

https://www.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/

https://github.com/likelovewant/ollama-for-amd
https://github.com/mostlygeek/llama-swap
https://github.com/arabold/docs-mcp-server
https://martech.org/how-to-run-deepseek-locally-on-your-computer/

https://github.com/ItzCrazyKns/Perplexica
https://github.com/NPC-Worldwide/npcpy
https://github.com/NPC-Worldwide/npc-studio

https://github.com/Mote-Software/nanocoder


-----------------------------
########################################
VSCode Ollama Integration
########################################


Integrating Ollama with Visual Studio Code (VSCode) allows developers to leverage local Large Language Models (LLMs) directly within their code editor for enhanced productivity, privacy, and customization. This integration enables features like code completion, natural language queries for explanations and code snippets, and interactive chat panels for real-time assistance.

To set up the integration, first install Ollama on your machine using the appropriate command for your operating system, such as curl -fsSL https://ollama.com/install.sh | sh for macOS and Linux, or using WSL2 for Windows.
 After installation, confirm Ollama is running by checking its version.
 Next, install a compatible VSCode extension from the marketplace. Popular options include the official "VSCode Ollama" extension 
 , "ollama-vscode-integration" 
 , "Ollama connection" 
 , and "Ollama Chat".
 Each extension provides a way to connect to the locally running Ollama server, typically accessible at http://127.0.0.1:11434.
 
 
https://ollama.com/blog/continue-code-assistant

https://marketplace.visualstudio.com/items?itemName=warm3snow.vscode-ollama
https://marketplace.visualstudio.com/items?itemName=Continue.continue
https://plugins.jetbrains.com/plugin/22707-continue
https://code.visualstudio.com/docs/intelligentapps/models
https://marketplace.visualstudio.com/items?itemName=10nates.ollama-autocoder
https://hacktobeer.eu/posts/local-llms/
https://marketplace.visualstudio.com/items?itemName=ekbanasolutions.codellm
https://github.com/ex3ndr/llama-coder
https://marketplace.visualstudio.com/items?itemName=AnikGhosh.ollama-copilot
https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt
https://marketplace.visualstudio.com/items?itemName=Continue.continue




ollama run codestral

Click on the VSCode gear icon in the bottom right corner of Continue to open your config.json and add

{
  "models": [
    {
      "title": "Codestral",
      "provider": "ollama",
      "model": "codestral"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Codestral",
    "provider": "ollama",
    "model": "codestral"
  }
}

ollama run llama3:8b

open your config.json and add

{
  "models": [
    {
      "title": "Llama 3 8B",
      "provider": "ollama",
      "model": "llama3:8b"
    }
  ],
  "tabAutocompleteModel": {
    "title": "DeepSeek Coder 6.7B",
    "provider": "ollama",
    "model": "deepseek-coder:6.7b-base"
  }
}

https://docs.codegpt.co/docs/tutorial-ai-providers/ollama


ollama run deepseek-r1
For localhost: htttp://localhost:11434

Autocomplete models availables:
codestral:latest
qwen2.5-coder:7b
deepseek-coder:base
qwen2.5-coder:1.5b
codegemma:code
codellama:code




Ollama coding models
https://www.reddit.com/r/ollama/comments/1ocuuej/playing_with_coding_models/
https://github.com/MarekIksinski/experiments_various

deepseek-r1:32b
devstral:latest    # 14GB
mistral-small:24b  # 14GB
phi4-reasoning:14b-plus-q8_0
qwen3-coder:latest   # 19GB
qwen2-5-coder:32b  # 20GB

Winners:
phi4-reasoning:14b-plus-q8_0
magistral:latest
qwen2_5-coder:32b
mistral-small:24b
qwen3-coder:latest

codellama:7b   # 3.8GB
codegemma:2b  # 1.6GB
cogito:3b  # 2.2GB
qwen2.5-coder:0.5b # 398MB
qwen2.5-coder:1.5b # 986MB
deepseek-coder:1.3b # 776MB
deepcoder:1.5b # 1.1GB
granite-code:3b # 2.0GB
starcoder:1b # 726MB
starcoder:3b # 1.8GB
opencoder:1.5b # 1.4GB





#################################################
ollama vscode
#################################################

https://ollama.com/library/minimax-m2:cloud
https://ollama.com/blog/minimax-m2
https://github.com/MiniMax-AI/MiniMax-M2
https://www.minimax.io/news/minimax-m2
https://agent.minimax.io/
https://docs.vllm.ai/projects/recipes/en/latest/MiniMax/MiniMax-M2.html#launching-minimax-m2-with-vllm

........
Usage with VS Code
First, pull the coding models so they can be accessed via VS Code:

ollama pull minimax-m2:cloud
Open the copilot chat sidebar
Select the model dropdown → Manage models
Click on Ollama under Provider Dropdown, then select desired models
Select the model dropdown → and choose the model (e.g. minimax-m2:cloud)
........
Usage with Zed
First pull the coding models so they can be accessed via Zed:

ollama pull minimax-m2:cloud
Then, open Zed (now available for Windows!)

Click on the agent panel button (glittering stars)
Click on the model dropdown → Configure
Select LLM providers → Ollama
Confirm the Host URL is http://localhost:11434, then click Connect
Select a model under Ollama



-------------------------------
nanocoder
https://github.com/Nano-Collective/nanocoder

codex
https://developers.openai.com/codex/cli/
npm i -g @openai/codex
npm install -g @openai/codex

qodo
https://www.qodo.ai/features/qodo-cli/
npm install -g @qodo/command

claud code cli
https://github.com/anthropics/claude-code
https://code.claude.com/docs/en/cli-reference
https://code.claude.com/docs/en/quickstart
npm install -g @anthropic-ai/claude-code

gemini cli
https://github.com/google-gemini/gemini-cli
https://geminicli.com/
npm install -g @google/gemini-cli

grok cli
https://github.com/superagent-ai/grok-cli
https://grokcli.io/
npm install -g @vibe-kit/grok-cli


kiro
https://kiro.dev/cli/

opencode
https://github.com/anomalyco/opencode
https://opencode.ai/
npm install -g opencode-ai@latest
npm i -g opencode-ai
curl -fsSL https://opencode.ai/install | bash

copilot
https://developer.microsoft.com/blog/making-windows-terminal-awesome-with-github-copilot-cli
npm install -g @github/copilot


Droid
https://factory.ai/
curl -fsSL https://app.factory.ai/cli | sh

Rovo Dev agent
https://developer.atlassian.com/cloud/acli/guides/install-linux/


