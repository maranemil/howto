curl localhost:11434
OLLAMA_HOST=0.0.0.0:11434
sudo netstat -pant | grep :11434
sudo tcpdump -i any dst port 11434
sudo tcpdump -i any dst port 11434 or src port 11434
sudo ss -tulnp | grep 11434
systemctl daemon-reload
systemctl restart ollama
sudo ufw allow 11434/tcp
sudo systemctl enable --now docker

sudo apt-get install systemd
# udo apt-get install --reinstall systemd
systemd --user
sudo loginctl enable-linger root
systemctl reboot
systemctl soft-reboot

service --status-all
[ - ]  dbus
 [ ? ]  hwclock.sh
service dbus start
service dbus status
dbus is running.


System has not been booted with systemd as init system (PID 1). Can't operate.
Failed to connect to bus: Host is down

docker restart debianollama
systemd -h


systemctl daemon-reload
apt install systemd-sysv
dpkg -S /sbin/init

docker run -ti -d ubuntu:20.04 "/sbin/init"
docker run -ti -d --privileged ubuntu:20.04 "/sbin/init"
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
docker exec -it ollama ollama pull tinyllama



systemctl edit ollama.service
[Service] Environment="OLLAMA_HOST=0.0.0.0:11434"
systemctl daemon-reload systemctl restart ollama


ollama serve --help
 OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)
apt install nano
nano /etc/systemd/system/ollama.service
ls /etc/systemd/system/

-----


ollama:
  image: 'ollama/ollama:latest'
  container_name: ollama
  ports:
    - '11434:11434'
  restart: unless-stopped
  environment:
    - OLLAMA_MODELS=/ollama/data/models
  volumes:
    - '${DATA_DIR_LOCAL}/raw/ia/ollama_data:/ollama/data'
  networks:
    - internal_net
    

export OLLAMA_HOST=0.0.0.0:8080
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_HOST=0.0.0.0
echo $OLLAMA_HOST
launchctl setenv OLLAMA_HOST 0.0.0.0

export OLLAMA_HOST=0.0.0.0:8080 or export OLLAMA_HOST=0.0.0.0:11434


export OLLAMA_HOST=127.0.0.1:8080 ollama serve
export OLLAMA_HOST=0.0.0.0 ollama serve

curl localhost:11434
curl http://host.docker.internal:11434
ps aux | grep ollama
netstat -an | grep 11434
lsof -n -iTCP:11434 -sTCP:LISTEN


ip addr show eth0 | grep inet
export OLLAMA_HOST=0.0.0.0
OLLAMA_HOST=0.0.0.0:11434
sudo netstat -anp | grep ollama

 
curl http://172.17.0.2:11434/api/chat -d '{ "model": "gemma3:270m", "messages": [{ "role": "user", "content": "Hello" }], "stream": false }'
curl http://172.17.0.2:11434/api/generate -d '{   "model": "gemma3:270m",   "prompt": "Why is the sky blue?" }'

http://172.17.0.2:11434/   # Ollama is running
http://172.17.0.2:11434/api/chat  # 405 method not allowed
http://172.17.0.2:11434/api/    # 404 page not found

# venv
python3 -m venv /path/to/new/virtual/environment
python3 -m venv .env
source .env/bin/activate

python3 -m venv myfirstproject
source myfirstproject/bin/activate

# virtualenv
pip3 install virtualenv 
mkdir venv
cd venv
virtualenv -p python3 python3_virtualenv
virtualenv --python=python3.5 envname
source ~/venv/python3_virtualenv/bin/activate && python -V
which virtualenv

virtualenv -p python3 envname
pip install --upgrade virtualenv


pip install ollama-gui
ollama-gui



ollama list
NAME              ID              SIZE      MODIFIED     
smollm:360m       b3ba1ccba2b8    229 MB    6 weeks ago     
gemma3:270m       e7d36fb2c3b3    291 MB    6 weeks ago     
tinyllama:1.1b    2644915ede35    637 MB    4 months ago 

ollama list
NAME              ID              SIZE      MODIFIED       
qwen2:0.5b        6f48b936a09f    352 MB    22 minutes ago    
qwen:0.5b         b5dc5e784f2a    394 MB    28 minutes ago    
smollm:360m       b3ba1ccba2b8    229 MB    6 weeks ago       
gemma3:270m       e7d36fb2c3b3    291 MB    6 weeks ago       
tinyllama:1.1b    2644915ede35    637 MB    4 months ago


https://github.com/ArdaGnsrn/ollama-php
https://github.com/ollama-ui/ollama-ui
https://github.com/chyok/ollama-gui#python
https://github.com/mmo80/alpaca-webui
https://github.com/ollama/ollama?tab=readme-ov-file#libraries-1