

#############################################
Run an unlocked LLM on your desktop in 15 minutes
#############################################

If you’re sick of seeing “I’m sorry, I can’t help with that,” or want unhinged responses to your inputs,
here’s how to run a LLM right on your computer in 15 minutes while being private, free, and with no rules.

First Install the LLM Ollama on your computer

Windows: Go to https://ollama.com/download and install it like any normal app.


Mac/Linux: Open Terminal and run:
sudo apt install curl -y
curl -fsSL https://ollama.com/install.sh | sh

After that, run an unfiltered AI model by opening your terminal or command prompt and type:
“ollama run mistral”   4GB

or for a even more unfiltered experience:
“ollama run dolphin-mistral”

It’ll download the model, then you’ll get a prompt like: >>>
Boom. You’re unlocked and ready to go. Now you can ask anything. No filters, no guardrails.

You can run a 7B uncensored LLM like Mistral on any modern desktop or laptop with 8GB RAM, no GPU required, using a quantized model.

docker run -d -p 3000:3000 -e OLLAMA_HOST=http://host.docker.internal:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
Open your browser and go to: http://localhost:3000

docker run -d -p 3000:8080 -e OLLAMA_API_BASE_URL=http://host.docker.internal:11434/api -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
Open your browser and go to: http://localhost:3000


----------------------------------------
Troubleshooting
----------------------------------------

ollama run qwen3:0.6b
ollama rm qwen3:0.6b
ollama run deepseek-r1:1.5b

sudo apt install docker.io -y
sudo groupadd docker
sudo usermod -aG docker $USER
exec su -l $USER
docker run hello-world

----------------------------------------

ollama pull llava:13b
ollama list

check
http://localhost:11434/
http://127.0.0.1:11434/

docker ps
docker stop beffdde5e489
docker rm beffdde5e489

systemctl daemon-reload
systemctl restart ollama

https://ollama.com/search
https://ollama.com/library/qwen3:0.6b
https://ollama.com/library/deepseek-r1
https://ollama.com/library/llama3.2
------------

https://pypi.org/project/ollama/
https://www.gpu-mart.com/blog/how-to-run-mistral-using-ollama
https://www.byteplus.com/en/topic/398500?title=ollama-models-that-can-generate-images-a-comprehensive-guide-to-ai-image-creation

# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull a vision model
ollama pull llama3.2:3b

# Verify installation
ollama list

sudo apt install python3-pip
pip install ollama

###########################################
ollama docker
###########################################

https://hub.docker.com/r/ollama/ollama
https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image
https://github.com/mythrantic/ollama-docker
https://hub.docker.com/r/ollama/ollama
https://ollama.com/library/qwen2.5vl
https://ollama.com/library/qwen3
https://ollama.com/library/deepseek-r1
https://ollama.com/library/phi3:mini
https://ollama.com/library/orca-mini
https://ollama.com/library/nemotron-mini
https://ollama.com/library/all-minilm
https://github.com/ollama/ollama

curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3.2:1b
ollama run moondream
ollama run deepseek-r1:1.5b
ollama run qwen3:0.6b
ollama run qwen3:1.7b

docker pull ollama/ollama
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
docker exec -it ollama ollama run llama3


