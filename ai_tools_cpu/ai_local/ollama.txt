
#############################################
Run an unlocked LLM on your desktop in 15 minutes
#############################################

If you’re sick of seeing “I’m sorry, I can’t help with that,” or want unhinged responses to your inputs,
here’s how to run a LLM right on your computer in 15 minutes while being private, free, and with no rules.

First Install the LLM Ollama on your computer

Windows: Go to https://ollama.com/download and install it like any normal app.


Mac/Linux: Open Terminal and run:
sudo apt install curl -y
curl -fsSL https://ollama.com/install.sh | sh

After that, run an unfiltered AI model by opening your terminal or command prompt and type:
“ollama run mistral”   4GB

or for a even more unfiltered experience:
“ollama run dolphin-mistral”

It’ll download the model, then you’ll get a prompt like: >>>
Boom. You’re unlocked and ready to go. Now you can ask anything. No filters, no guardrails.

You can run a 7B uncensored LLM like Mistral on any modern desktop or laptop with 8GB RAM, no GPU required, using a quantized model.

docker run -d -p 3000:3000 -e OLLAMA_HOST=http://host.docker.internal:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
Open your browser and go to: http://localhost:3000

docker run -d -p 3000:8080 -e OLLAMA_API_BASE_URL=http://host.docker.internal:11434/api -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
Open your browser and go to: http://localhost:3000


----------------------------------------
Troubleshooting
----------------------------------------

ollama run qwen3:0.6b
ollama rm qwen3:0.6b
ollama run deepseek-r1:1.5b

sudo apt install docker.io -y
sudo groupadd docker
sudo usermod -aG docker $USER
exec su -l $USER
docker run hello-world

----------------------------------------

ollama pull llava:13b
ollama list

check
http://localhost:11434/
http://127.0.0.1:11434/

docker ps
docker stop beffdde5e489
docker rm beffdde5e489

systemctl daemon-reload
systemctl restart ollama

https://ollama.com/search
https://ollama.com/library/qwen3:0.6b
https://ollama.com/library/deepseek-r1
https://ollama.com/library/llama3.2
------------

https://pypi.org/project/ollama/
https://www.gpu-mart.com/blog/how-to-run-mistral-using-ollama
https://www.byteplus.com/en/topic/398500?title=ollama-models-that-can-generate-images-a-comprehensive-guide-to-ai-image-creation

# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull a vision model
ollama pull llama3.2:3b

# Verify installation
ollama list

sudo apt install python3-pip
pip install ollama

###########################################
ollama docker
###########################################

https://hub.docker.com/r/ollama/ollama
https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image
https://github.com/mythrantic/ollama-docker
https://hub.docker.com/r/ollama/ollama
https://ollama.com/library/qwen2.5vl
https://ollama.com/library/qwen3
https://ollama.com/library/deepseek-r1
https://ollama.com/library/phi3:mini
https://ollama.com/library/orca-mini
https://ollama.com/library/nemotron-mini
https://ollama.com/library/all-minilm
https://github.com/ollama/ollama

curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3.2:1b
ollama run moondream
ollama run deepseek-r1:1.5b
ollama run qwen3:0.6b
ollama run qwen3:1.7b

docker pull ollama/ollama
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
docker exec -it ollama ollama run llama3



####################################################################
Ollama CLI tutorial: Running Ollama via the terminal - Ariffud M.
####################################################################
https://www.hostinger.com/tutorials/ollama-cli-tutorial

ollama serve	Starts Ollama on your local system.
ollama create <new_model>	Creates a new model from an existing one for customization or training.
ollama show <model>	Displays details about a specific model, such as its configuration and release date.
ollama run <model>	Runs the specified model, making it ready for interaction
ollama pull <model>	Downloads the specified model to your system.
ollama list	Lists all the downloaded models.
ollama ps	Shows the currently running models.
ollama stop <model>	Stops the specified running model.
ollama rm <model>	Removes the specified model from your system

ollama pull llama3.2
ollama run llama3.2 "Explain the basics of machine learning."
ollama run llama3.2
/bye
Hey, I want you to learn about [topic]. Can I train you on this?
Can you ask me a few questions about [topic] to help you understand it better?


ollama run llama3.2 "Summarize the content of this file in 50 words." < input.txt
ollama run llama3.2 "Tell me about renewable energy."> output.txt
ollama run llama3.2 "Write a short article on the benefits of using AI in healthcare."> article.txt
ollama run llama3.2 "What are the latest trends in AI, and how will they affect healthcare?"
ollama run llama3.2 "Analyze the sentiment of this customer review: 'The product is fantastic, but delivery was slow.'"
ollama run llama3.2 "Classify this text into the following categories: News, Opinion, or Review." < textfile.txt
ollama run llama3.2 "Predict the stock price trend for the next month based on the following data:" < stock-data.txt

curl -X GET "https://api.example.com/data" | ollama run llama3.2 "Analyze the following API data and summarize key insights."

import subprocess

result = subprocess.run(['ollama', 'run', 'llama3.2', 'Give me the latest stock market trends'], capture_output=True)

print(result.stdout.decode())

