
##############################################################################
# Reading and writing files from/to Amazon S3 with Pandas
##############################################################################

https://towardsdatascience.com/reading-and-writing-files-from-to-amazon-s3-with-pandas-ccaf90bfe86c
https://gist.github.com/onelharrison/83b428803d605c7ddf312de8f2349e0f#file-pandas_write_to_s3_using_boto3-py


python -m pip install boto3 pandas "s3fs<=0.4"
python -m pip install boto3 pandas s3fs

---------------------------------
Write pandas data frame to CSV file on S3
---------------------------------
"""
Demo script for writing a pandas data frame to a CSV file on S3 using the boto3 library
"""

import io
import os

import boto3
import pandas as pd


AWS_S3_BUCKET = os.getenv("AWS_S3_BUCKET")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_SESSION_TOKEN = os.getenv("AWS_SESSION_TOKEN")

s3_client = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    aws_session_token=AWS_SESSION_TOKEN,
)

books_df = pd.DataFrame(
    data={"Title": ["Book I", "Book II", "Book III"], "Price": [56.6, 59.87, 74.54]},
    columns=["Title", "Price"],
)


with io.StringIO() as csv_buffer:
    books_df.to_csv(csv_buffer, index=False)

    response = s3_client.put_object(
        Bucket=AWS_S3_BUCKET, Key="files/books.csv", Body=csv_buffer.getvalue()
    )

    status = response.get("ResponseMetadata", {}).get("HTTPStatusCode")

    if status == 200:
        print(f"Successful S3 put_object response. Status - {status}")
    else:
        print(f"Unsuccessful S3 put_object response. Status - {status}")



---------------------------------
Using s3fs-supported pandas API
---------------------------------

"""
Demo script for writing a pandas data frame to a CSV file on S3 using s3fs-supported pandas APIs
"""

import os

import pandas as pd

AWS_S3_BUCKET = os.getenv("AWS_S3_BUCKET")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_SESSION_TOKEN = os.getenv("AWS_SESSION_TOKEN")

books_df = pd.DataFrame(
    data={"Title": ["Book I", "Book II", "Book III"], "Price": [56.6, 59.87, 74.54]},
    columns=["Title", "Price"],
)

key = "files/books.csv"

books_df.to_csv(
    f"s3://{AWS_S3_BUCKET}/{key}",
    index=False,
    storage_options={
        "key": AWS_ACCESS_KEY_ID,
        "secret": AWS_SECRET_ACCESS_KEY,
        "token": AWS_SESSION_TOKEN,
    },
)



Read a CSV file on S3 into a pandas data frame
---------------------------------

"""
Demo script for reading a CSV file from S3 into a pandas data frame using the boto3 library
"""

import os

import boto3
import pandas as pd


AWS_S3_BUCKET = os.getenv("AWS_S3_BUCKET")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_SESSION_TOKEN = os.getenv("AWS_SESSION_TOKEN")

s3_client = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    aws_session_token=AWS_SESSION_TOKEN,
)

response = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key="files/books.csv")

status = response.get("ResponseMetadata", {}).get("HTTPStatusCode")

if status == 200:
    print(f"Successful S3 get_object response. Status - {status}")
    books_df = pd.read_csv(response.get("Body"))
    print(books_df)
else:
    print(f"Unsuccessful S3 get_object response. Status - {status}")




Using s3fs-supported pandas API
---------------------------------
"""
Demo script for reading a CSV file from S3 into a pandas data frame using s3fs-supported pandas
APIs
"""

import os

import pandas as pd

AWS_S3_BUCKET = os.getenv("AWS_S3_BUCKET")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_SESSION_TOKEN = os.getenv("AWS_SESSION_TOKEN")

key = "files/books.csv"

books_df = pd.read_csv(
    f"s3://{AWS_S3_BUCKET}/{key}",
    storage_options={
        "key": AWS_ACCESS_KEY_ID,
        "secret": AWS_SECRET_ACCESS_KEY,
        "token": AWS_SESSION_TOKEN,
    },
)

print(books_df)



##############################################################################
# How To Write Pandas Dataframe As CSV To S3 Using Boto3 Python â€“ Definitive Guide
##############################################################################


https://www.stackvidhya.com/write-pandas-dataframe-as-csv-to-s3-using-boto3/


from sklearn import datasets
import pandas as pd
iris = datasets.load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df



Using To_CSV() And S3 Path
---------------------------------
df.to_csv("s3://stackvidhya/df_new.csv",
          storage_options={'key': '<your_access_key_id>',
                           'secret': '<your_secret_access_key>'})

print("Dataframe is saved as CSV in S3 bucket.")





Using Object.Put()
---------------------------------
from io import StringIO
import boto3


#Creating Session With Boto3.
session = boto3.Session(
aws_access_key_id='<your_access_key_id>',
aws_secret_access_key='<your_secret_access_key>'
)

#Creating S3 Resource From the Session.
s3_res = session.resource('s3')

csv_buffer = StringIO()
df.to_csv(csv_buffer)
bucket_name = 'stackvidhya'
s3_object_name = 'df.csv'
s3_res.Object(bucket_name, s3_object_name).put(Body=csv_buffer.getvalue())
print("Dataframe is saved as CSV in S3 bucket.")




##############################################################################
#       Upload a Pandas Dataframe to AWS S3 With Ease
##############################################################################

https://www.mayn.es/post/2021-08-31-upload-pandas-df-to-s3/
https://stackoverflow.com/questions/42809096/difference-in-boto3-between-resource-client-and-session
https://finbox.in/blog/a-compact-way-to-store-your-dataframes-to-s3-directly-from-python/
https://towardsai.net/p/data-science/aws-s3-read-write-operations-using-the-pandas-api

from io import BytesIO
import boto3
import pandas
from pandas import util
df = util.testing.makeMixedDataFrame()
s3_resource = boto3.resource("s3")
buffer = BytesIO()
df.to_csv(buffer, sep=",", index=False, mode="wb", encoding="UTF-8")
df.seek(0)  # Make sure the stream position is at the beginning!
s3_resource.Object("test-bucket", "test_df_from_resource.csv").put(Body=buffer.getvalue())


from io import BytesIO
import boto3
import pandas
from pandas import util
df = util.testing.makeMixedDataFrame()
s3_client = boto3.client("s3")
campaign_buffer = BytesIO()
df.to_csv(campaign_buffer, sep=",", index=False, mode="wb", encoding="UTF-8")
df.seek(0)
s3_client.upload_fileobj(campaign_buffer, Bucket="test-bucket", Key="test_df_from_client.csv")




##############################################################################
#       Write a Pandas dataframe to Parquet format on AWS S3.
##############################################################################

https://gist.github.com/jitsejan/557124bcbaf0780ab4efc6054199550a


write_dataframe_to_parquet_on_s3.py
# Note: make sure `s3fs` is installed in order to make Pandas use S3.
#       Credentials for AWS in the normal location ~/.aws/credentials
def _write_dataframe_to_parquet_on_s3(dataframe, filename):
    """ Write a dataframe to a Parquet on S3 """
    print("Writing {} records to {}".format(len(dataframe), filename))
    output_file = f"s3://{DESTINATION}/{filename}/data.parquet"
    dataframe.to_parquet(output_file)




##############################################################################
#           Save Dataframe to csv directly to s3 Python
##############################################################################

https://stackoverflow.com/questions/38154040/save-dataframe-to-csv-directly-to-s3-python

import pandas as pd
df = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])
df
df.to_csv('s3://experimental/playground/temp_csv/dummy.csv', index=False)
pd.__version__
new_df = pd.read_csv('s3://experimental/playground/temp_csv/dummy.csv')
new_df

.......

import s3fs
bytes_to_write = df.to_csv(None).encode()
fs = s3fs.S3FileSystem(key=key, secret=secret)
with fs.open('s3://bucket/path/to/file.csv', 'wb') as f:
    f.write(bytes_to_write)


import s3fs
s3 = s3fs.S3FileSystem(anon=False)
# Use 'w' for py3, 'wb' for py2
with s3.open('<bucket-name>/<filename>.csv','w') as f:
    df.to_csv(f)


import awswrangler as wr
wr.s3.to_csv(
    df=df,
    path="s3://...",
)

.......

from io import StringIO
import boto3
s3 = boto3.client("s3",\
                  region_name=region_name,\
                  aws_access_key_id=aws_access_key_id,\
                  aws_secret_access_key=aws_secret_access_key)
csv_buf = StringIO()
df.to_csv(csv_buf, header=True, index=False)
csv_buf.seek(0)
s3.put_object(Bucket=bucket, Body=csv_buf.getvalue(), Key='path/test.csv')

.......

import awswrangler as wr
import pandas as pd
# read a local dataframe
df = pd.read_parquet('my_local_file.gz')
# upload to S3 bucket
wr.s3.to_parquet(df=df, path='s3://mys3bucket/file_name.gz')

.......

from io import StringIO # python3; python2: BytesIO
import boto3
bucket = 'my_bucket_name' # already created on S3
csv_buffer = StringIO()
df.to_csv(csv_buffer)
s3_resource = boto3.resource('s3')
s3_resource.Object(bucket, 'df.csv').put(Body=csv_buffer.getvalue())





############################################################################
localstack full
############################################################################

https://hub.docker.com/u/localstack

https://hub.docker.com/r/localstack/localstack
https://hub.docker.com/r/localstack/localstack-light
https://hub.docker.com/r/localstack/localstack-full
https://hub.docker.com/r/localstack/lambda-provided
https://hub.docker.com/r/localstack/lambda-nodejs
https://hub.docker.com/r/localstack/lambda-python
https://hub.docker.com/r/localstack/localstack-docker-desktop
https://hub.docker.com/r/localstack/localstack-docker-extension
https://hub.docker.com/r/localstack/lambda
https://hub.docker.com/r/localstack/lambda-js
https://hub.docker.com/r/localstack/lambda-runtime-python

....

https://github.com/localstack/localstack
https://github.com/localstack/localstack/blob/master/Dockerfile
https://github.com/localstack/localstack-docker-extension
https://github.com/localstack/localstack/issues/4535
https://rochisha-jaiswal70.medium.com/using-aws-lambda-with-amazon-simple-queue-service-bb0694257a2b
https://leviwheatcroft.github.io/selfhosted-awesome-unlist/localstack.html



....

ERROR WARNING

It seems you are mounting the LocalStack volume into /tmp/localstack.

2022-11-17T09:49:47.974795895Z   This will break the LocalStack container! Please update your volume mount
2022-11-17T09:49:47.974797747Z   destination to /var/lib/localstack. In the meantime, we have set
2022-11-17T09:49:47.974799591Z   LEGACY_DIRECTORIES=1, to make LocalStack behave as in <1.0.0, and which
2022-11-17T09:49:47.974801794Z   should prevent any serious issues, but will be removed soon!
2022-11-17T09:49:47.974803653Z   You can suppress this warning by setting LEGACY_DIRECTORIES=1


Waiting for all LocalStack services to be ready
2022-11-17T09:52:05.030411302Z 2022-11-17 09:52:05,030 CRIT Supervisor is running as root.  Privileges were not dropped because no user is specified in the config file.  If you intend to run as root, you can set user=root in the config file to avoid this message.
2022-11-17T09:52:05.031273721Z 2022-11-17 09:52:05,031 INFO supervisord started with pid 16
2022-11-17T09:52:06.035483764Z 2022-11-17 09:52:06,034 INFO spawned: 'infra' with pid 21
2022-11-17T09:52:06.199050091Z SERVICES variable is ignored if EAGER_SERVICE_LOADING=0.



....

version: "3"

services:
  localstack:
    image: localstack/localstack-full:latest
    container_name: localstack
    ports:
      - "443:443"
      - "4566:4566"
      - "4571:4571"
      - "${PORT_WEB_UI-8080}:${PORT_WEB_UI-8080}"
    environment:
      - SERVICES=s3,cognito,amplify,lambda,iam,cloudwatch,cloudformation,sts
      - DEFAULT_REGION=ap-northeast-2
      - HOSTNAME=localhost
      - DATA_DIR=/tmp/localstack/data
      - DOCKER_HOST=unix:///var/run/docker.sock
      - LOCALSTACK_API_KEY=
      - PORT_WEB_UI=${PORT_WEB_UI- }
      - START_WEB=1

    volumes:
      - "${TMPDIR:-/tmp/localstack}:/var/lib/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"

....

version: '2.1'services:
  localstack:
    container_name: "localstack-image"
    image: localstack/localstack-full
    network_mode: bridge
    ports:
      - "4566:4566"
      - "4571:4571"
      - "8082:8082"
    environment:
      - USE_LIGHT_IMAGE=0
      - DEBUG=1
      - PORT_WEB_UI=8082
      - LAMBDA_EXECUTOR=local
      - DOCKER_HOST=unix:///var/run/docker.sock
      - HOST_TMP_FOLDER=${TMPDIR}
      - START_WEB=1
    volumes:
      - "${TMPDIR:-/tmp/localstack}:/var/lib/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"


docker-compose up

aws iam create-role --role-name lambda-ex --assume-role-policy-document "{"Version": "2012-10-17","Statement": [{ "Effect": "Allow", "Principal": {"Service": "lambda.amazonaws.com"}, "Action": "sts:AssumeRole"}]}"  --endpoint-url http://localhost:4566

aws iam create-policy --policy-name my-policy --policy-document file://policy.txt --endpoint-url http://localhost:4566

aws iam attach-role-policy --policy-arn arn:aws:iam::000000000000:policy/my-policy --role-name lambda-ex --endpoint-url http://localhost:4566

aws lambda create-function --function-name my-math-function --zip-file fileb://blank-java-1.0-SNAPSHOT.jar --handler example.Handler::handleRequest --runtime java8 --role arn:aws:iam::000000000000:role/lambda-ex --endpoint-url http://localhost:4566

aws sqs create-queue --queue-name MyQueue --endpoint-url http://localhost:4566

aws sqs get-queue-attributes --queue-url http://localhost:4566/000000000000/MyQueue --attribute-names All --endpoint-url http://localhost:4566

aws lambda create-event-source-mapping --function-name my-math-function --batch-size 5 --maximum-batching-window-in-seconds 60  --event-source-arn arn:aws:sqs:us-east-1:000000000000:MyQueue --endpoint-url http://localhost:4566

aws sqs send-message --queue-url  http://localhost:4566/000000000000/MyQueue  --message-body "Successfull!!" --endpoint-url http://localhost:4566
