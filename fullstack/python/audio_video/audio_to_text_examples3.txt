# Resample

import wave

obj = wave.open("file.wav","rb")
print(obj.getnchannels())
print(obj.getsamplewidth())
print(obj.getframerate())
frames = obj.readframes(-1)
obj.close()

new_obj = wave.open("file.wav","rb")
new_obj.setnchannels(1)
new_obj.setframerate(16000.0)
new_obj.writeFrames(frames)
new_obj.close()

import wave
import matplotlib.pyplot as plt
import numpy as np

obj = wave.open("file.wav","rb")
sample_freq = obj.getframerate()
number_samples = obj.getnframes()
signal_wave = obj.readframes(-1)
obj.close()

t_audio = number_samples / sample_freq
signal_array = np.frombuffer(signal_wave,dtype=np.int16)
times = np.linspace(0,t_audio, num = number_samples)

plt.figure(figsize=(15,5))
plt.plot(times,signal_array)
plt.title("audio")
plt.ylabel("signal")
plt.xlabel("time")
plt.lim(0,t_audio)
plt.show()


------------------------------------------------------------------------------


https://www.kaggle.com/code/robikscube/working-with-audio-in-python/notebook


import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns

from glob import glob

import librosa
import librosa.display
import IPython.display as ipd

from itertools import cycle

sns.set_theme(style="white", palette=None)
color_pal = plt.rcParams["axes.prop_cycle"].by_key()["color"]
color_cycle = cycle(plt.rcParams["axes.prop_cycle"].by_key()["color"])


audio_files = glob('../input/ravdess-emotional-speech-audio/*/*.wav')
# Play audio file
ipd.Audio(audio_files[0])


y, sr = librosa.load(audio_files[0])
print(f'y: {y[:10]}')
print(f'shape y: {y.shape}')
print(f'sr: {sr}')

# -----------------------
pd.Series(y).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio Example',
                 color=color_pal[0])
plt.show()

# -----------------------
# Trimming leading/lagging silence
# -----------------------

y_trimmed, _ = librosa.effects.trim(y, top_db=20)
pd.Series(y_trimmed).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio Trimmed Example',
                 color=color_pal[1])
plt.show()

# -----------------------
# -----------------------
pd.Series(y[30000:30500]).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio Zoomed In Example',
                 color=color_pal[2])
plt.show()


# -----------------------
# Spectogram
# -----------------------

D = librosa.stft(y)
S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
S_db.shape

# Plot the transformed audio data
fig, ax = plt.subplots(figsize=(10, 5))
img = librosa.display.specshow(S_db,
                              x_axis='time',
                              y_axis='log',
                              ax=ax)
ax.set_title('Spectogram Example', fontsize=20)
fig.colorbar(img, ax=ax, format=f'%0.2f')
plt.show()

# Mel Spectogram
S = librosa.feature.melspectrogram(y=y,
                                   sr=sr,
                                   n_mels=128 * 2,)
S_db_mel = librosa.amplitude_to_db(S, ref=np.max)
fig, ax = plt.subplots(figsize=(10, 5))
# Plot the mel spectogram
img = librosa.display.specshow(S_db_mel,
                              x_axis='time',
                              y_axis='log',
                              ax=ax)
ax.set_title('Mel Spectogram Example', fontsize=20)
fig.colorbar(img, ax=ax, format=f'%0.2f')
plt.show()




########################################################
pydub 0.25.1
########################################################

pip install pydub

https://pypi.org/project/pydub/
https://audiosegment.readthedocs.io/en/latest/audiosegment.html
https://github.com/jiaaro/pydub
https://pydub.com/
https://github.com/jiaaro/pydub/blob/master/API.markdown

# export
song = AudioSegment.from_file("/path/to/sound.mp3", format="mp3")
song = AudioSegment.from_ogg("never_gonna_give_you_up.ogg")
first_10_seconds = song[:10000]
last_5_seconds = song[-5000:]
without_the_middle = first_10_seconds + last_5_seconds
do_it_over = without_the_middle * 2
do_it_over.export("mashup.ogg", format="ogg")


# slice
from pydub import AudioSegment
sound = AudioSegment.from_file("/path/to/sound.wav", format="wav")
# split sound in 5-second slices and export
for i, chunk in enumerate(sound[::5000]):
  with open("sound-%s.mp3" % i, "wb") as f:
    chunk.export(f, format="mp3")


# append
from pydub import AudioSegment
sound1 = AudioSegment.from_file("sound1.wav")
sound2 = AudioSegment.from_file("sound2.wav")
# default 100 ms crossfade
combined = sound1.append(sound2)



https://github.com/librosa/librosa
https://librosa.org/doc/main/generated/librosa.load.html
https://github.com/librosa/librosa#audioread-and-mp3-support
https://www.kaggle.com/code/jagamts1/librosa-read-audio-files
https://www.programcreek.com/python/example/98223/librosa.load

python -m pip install librosa
pip install librosa
apt-get install ffmpeg
apt-get install gstreamer1.0-plugins-base gstreamer1.0-plugins-ugly

import librosa
y, sr = librosa.load('your_file.mp3', sr=11025, offset=15.0, duration=5.0)


audio, sfreq = librosa.load(audio_file)
time = np.arange(0, len(audio))/sfreq
plt.plot(time,audio)

for file in range(0, len(audio_files),1):
    audio, sfreq = lr.load(audio_files[file])
    fig,ax = plt.subplots()
    time = np.arange(0,len(audio))/sfreq
    ax.plot(time,audio)
    plt.show()



audio = './data/esc10/audio/Dog/1-30226-A.ogg'
y, sr = librosa.load(audio, sr=44100)
y_ps = librosa.effects.pitch_shift(y, sr, n_steps=6)  # n_steps
y_ts = librosa.effects.time_stretch(y, rate=1.2)  # rate
plt.subplot(311)
plt.plot(y)
plt.title('Original waveform')
plt.axis([0, 200000, -0.4, 0.4])
# plt.axis([88000, 94000, -0.4, 0.4])
plt.subplot(312)
plt.plot(y_ts)
plt.title('Time Stretch transformed waveform')
plt.axis([0, 200000, -0.4, 0.4])
plt.subplot(313)
plt.plot(y_ps)
plt.title('Pitch Shift transformed waveform')
plt.axis([0, 200000, -0.4, 0.4])
# plt.axis([88000, 94000, -0.4, 0.4])
plt.tight_layout()
plt.show()


wavs = list()
for file in os.listdir(wav_dir):
	file_path = os.path.join(wav_dir, file)
	wav, _ = librosa.load(file_path, sr = sr, mono = True)
	#wav = wav.astype(np.float64)
	wavs.append(wav)

