###################################################
#
# tensorflow jupyter install 2019-01-10
#
###################################################

sudo apt install python-pip -y
pip install tensorflow

# Current release for CPU-only
pip install tensorflow

# Nightly build for CPU-only (unstable)
pip install tf-nightly

# GPU package for CUDA-enabled GPU cards
pip install tensorflow-gpu

# Nightly build with GPU support (unstable)
pip install tf-nightly-gpu

docker pull tensorflow/tensorflow                  # Download latest image
docker run -it -p 8888:8888 tensorflow/tensorflow  # Start a Jupyter notebook server

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.tensorflow.org/install/
https://www.tensorflow.org/tutorials/



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Get Started with TensorFlow
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

python tfstart.py
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
11501568/11490434 [==============================] - 0s 0us/step
Epoch 1/5
60000/60000 [==============================] - 9s 146us/step - loss: 0.2006 - acc: 0.9411
Epoch 2/5
60000/60000 [==============================] - 11s 176us/step - loss: 0.0804 - acc: 0.9757
Epoch 3/5
60000/60000 [==============================] - 9s 148us/step - loss: 0.0515 - acc: 0.9841
Epoch 4/5
60000/60000 [==============================] - 10s 164us/step - loss: 0.0360 - acc: 0.9885
Epoch 5/5
60000/60000 [==============================] - 10s 162us/step - loss: 0.0268 - acc: 0.9917
10000/10000 [==============================] - 0s 43us/step

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

###################################################
#
#   jupyter install
#
###################################################

sudo apt install jupyter-core -y
pip install  jupyter
reboot
jupyter notebook --port 9999
pip install --upgrade pip
pip install matplotlib
pip list

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.tensorflow.org/tutorials/keras/overfit_and_underfit
https://www.tensorflow.org/tutorials/keras/basic_regression
https://www.tensorflow.org/tutorials/keras/basic_text_classification

https://github.com/tensorflow/docs/tree/master/site/en/tutorials/images
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/_image_classification.ipynb
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/_pix2pix.ipynb
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/deep_cnn.md

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/index.md
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_regression.ipynb
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_restore_models.ipynb


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://pytorch.org/get-started/locally/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.tensorflow.org/tutorials/
https://www.tensorflow.org/tutorials/eager/custom_training
https://www.tensorflow.org/tutorials/keras/save_and_restore_models
https://www.tensorflow.org/tutorials/keras/save_and_restore_models#get_an_example_dataset
https://www.tensorflow.org/tutorials/keras/save_and_restore_models#installs_and_imports
https://www.tensorflow.org/tutorials/keras/save_and_restore_models#define_a_model
https://www.tensorflow.org/tutorials/keras/save_and_restore_models#options
https://www.tensorflow.org/tutorials/keras/save_and_restore_models#define_a_model
https://www.tensorflow.org/guide/datasets_for_estimators
https://www.tensorflow.org/guide/datasets
https://www.tensorflow.org/guide/datasets
https://www.tensorflow.org/api_docs/python/tf/image/decode_image
https://www.tensorflow.org/api_docs/python/tf/image/decode_image

Dogs vs. Cats Redux: Kernels Edition
https://www.kaggle.com/freeman89/create-dataset-with-tensorflow
https://www.kaggle.com/freeman89/create-dataset-with-tensorflow/data
https://www.kaggle.com/jangho/create-dataset-with-tensorflow-d915e134
https://www.kaggle.com/heslicia/create-dataset-with-tensorflow

https://support.labelbox.com/docs/examplepy-tensorflow
http://docs.w3cub.com/tensorflow~python/tf/image/decode_image/
https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/
https://medium.com/ymedialabs-innovation/how-to-use-dataset-and-iterators-in-tensorflow-with-code-samples-3bb98b6b74ab
https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

import tensorflow as tf
import numpy as np

print ("TF Version: %s" % tf.__version__)

import urllib

png16 = urllib.request.urlopen('http://www.schaik.com/pngsuite/basn0g16.png').read(1000)
with tf.Session() as session:
    content = tf.placeholder(tf.string)
    tensor = tf.image.decode_image(
        contents=content,
        channels=1
    )

    out = (session.run(tensor, {content: png16}))
    np_out = np.array(out)
    print ('Shape', np_out.shape)
    print ('Max', np_out.max())

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Outputs:

TF Version: 1.4.1
Shape (32, 32, 1)
Max 255

python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

filenames = ['/Users/darshak/TensorFlow/100.jpg', '/Users/darshak/TensorFlow/10.jpg']
filename_queue = tf.train.string_input_producer(filenames)
reader = tf.WholeFileReader()
filename, content = reader.read(filename_queue)
#image = tf.image.decode_jpeg(content, channels=3)
#image = tf.cast(image, tf.float32)
image = tf.image.decode_jpeg(content, channels=3)
image = tf.cast(image, tf.float32) / 255.0
resized_image = tf.image.resize_images(image, [256, 256])
image_batch = tf.train.batch([resized_image], batch_size=9)
sess = tf.InteractiveSession()
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)
plt.imshow(image.eval())
plt.show()
sess.close()


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

KAGGLE COMPETITIONS DOWNLOAD -C DOGS-VS-CATS-REDUX-KERNELS-EDITION
https://github.com/micahstubbs/dogs-vs-cats-redux-kernels-edition
https://github.com/RomanKornev/dogs-vs-cats-redux
https://gist.github.com/wontheone1/04b8329ef25543160c31c1e8f1c6c91b
https://github.com/aquatiko/Dog-vs-Cat-Redux-Kernel-Edition-Transfer-Learning
https://github.com/karantan/Dogs-vs-Cats#
https://github.com/aquatiko/Dog-vs-Cat-Redux-Kernel-Edition-Transfer-Learning/blob/master/Dogs%20vs%20Cats.ipynb
https://wilsonmar.github.io/data-sources/
https://pythonprogramming.net/convolutional-neural-network-kats-vs-dogs-machine-learning-tutorial/
https://medium.com/anubhav-shrimal/dogs-vs-cats-image-classification-using-resnet-d2ed7e6db2bb
https://github.com/datitran/Dogs-vs-Cats/blob/master/README.md
https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/train.zip

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


pip install kaggle-cli
kg download –u user@example.com –p password –c dogs-vs-cats-redux-kernels-edition
kg download -u <username> -p <password> -c <competition>
kg download -u <username> -p <password> -c <competition> -f train.zip

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dogs vs. Cats Redux: Kernels Edition - GitHub
https://github.com/karantan/Dogs-vs-Cats
https://github.com/DavexPro/dogs-vs-cats

http://files.fast.ai/data/dogscats.zip
unzip dogscats.zip

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~