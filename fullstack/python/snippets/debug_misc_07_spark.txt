
##########################################################
install spark
##########################################################

https://stackoverflow.com/questions/32967805/sqlcontext-object-has-no-attribute-read-while-reading-csv-in-pyspark/32967849
https://stackoverflow.com/questions/65375537/function-object-has-no-attribute-read-on-jupyter
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/
https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/
https://stackoverflow.com/questions/41254011/sparksql-read-parquet-file-directly
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://spark.apache.org/docs/1.6.1/sql-programming-guide.html


python -m pip install spark
python -m pip install pyspark

parquetFile = spark.read.parquet("people.parquet")

module 'spark' has no attribute 'read'

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
df = sqlContext.load(source="com.databricks.spark.csv", header="true", path = "cars.csv")
df.select("year", "model").save("newcars.csv", "com.databricks.spark.csv")



import pyspark
conf = pyspark.SparkConf()
# conf.set('spark.app.name', app_name) # Optional configurations
# init & return
sc = pyspark.SparkContext.getOrCreate(conf=conf)
sqlcontext = SQLContext(sc)
df = sqlcontext.read.json('random.json')



df = sqlContext.read.parquet("src/main/resources/peopleTwo.parquet")
df.printSchema
// after registering as a table you will be able to run sql queries
df.registerTempTable("people")
sqlContext.sql("select * from people").collect.foreach(println)


from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)



from pyspark.sql import SQLContext
sc = SparkContext.getOrCreate()
sqlContext = SQLContext(sc)
df = sqlContext.read.json("examples/src/main/resources/people.json")
# Displays the content of the DataFrame to stdout
df.show()


from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
df = sqlContext.sql("SELECT * FROM table")

JAVA_HOME is not set

sudo apt install openjdk-11-jre-headless  # version 11.0.15+10-0ubuntu0.22.04.1, or
sudo apt install default-jre              # version 2:1.11-72build2
sudo apt install openjdk-17-jre-headless  # version 17.0.3+7-0ubuntu0.22.04.1
sudo apt install openjdk-18-jre-headless  # version 18~36ea-1
sudo apt install openjdk-8-jre-headless   # version 8u312-b07-0ubuntu1


--------------------------------------------------------------
####################################################
PySpark Read and Write Parquet File
####################################################

https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/
https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/
https://www.projectpro.io/recipes/read-and-write-parquet-files-pyspark

parquetFile = spark.read.parquet("people.parquet")

df.write.parquet("/tmp/out/people.parquet")
parDF1=spark.read.parquet("/temp/out/people.parquet")

--------------------------------------------------------------
####################################################
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>
####################################################

22/08/17 12:34:24 WARN Utils: Your hostname,xxx resolves to a loopback address: 127.0.1.1; using 192.168.0.115 instead (on interface wlp0s20f3)
22/08/17 12:34:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/08/17 12:34:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

22/08/17 13:07:43 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory
Scaling row group sizes to 95,00% for 8 writers
22/08/17 13:11:24 WARN TaskSetManager: Stage 2 contains a task of very large size (8975 KiB). The maximum recommended task size is 1000 KiB.
22/08/17 13:11:25 ERROR Inbox: An error happened while processing message in the inbox for LocalSchedulerBackendEndpoint
java.lang.OutOfMemoryError: Java heap space


convert ALL columns to strings, you can simply use:
df = df.astype(str)

df[["D", "E"]] = df[["D", "E"]].astype(int)
total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)

------------------------------------------------------------------------

TypeError: Sparse pandas data (column anb_em_ch_ken) not supported

https://pandas.pydata.org/docs/user_guide/sparse.html
https://arrow.apache.org/docs/python/dataset.html
https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html

------------------------------------------------------------------------

####################################################
Read data from SQLite
####################################################

https://vinta.ws/code/spark-sql-cookbook-pyspark.html

$ pyspark --packages "org.xerial:sqlite-jdbc:3.16.1"

from pyspark.sql.functions import lit
from pyspark.sql.types import IntegerType, StructField, StructType, TimestampType

props = {'driver': 'org.sqlite.JDBC', 'date_string_format': 'yyyy-MM-dd HH:mm:ss'}
df = spark.read.jdbc("jdbc:sqlite:db.sqlite3", "app_repostarring", properties=props)

df = df.where(df.stargazers_count >= min_stargazers_count)
df = df.select('from_user_id', 'repo_id', 'created_at')
df = df.toDF('user', 'item')
df = df.withColumn('rating', lit(1))

schema = StructType([
    StructField('user', IntegerType(), nullable=False),
    StructField('item', IntegerType(), nullable=False),
    StructField('rating', IntegerType(), nullable=False),
    StructField('item_created_at', TimestampType(), nullable=False),
])
df = spark.createDataFrame(df.rdd, schema)


# Read data from parquet
from pyspark.sql.utils import AnalysisException

raw_df_filepath = 'raw_df.parquet'

try:
    raw_df = spark.read.format('parquet').load(raw_df_filepath)
except AnalysisException as exc:
    if 'Path does not exist' in exc.desc:
        raw_df = load_raw_data()
        raw_df.write.format('parquet').save(raw_df_filepath)
    else:
        raise exc


Create a DataFrame
matrix = [
    (1, 1, 1),
    (1, 2, 1),
    (2, 1, 0),
    (3, 1, 1),
    (3, 3, 1),
    (3, 4, 1),
    (4, 1, 0),
    (4, 2, 0),
    (5, 9, 1),
    (5, 5, 0),
]
df = spark.createDataFrame(matrix, ['user', 'item', 'rating'])


Create a DataFrame with explicit schema
from pyspark.sql.types import *

matrix = [
    ('Alice', 0.5, 5.0),
    ('Bob',   0.2, 92.0),
    ('Tom',   0.0, 122.0),
    ('Zack',  0.1, 1.0),
]
schema = StructType([
    StructField('name', StringType(), nullable=False),
    StructField('prediction', DoubleType(), nullable=False),
    StructField('rating', DoubleType(), nullable=False)
])
df = spark.createDataFrame(matrix, schema)
df.printSchema()
new_df = spark.createDataFrame(someRDD, df.schema)




# Get numbers of partitions
-----------------------------------------------
df.rdd.getNumPartitions()


# Split a DataFrame into chunks (partitions)
-----------------------------------------------
def write_to_db(partial_rows):
    values_tuples = [(row.user, row.item, row.score, year, week) for row in chunk_rows]
    result = your_mysql_insert_func(values_tuples)
    return [result, ]

save_result = df \
    .repartition(400) \
    .rdd \
    .mapPartitions(write_to_db) \
    .collect()



# Show a DataFrame
-----------------------------------------------
# print the schema in a tree format
df.printSchema()

# print top 20 rows
df.show()

# print top 100 rows
df.show(100)

# calculate the descriptive statistics
df.describe().show()
df.describe(['like_count', ]).show()


# Create a column with a literal value
-----------------------------------------------
from pyspark.sql.functions import lit
df = df.withColumn('like', lit(1))


Return a fraction of a DataFrame
-----------------------------------------------
fraction = {
    'u1': 0.5,
    'u2': 0.5,
    'u3': 0.5,
    'u4': 0.5,
    'u5': 0.5,
}
df.sampleBy('user', fraction).show()



Show distinct values of a column
-----------------------------------------------
df.select('user').distinct().show()



Rename columns
-----------------------------------------------
df.printSchema()
df = df.toDF('user', 'item')
# or
df = df.withColumnRenamed('from_user_id', 'user').withColumnRenamed('repo_id', 'item')


Convert a column to double type
-----------------------------------------------
df = df.withColumn('prediction', df['prediction'].cast('double'))


Update a colume based on conditions
-----------------------------------------------
from pyspark.sql.functions import when
df = df.withColumn('label', when(df['prediction'] > 0.5, 1).otherwise(0))



Drop columns from a DataFrame
-----------------------------------------------
predictions = predictions.dropna(subset=['prediction', S])



DataFrame subtract another DataFrame
-----------------------------------------------
matrix1 = [
    (1, 2, 123),
    (3, 4, 1),
    (5, 6, 45),
    (7, 8, 3424),
]
df1 = spark.createDataFrame(matrix1, ['user','item', 'play_count'])

matrix2 = [
    (3, 4),
    (5, 6),
    (9, 1),
    (7, 8),
    (1, 6),
    (1, 1),
]
df2 = spark.createDataFrame(matrix2, ['user','item'])
df2.subtract(df1.select('user', 'item')).show()
# or
testing_rdd = df.rdd.subtract(training.rdd)
testing = spark.createDataFrame(testing_rdd, df.schema)




Convert a DataFrame column into a Python list
-----------------------------------------------
# list
popluar_items = [row['item'] for row in popular_df.select('item').collect()]

# set
all_items = {row.id for row in als_model.itemFactors.select('id').collect()}


Concatenate (merge) two DataFrames
-----------------------------------------------
full_df = df1.union(df2)


Convert a DataFrame to a Python dict
-----------------------------------------------
d = df.rdd.collectAsMap()
d['some_key']


Compute (approximate or exact) median of a numerical column
-----------------------------------------------
approximate_median = df.approxQuantile('count', [0.5, ], 0.25)
exact_median = df.approxQuantile('count', [0.5, ], 0.0)
maximum = df.approxQuantile('count', [1.0, ], 0.1)
minimum = df.approxQuantile('count', [0.0, ], 0.1)


Find frequent items for columns
-----------------------------------------------
df = rating_df.freqItems(['item', ], support=0.01)
popular_items = df.collect()[0].item_freqItems


Broadcast a value
-----------------------------------------------
bc_candidates = sc.broadcast(set([1, 2, 4, 5, 8]))
print(bc_candidates.value)
bc_df = sc.broadcast(df.collect())
df = spark.createDataFrame(bc_df.value)



Broadcast a DataFrame in join
-----------------------------------------------
import pyspark.sql.functions as F
large_df.join(F.broadcast(small_df), 'some_key')


Cache a DataFrame
-----------------------------------------------
df.cache()


Show query execution plan
-----------------------------------------------
df.explain(extended=True)




Use SQL to query a DataFrame
-----------------------------------------------
props = {'driver': 'org.sqlite.JDBC', 'date_string_format': 'yyyy-MM-dd HH:mm:ss'}
df = spark.read.jdbc("jdbc:sqlite:db.sqlite3", "app_repostarring", properties=props)
df.createOrReplaceTempView('repo_stars')

query = 'SELECT DISTINCT repo_id AS item FROM repo_stars WHERE stargazers_count > 1000'
df2 = spark.sql(query)
df2.show()

query = """
SELECT
    from_user_id AS user,
    count(repo_id) AS count
FROM repo_stars
GROUP BY from_user_id
ORDER BY count DESC
"""
df = spark.sql(query)

params = {'top_n': top_n}
query = """
SELECT
    repo_id AS item,
    MAX(stargazers_count) AS stars
FROM repo_stars
GROUP BY repo_id
ORDER BY stars DESC
LIMIT {top_n}
""".format(**params)
popular_df = spark.sql(query)




WHERE ... IN ...
-----------------------------------------------
from pyspark.sql.functions import col

item_ids = [1, 2, 5, 8]
raw_df \
    .where(col('repo_id').isin(item_ids)) \
    .select('repo_url') \
    .collectAsMap()




ORDER BY multiple columns
-----------------------------------------------
import pyspark.sql.functions as F

rating_df = raw_df \
    .selectExpr('from_user_id AS user', 'repo_id AS item', '1 AS rating', 'starred_at') \
    .orderBy('user', F.col('starred_at').desc())


Aggregate
-----------------------------------------------
df.agg(min('user'), max('user'), min('item'), max('item')).show()
max_value = user_star_count_df.agg(F.max('stars')).collect()[0]["max('stars')"]



SELECT COUNT(DISTINCT ) ...
-----------------------------------------------
import pyspark.sql.functions as F
matrix = [
    (1, 1, 1),
    (1, 2, 1),
    (2, 1, 0),
    (3, 1, 1),
    (3, 3, 1),
    (3, 4, 1),
    (4, 1, 0),
    (4, 2, 0),
    (5, 9, 1),
    (5, 5, 0),
]
df = spark.createDataFrame(matrix, ['user', 'item', 'rating'])
df.agg(F.countDistinct('user')).show()



SELECT MAX(xxx) ... GROUP BY
-----------------------------------------------
df.groupBy('user').count().filter('count >= 4').show()
popular_df = raw_df \
    .groupBy('repo_id') \
    .agg(F.max('stargazers_count').alias('stars')) \
    .orderBy('stars', ascending=False) \
    .limit(top_n)



SELECT COUNT() ... GROUP BY
-----------------------------------------------
prediction_df.groupBy('user').count().show()

stargazers_count_df = rating_df \
    .groupBy('item') \
    .agg(F.count('user').alias('stargazers_count')) \
    .orderBy('stargazers_count', ascending=False)


starred_count_df = rating_df \
    .groupBy('user') \
    .agg(F.count('item').alias('starred_count')) \
    .orderBy('starred_count', ascending=False)


GROUP_CONCAT a column
-----------------------------------------------
from pyspark.sql.functions import expr
per_user_predictions_df = output_df \
  .orderBy(['user', 'prediction'], ascending=False) \
  .groupBy('user') \
  .agg(expr('collect_list(item) as items'))



GROUP_CONCAT multiple columns
-----------------------------------------------
from pyspark.sql.functions import col, collect_list, struct

matrix = [
    (1, 1, 0.1),
    (1, 2, 5.1),
    (1, 6, 0.0),
    (2, 6, 9.3),
    (3, 1, 0.54),
    (3, 5, 0.83),
    (4, 1, 0.65),
    (4, 4, 1.023),
]
df = spark.createDataFrame(matrix, ['user', 'item', 'prediction'])

df \
    .groupBy("user") \
    .agg(collect_list(struct(col('item'), col('prediction'))).alias("recommendations")) \
    .show(truncate=False)


SELECT ... RANK() OVER (PARTITION BY ... ORDER BY)
-----------------------------------------------
from pyspark.sql import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import expr
import pyspark.sql.functions as F

window_spec = Window.partitionBy('from_user_id').orderBy(col('starred_at').desc())
per_user_actual_items_df = raw_df \
    .select('from_user_id', 'repo_id', 'starred_at', F.rank().over(window_spec).alias('rank')) \
    .where('rank <= 10') \
    .groupBy('from_user_id') \
    .agg(expr('collect_list(repo_id) as items')) \
    .withColumnRenamed('from_user_id', 'user')

window_spec = Window.partitionBy('user').orderBy(col('prediction').desc())
per_user_predicted_items_df = output_df \
    .select('user', 'item', 'prediction', F.rank().over(window_spec).alias('rank')) \
    .where('rank <= 10') \
    .groupBy('user') \
    .agg(expr('collect_list(item) as items'))


Left anti join / Left excluding join
-----------------------------------------------
clean_df = rating_df.join(to_remove_items, 'item', 'left_anti')



Cross join
-----------------------------------------------
df = m1df.select('user').distinct().crossJoin(m2df.select('item'))
df.show()

query = """
SELECT f1.user, f2.item, 0 AS rating
FROM f1
CROSS JOIN f2
"""
df = spark.sql(query)
df.show()

all_user_item_pair_df = als_model.userFactors.selectExpr('id AS user') \
    .crossJoin(alsModel.itemFactors.selectExpr('id AS item'))



Outer join
-----------------------------------------------
m1 = [
    (1, 1, 1),
    (1, 2, 1),
    (1, 4, 1),
    (2, 2, 1),
    (2, 3, 1),
    (3, 5, 1),
]
m1df = spark.createDataFrame(m1, ['user', 'item', 'rating'])
m1df = m1df.where('user = 1').alias('m1df')

m2 = [
    (1, 100),
    (2, 200),
    (3, 300),
    (4, 400),
    (5, 500),
    (6, 600),
]
m2df = spark.createDataFrame(m2, ['item', 'count'])
m2df = m2df.alias('m2df')

m1df.join(m2df, m1df.item == m2df.item, 'rightouter') \
.where('m1df.user IS NULL') \
.orderBy('m2df.count', ascending=False) \
.selectExpr('1 AS user', 'm2df.item', '0 AS rating') \
.show()








------------------------------------------------------------------------


https://gist.github.com/beauzeaux/a68d6f32f4985ed547ce

import sqlite3
import os
import argparse

try:
    import pyspark
    import pyspark.sql
except ImportError:
    import sys
    import os
    # TODO: unhardcode these
    os.environ["SPARK_HOME"] = '/opt/spark'
    sys.path.append('/opt/spark/python')
    sys.path.append('/opt/spark/python/lib/py4j-0.8.2.1-src.zip')
    import pyspark
    import pyspark.sql
    from pyspark.sql.types import *


# load spark stuff
conf = pyspark.SparkConf()
conf.set('spark.executor.memory', '4g')
conf.set('spark.sql.parquet.compression.codec', 'gzip')
#conf.set('spark.sql.parquet.compression.codec', 'snappy')
sc = pyspark.SparkContext("local", conf=conf)
sqlContext = pyspark.SQLContext(sc)


def get_table_list(sqldb_loc):
    """Gets the list of tables in the sqlite database given a file path
        to the sqlite database
    """
    conn = sqlite3.connect(sqldb_loc)
    cur = conn.cursor()
    res = cur.execute("""SELECT name
                           FROM sqlite_master
                           WHERE type='table'""")
    names = [x[0] for x in res]
    cur.close()
    return names


def get_generator_from_table(sqldb_loc, table_name):
    conn = sqlite3.connect(sqldb_loc)
    cur = conn.cursor()
    res = cur.execute("""
    SELECT * FROM {0}
    """.format(table_name))
    for row in  res:
        yield row
    cur.close()
    conn.close()

def get_column_names_from_table(sqldb_loc, table_name):
    conn = sqlite3.connect(sqldb_loc)
    cur = conn.cursor()
    res = cur.execute("""
    SELECT *
    FROM {0}
    """.format(table_name))
    names = [x[0] for x in cur.description]
    cur.close()
    conn.close()
    return names

def create_df_from_generator(gen, names):
    a = sc.parallelize(gen, 20)
    a.persist(pyspark.StorageLevel(True, True, False, True, 1))
    df = sqlContext.createDataFrame(a, schema=names, samplingRatio=None).repartition(20)
    #df.persist(pyspark.StorageLevel(True, True, False, True, 1))
    return df


def sqlite2parquet(db_path, output_dir, skip_tables=['sqlite_sequence']):
    tables = get_table_list(db_path)
    for table in tables:
        if table in skip_tables:
            print "Skipping: {0}".format(table)
            continue
        print "Converting: {0}".format(table)
        gen = get_generator_from_table(db_path, table)
        if table in schemas:
            print "\t known schema"
            schema = schemas[table]
        else:
            schema = get_column_names_from_table(db_path, table)
        print "\t converting to data-frame"
        df = create_df_from_generator(gen, schema)
        p = os.path.join(output_dir, table + '.parquet')
        print "\t saving..."
        df.saveAsParquetFile(p)

def main():
    parser = argparse.ArgumentParser(description='Convert sqlite database into parquet files')
    parser.add_argument('sqlite_db_path')
    args = parser.parse_args()
    try:
        os.makedirs(args.sqlite_db_path + '.parquets')
    except OSError:
        #mmm quiet failure... brilliant!
        pass
    sqlite2parquet(args.sqlite_db_path, args.sqlite_db_path + '.parquets', skip_tables=['task', 'sqlite_sequence', 'sqlite_stat1', 'crawl'])

if __name__ == "__main__":
    main()

------------------------------------------------------------------------


http://mitzen.blogspot.com/2017/06/pyspark-working-with-jdbc-sqlite.html
https://sparkdatabox.com/tutorials/python-flask/flask-sqlite


from pyspark import SparkContext
sc = SparkContext.getOrCreate()
from pyspark.sql import SQLContext
sqlCtx = SQLContext(sc)

sqlContext.read.format("jdbc").options(url ="jdbc:sqlite:c:test.db", driver="org.sqlite.JDBC", dbtable="employee").load().take(10)



------------------------------------------------------------------------


------------------------------------------------------------------------

https://intellipaat.com/community/11477/converting-pandas-dataframe-into-spark-dataframe-error

import pyspark
from pyspark.sql import SparkSession
import pandas as pd
spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()
pdDF = pd.read_csv("samp.csv")
df = spark.createDataFrame(pdDF,schema=mySchema)


------------------------------------------------------------------------
https://cumsum.wordpress.com/2020/02/02/typeerror-field-xxx-can-not-merge-type-class-pyspark-sql-types-longtype-and-class-pyspark-sql-types-stringtype/
https://sqlwithmanoj.com/2021/04/08/python-error-while-converting-pandas-dataframe-or-python-list-to-spark-dataframe-can-not-merge-type/
https://python.hotexamples.com/examples/odo/-/odo/python-odo-function-examples.html
https://sparkbyexamples.com/hadoop/hadoop-unable-to-load-native-hadoop-library-for-your-platform-warning/
https://sparkbyexamples.com/pyspark-tutorial/
https://sleeplessbeastie.eu/2021/08/06/how-to-load-native-hadoop-libraries/
https://gist.github.com/beauzeaux/a68d6f32f4985ed547ce
https://stackoverflow.com/questions/38985350/how-to-load-table-from-sqllite-db-file-from-pyspark
https://people.duke.edu/~ccc14/sta-663-2016/21C_Spark_SQL.html
https://www.edureka.co/community/110/hadoop-unable-native-hadoop-library-your-platform-warning
------------------------------------------------------------------------
https://stackoverflow.com/questions/51874225/how-to-combine-small-parquet-files-to-one-large-parquet-file
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://gist.github.com/jomoespe/cd0dcfcb7b910e1a5df779e0e7687181
https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/
https://spark.apache.org/docs/2.3.1/sql-programming-guide.html#sql

spark\
    .read\
    .parquet('...'))\
    .repartition('key1', 'key2',...)\
    .write\
    .partitionBy('key1', 'key2',...)\
    .option('path', target_part)\
    .saveAsTable('partitioned')



val partitions = 5;   // this value depends on data and volumes. Will be different in every case.
val df = sqlContext.read.json(“URI://path/to/parquet/files/")
df.createOrReplaceTempView("df")
val df_output = spark
  .sql("SELECT DISTINCT * FROM df") // this removes duplicates. If it's not needed, simply remove this line
  .coalesce(partitions)
df_output.write.parquet("URI://path/to/destination")

------------------------------------------------------------------------





###########################################################
Setting default log level to "WARN" in Spark
###########################################################

https://stackoverflow.com/questions/54934399/setting-default-log-level-to-warn-in-spark
https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-logging.html
https://sparkbyexamples.com/spark/spark-stop-info-and-debug-logging-console/
https://kontext.tech/article/457/tutorial-turn-off-info-logs-in-spark
https://stackoverflow.com/questions/40608412/how-can-set-the-default-spark-logging-level
https://ydd9.github.io/spark/python/2017/12/07/Spark-Python3-Linux
https://support.datastax.com/s/article/Spark-hostname-resolving-to-loopback-address-warning-in-spark-worker-logs
https://stackoverflow.com/questions/25193488/how-to-turn-off-info-logging-in-spark
https://stackoverflow.com/questions/49523377/how-to-set-loglevel-in-a-pyspark-job/49523401
https://python.hotexamples.com/fr/examples/pyspark/SparkContext/setLogLevel/python-sparkcontext-setloglevel-method-examples.html

spark = SparkSession.builder.master("local").appName("test-mf").getOrCreate()
spark.sparkContext.setLogLevel("ERROR")


from pyspark.sql import SparkSession
    spark = SparkSession.builder.\
        master('local').\
        appName('foo').\
        getOrCreate()
spark.sparkContext.setLogLevel('ERROR')

val spark:SparkSession = SparkSession.builder()
    .master("local[1]")
    .appName("SparkByExamples.com")
    .getOrCreate()
spark.sparkContext.setLogLevel("ERROR")


#  Valid log levels: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN


from pyspark.sql import SparkSession
appName = "Spark - Setting Log Level"
master = "local"
# Create Spark session
spark = SparkSession.builder \
    .appName(appName) \
    .master(master) \
    .getOrCreate()
spark.sparkContext.setLogLevel("WARN")

spark = SparkSession.builder.getOrCreate()
spark.sparkContext.setLogLevel("WARN")
sc.setLogLevel("WARN")


Solve pyspark ip loopback issue
export SPARK_LOCAL_IP="<IP address>"
export SPARK_MASTER_IP="<IP address>"

ssh predix@127.0.1.1
ssh -N -L 8888:localhost:8889 predix@127.0.1.1


from pyspark import SparkContext, SparkConf
from pyspark.streaming import StreamingContext
conf = SparkConf().setMaster("localhost")
sc = SparkContext("local[*]", "camera_mechine_gen")
sc.setLogLevel("WARN")


sc = SparkContext(appName="MyApp")
sc.setLogLevel('ERROR')


###########################################################
pyspark  dataframe select column values
###########################################################

https://www.geeksforgeeks.org/how-to-get-a-value-from-the-row-object-in-pyspark-dataframe/
https://spark.apache.org/docs/latest/api/python//reference/pyspark.pandas/api/pyspark.pandas.DataFrame.set_index.html
https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/series.html
https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.values.html#pyspark.pandas.DataFrame.values
https://spark.apache.org/docs/latest/api/python/user_guide/index.html
https://spark.apache.org/docs/latest/api/python/getting_started/index.html
https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#DataFrame-Creation
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html
https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/frame.html
https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.columns.html
https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/


import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import Row

random_value_session = SparkSession.builder.appName(
    'Random_Value_Session'
).getOrCreate()

rows = [['All England Open', 'March', 'Super 1000'],
        ['Spain Masters', 'March', 'Super 300']]
columns = ['Tournament', 'Month', 'Level']
dataframe = random_value_session.createDataFrame(rows, columns)
dataframe.show()
# getting list of rows using collect()
row_list = dataframe.collect()
row_list = dataframe.select('Tournament').collect() # select single col
for row in row_list:
	print(row)
	print(Tournament.row)
dataframe.count() # count rows
dataframe.head(1) # show first row
dataframe.printSchema()
dataframe.show(1)
column_names = database.schema.fieldNames()
columns = len(database.columns) # count columns


###########################################################
PySpark Select Columns From DataFrame
###########################################################
https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/
https://sparkbyexamples.com/pyspark/pyspark-find-datatype-column-names-of-dataframe/

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data = [("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  ]
columns = ["firstname","lastname","country","state"]
df = spark.createDataFrame(data = data, schema = columns)
df.show(truncate=False)

# Select Single & Multiple Columns From PySpark

df.select("firstname","lastname").show()
df.select(df.firstname,df.lastname).show()
df.select(df["firstname"],df["lastname"]).show()

#By using col() function
from pyspark.sql.functions import col
df.select(col("firstname"),col("lastname")).show()

#Select columns by regular expression
df.select(df.colRegex("`^.*name*`")).show()

# Select All Columns From List

df.select(*columns).show()
# Select All columns
df.select([col for col in df.columns]).show()
df.select("*").show()


# Select Columns by Index

#Selects first 3 columns and top 3 rows
df.select(df.columns[:3]).show(3)

#Selects columns 2 to 4  and top 3 rows
df.select(df.columns[2:4]).show(3)

#  Select Nested Struct Columns from PySpark

data = [
        (("James",None,"Smith"),"OH","M"),
        (("Anna","Rose",""),"NY","F"),
        (("Julia","","Williams"),"OH","F"),
        (("Maria","Anne","Jones"),"NY","M"),
        (("Jen","Mary","Brown"),"NY","M"),
        (("Mike","Mary","Williams"),"OH","M")
        ]

from pyspark.sql.types import StructType,StructField, StringType
schema = StructType([
    StructField('name', StructType([
         StructField('firstname', StringType(), True),
         StructField('middlename', StringType(), True),
         StructField('lastname', StringType(), True)
         ])),
     StructField('state', StringType(), True),
     StructField('gender', StringType(), True)
     ])
df2 = spark.createDataFrame(data = data, schema = schema)
df2.printSchema()
df2.show(truncate=False) # shows all columns
df2.select("name").show(truncate=False)
df2.select("name.firstname","name.lastname").show(truncate=False)
df2.select("name.*").show(truncate=False)

Complete Example

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

data = [("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  ]

columns = ["firstname","lastname","country","state"]
df = spark.createDataFrame(data = data, schema = columns)
df.show(truncate=False)

df.select("firstname").show()

df.select("firstname","lastname").show()

#Using Dataframe object name
df.select(df.firstname,df.lastname).show()

# Using col function
from pyspark.sql.functions import col
df.select(col("firstname"),col("lastname")).show()

data = [(("James",None,"Smith"),"OH","M"),
        (("Anna","Rose",""),"NY","F"),
        (("Julia","","Williams"),"OH","F"),
        (("Maria","Anne","Jones"),"NY","M"),
        (("Jen","Mary","Brown"),"NY","M"),
        (("Mike","Mary","Williams"),"OH","M")
        ]

from pyspark.sql.types import StructType,StructField, StringType
schema = StructType([
    StructField('name', StructType([
         StructField('firstname', StringType(), True),
         StructField('middlename', StringType(), True),
         StructField('lastname', StringType(), True)
         ])),
     StructField('state', StringType(), True),
     StructField('gender', StringType(), True)
     ])

df2 = spark.createDataFrame(data = data, schema = schema)
df2.printSchema()
df2.show(truncate=False) # shows all columns

df2.select("name").show(truncate=False)
df2.select("name.firstname","name.lastname").show(truncate=False)
df2.select("name.*").show(truncate=False)

###########################################################
PySpark Retrieve All Column DataType and Names
###########################################################
https://sparkbyexamples.com/pyspark/pyspark-find-datatype-column-names-of-dataframe/

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data = [(1,"Robert"), (2,"Julia")]
df =spark.createDataFrame(data,["id","name"])

#Get All column names and it's types
for col in df.dtypes:
    print(col[0]+" , "+col[1])

#Get All column names and it's types
for field in df.schema.fields:
    print(field.name +" , "+str(field.dataType))

# Get DataType of a Specific Column Name
#Get data type of a specific column
print(df.schema["name"].dataType)
#StringType

#Get data type of a specific column from dtypes
print(dict(df.dtypes)['name'] )
print(df.dtypes[:10])


# PySpark Get All Column Names as a List
#Get All column names from DataFrame
print(df.columns)


# Get DataFrame Schema
df.printSchema()

# Get Column Nullable Property & Metadata
df.schema["name"].nullable
metaData=df.schema["name"].metadata


# Other ways to get DataFrame Schema
print(df.schema.simpleString)
print(df.schema.json)
print(df.schema.jsonValue)





############################################################################################
TypeError: ‘GroupedData’ object is not iterable in pyspark dataframe
############################################################################################

https://stackoverflow.com/questions/46791254/typeerror-groupeddata-object-is-not-iterable-in-pyspark
https://python.tutorialink.com/typeerror-groupeddata-object-is-not-iterable-in-pyspark-dataframe/
https://www.anycodings.com/1questions/607234/typeerror-groupeddata-object-is-not-iterable-in-pyspark-dataframe

from pyspark.sql import SparkSession
spark = SparkSession.builder.master('local').appName("Python Spark").getOrCreate()
spark.sparkContext.setLogLevel('OFF')  # DEBUG WARN ERROR

df = spark.read.parquet("file.pqt")
stocks = df.select("stocks").collect()
print(stocks[:2])
df.select("stocks").show(2)
df.groupBy("stocks").count().show(2)
print(df.groupBy("stocks").count())
pystark TypeError: 'GroupedData' object is not iterable
res = df.groupby(field).count().collect()


import pyspark.sql.functions as F
import requests

def format_options(options):
    options = "&".join([f"{k}={v}" for k, v in options.items()])
    return options

def make_request(coords, radiuses, options):
    options = format_options(options) if options else ""
    url = f"http://router.project-osrm.org/match/v1/car/{coords}?{options}&radiuses={radiuses}"
    r = requests.get(url)
    print(url)
    return r.json()

coords = sdf.groupBy('date', 'user').agg(
    F.concat_ws(';',
        F.collect_list(F.format_string('%f,%f', 'lon', 'lat'))
    ).alias('coords'),
    F.concat_ws(';',
        F.collect_list(F.format_string('%d', 'radius'))
    ).alias('radius')
).collect()


###########################################################
pandas.DataFrame.groupby
###########################################################

https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html

import pandas as pd
parquet_file = "../file.pqt"
df = pd.read_parquet(parquet_file)
df.head(6)
#df.columns[:4]
#print(dict(df.dtypes)['stock'] )
df.dtypes
df.groupby(['stock']).head(3)


##########################################
pyspark-stubs 3.0.0.post3
##########################################

https://pypi.org/project/pyspark-stubs/
https://pypi.org/project/pyspark-stubs/2.4.0rc4/
https://openbase.com/python/pyspark-stubs
https://snyk.io/advisor/python/pyspark-stubs
https://anaconda.org/conda-forge/pyspark-stubs
https://anaconda.org/conda-forge/pyspark-stubs/files
https://package.wiki/pyspark-stubs

pip install pyspark-stubs
pip install pyspark-stubs==2.4.0.post10
pip install pyspark-stubs==3.0.0.post3

conda install -c conda-forge pyspark-stubs
conda install -c "conda-forge/label/cf202003" pyspark-stubs





