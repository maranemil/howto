https://pythonspeed.com/articles/pandas-sql-chunking/
https://andrewpwheeler.com/2021/08/12/chunking-it-up-in-pandas/

data = pandas.read_sql_table('tablename',db_connection)
data_chunks = pandas.read_sql_table('tablename',db_connection,chunksize=2000)

###################################################
read sqlite data in chuncks
###################################################

import pandas as pd
from sqlalchemy import create_engine

def process_sql_using_pandas():
    engine = create_engine(
        "postgresql://postgres:pass@localhost/example"
        #"sqlite:///example.db"
    )
    conn = engine.connect().execution_options(
        stream_results=True)

    for chunk_dataframe in pd.read_sql(
            "SELECT * FROM users", conn, chunksize=1000):
        print(f"Got dataframe w/{len(chunk_dataframe)} rows")
        # ... do something with dataframe ...
        print(chunk_dataframe.head(2))
        chunk_dataframe.to_parquet('file.pqt')

if __name__ == '__main__':
    process_sql_using_pandas()


--------------------------------------------

https://stackoverflow.com/questions/7389759/memory-efficient-built-in-sqlalchemy-iterator-generator

from sqlalchemy import create_engine, select
conn = create_engine("DB URL...").connect()
q = select([huge_table])
proxy = conn.execution_options(stream_results=True).execute(q)
while 'batch not empty':  # equivalent of 'while True', but clearer
batch = proxy.fetchmany(100000)  # 100,000 rows at a time
if not batch:
	break
for row in batch:
	# Do your stuff here...
proxy.close()

--------------------------------------------

could not parse rfc1738 url from string sqlite

https://stackoverflow.com/questions/63709263/python-flask-sqlalchemy-exc-argumenterror-could-not-parse-rfc1738-url-from-str
https://docs.sqlalchemy.org/en/14/dialects/sqlite.html

from sqlalchemy import create_engine

# relative path on Linux: with three slashes
e = create_engine('sqlite:///relative/path/to/database.db')

# absolute path on Linux: with four slashes
e = create_engine('sqlite:////absolute/path/to/database.db')

---------------------------------------------------------------------------------------------


https://docs.sqlalchemy.org/en/14/_modules/examples/performance/large_resultsets.html

Source code for examples.performance.large_resultsets
"""In this series of tests, we are looking at time to load a large number
of very small and simple rows.

A special test here illustrates the difference between fetching the
rows from the raw DBAPI and throwing them away, vs. assembling each
row into a completely basic Python object and appending to a list. The
time spent typically more than doubles.  The point is that while
DBAPIs will give you raw rows very fast if they are written in C, the
moment you do anything with those rows, even something trivial,
overhead grows extremely fast in cPython. SQLAlchemy's Core and
lighter-weight ORM options add absolutely minimal overhead, and the
full blown ORM doesn't do terribly either even though mapped objects
provide a huge amount of functionality.

"""
from sqlalchemy import Column
from sqlalchemy import create_engine
from sqlalchemy import Integer
from sqlalchemy import String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Bundle
from sqlalchemy.orm import Session
from . import Profiler


Base = declarative_base()
engine = None


class Customer(Base):
    __tablename__ = "customer"
    id = Column(Integer, primary_key=True)
    name = Column(String(255))
    description = Column(String(255))


Profiler.init("large_resultsets", num=500000)


@Profiler.setup_once
def setup_database(dburl, echo, num):
    global engine
    engine = create_engine(dburl, echo=echo)
    Base.metadata.drop_all(engine)
    Base.metadata.create_all(engine)

    s = Session(engine)
    for chunk in range(0, num, 10000):
        s.execute(
            Customer.__table__.insert(),
            params=[
                {
                    "name": "customer name %d" % i,
                    "description": "customer description %d" % i,
                }
                for i in range(chunk, chunk + 10000)
            ],
        )
    s.commit()


@Profiler.profile
def test_orm_full_objects_list(n):
    """Load fully tracked ORM objects into one big list()."""

    sess = Session(engine)
    list(sess.query(Customer).limit(n))


@Profiler.profile
def test_orm_full_objects_chunks(n):
    """Load fully tracked ORM objects a chunk at a time using yield_per()."""

    sess = Session(engine)
    for obj in sess.query(Customer).yield_per(1000).limit(n):
        pass


@Profiler.profile
def test_orm_bundles(n):
    """Load lightweight "bundle" objects using the ORM."""

    sess = Session(engine)
    bundle = Bundle(
        "customer", Customer.id, Customer.name, Customer.description
    )
    for row in sess.query(bundle).yield_per(10000).limit(n):
        pass


@Profiler.profile
def test_orm_columns(n):
    """Load individual columns into named tuples using the ORM."""

    sess = Session(engine)
    for row in (
        sess.query(Customer.id, Customer.name, Customer.description)
        .yield_per(10000)
        .limit(n)
    ):
        pass


@Profiler.profile
def test_core_fetchall(n):
    """Load Core result rows using fetchall."""

    with engine.connect() as conn:
        result = conn.execute(Customer.__table__.select().limit(n)).fetchall()
        for row in result:
            row["id"], row["name"], row["description"]


@Profiler.profile
def test_core_fetchmany_w_streaming(n):
    """Load Core result rows using fetchmany/streaming."""

    with engine.connect() as conn:
        result = conn.execution_options(stream_results=True).execute(
            Customer.__table__.select().limit(n)
        )
        while True:
            chunk = result.fetchmany(10000)
            if not chunk:
                break
            for row in chunk:
                row["id"], row["name"], row["description"]


@Profiler.profile
def test_core_fetchmany(n):
    """Load Core result rows using Core / fetchmany."""

    with engine.connect() as conn:
        result = conn.execute(Customer.__table__.select().limit(n))
        while True:
            chunk = result.fetchmany(10000)
            if not chunk:
                break
            for row in chunk:
                row["id"], row["name"], row["description"]


@Profiler.profile
def test_dbapi_fetchall_plus_append_objects(n):
    """Load rows using DBAPI fetchall(), generate an object for each row."""

    _test_dbapi_raw(n, True)


@Profiler.profile
def test_dbapi_fetchall_no_object(n):
    """Load rows using DBAPI fetchall(), don't make any objects."""

    _test_dbapi_raw(n, False)


def _test_dbapi_raw(n, make_objects):
    compiled = (
        Customer.__table__.select()
        .limit(n)
        .compile(
            dialect=engine.dialect, compile_kwargs={"literal_binds": True}
        )
    )

    if make_objects:
        # because if you're going to roll your own, you're probably
        # going to do this, so see how this pushes you right back into
        # ORM land anyway :)
        class SimpleCustomer(object):
            def __init__(self, id_, name, description):
                self.id_ = id_
                self.name = name
                self.description = description

    sql = str(compiled)

    conn = engine.raw_connection()
    cursor = conn.cursor()
    cursor.execute(sql)

    if make_objects:
        for row in cursor.fetchall():
            # ensure that we fully fetch!
            SimpleCustomer(id_=row[0], name=row[1], description=row[2])
    else:
        for row in cursor.fetchall():
            # ensure that we fully fetch!
            row[0], row[1], row[2]

    conn.close()


if __name__ == "__main__":
    Profiler.main()


--------------------------------------------

https://docs.ray.io/en/latest/index.html
https://docs.ray.io/en/latest/data/dask-on-ray.html
https://github.com/modin-project/modin
https://modin.readthedocs.io/en/latest/

pip install modin
pip install "modin[ray]" # Install Modin dependencies and Ray to run on Ray
pip install "modin[dask]" # Install Modin dependencies and Dask to run on Dask
pip install "modin[all]" # Install all of the above

export MODIN_ENGINE=ray  # Modin will use Ray
export MODIN_ENGINE=dask  # Modin will use Dask

from modin.config import Engine
Engine.put("ray")  # Modin will use Ray
Engine.put("dask")  # Modin will use Dask

import modin.pandas as pd
df = pd.read_csv("my_dataset.csv")

import modin.pandas as pd
import numpy as np
frame_data = np.random.randint(0, 100, size=(2**10, 2**8))
df = pd.DataFrame(frame_data)

import os
os.environ["MODIN_ENGINE"] = "ray"  # Modin will use Ray
os.environ["MODIN_ENGINE"] = "dask"  # Modin will use Dask
import modin.pandas as pd

------------------------------------------------------

split parquet files

https://gist.github.com/mndrake/2adf4a037ceccba87a70f4a24d432017
https://stackoverflow.com/questions/59887234/split-a-parquet-file-in-smaller-chunks-using-dask

df = df.repartition(partition_size="100MB")
df.to_parquet(output_path)


https://www.computernetworkingnotes.com/linux-tutorials/different-types-of-files-in-linux.html

file p0001
tmp/part0001: Apache Parquet


https://gist.github.com/mndrake/2adf4a037ceccba87a70f4a24d432017

import os
from io import BytesIO
import pyarrow as pa
import pyarrow.parquet as pq


kilobytes = 1024
megabytes = kilobytes * 1000
chunksize = int(10 * megabytes)


def write_split_parquet(df, todir, chunksize=chunksize, compression='GZIP'):
    # initialize output directory
    if not os.path.exists(todir):
        os.mkdir(todir)
    else:
        for file in os.listdir(todir):
            os.remove(os.path.join(todir, file))
    # create parquet in-memory stream from dataframe
    table = pa.Table.from_pandas(df)  # pyarrow table
    stream = BytesIO()
    pq.write_table(table, stream, compression=compression)
    stream.seek(0)  # reset stream
    # write chunks to files
    i = 0
    while True:
        chunk = stream.read(chunksize)
        if not chunk:
            break
        i += 1
        filename = os.path.join(todir, ('part%04d' % i))
        with open(filename, 'wb') as f:
            f.write(chunk)
    stream.close()
    assert i <= 9999  # join sort fails if 5 digits
    return i


def read_split_parquet(fromdir):
    with BytesIO() as s:
        for file in os.listdir(fromdir):
            with open(os.path.join(fromdir, file), 'rb') as f:
                s.write(f.read())
        table = pq.read_table(s)
        df = table.to_pandas()
        return df

df = pd.read_parquet("file.pqt",engine='pyarrow',columns=[])
write_split_parquet(df, 'tmp', chunksize=100000)

------------------------------------------------------
####################################################
read parquet files chunks
####################################################

https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks

import pandas as pd
from glob import glob
files = sorted(glob('dat.parquet/part*'))

data = pd.read_parquet(files[0],engine='fastparquet')
for f in files[1:]:
    data = pd.concat([data,pd.read_parquet(f,engine='fastparquet')])

pd.read_parquet("chunks_*", engine="fastparquet")
pd.read_parquet("chunks_[1-2]*", engine="fastparquet")

import pyarrow.parquet as pq
parquet_file = pq.ParquetFile('example.parquet')
for batch in parquet_file.iter_batches():
    print("RecordBatch")
    batch_df = batch.to_pandas()
    print("batch_df:", batch_df)


-------------------------------------------------------------
https://pandas.pydata.org/pandas-docs/version/1.1/user_guide/scale.html
https://gist.github.com/gritmind/680376e5b821eecab4aeb3429717dd99

-------------------------------------------------------------

https://docs.dask.org/en/stable/install.html
https://docs.dask.org/en/stable/dataframe-parquet.html


python -m pip install "dask[complete]"
python -m pip install dask

python -m pip install "dask[array]"       # Install requirements for dask array
python -m pip install "dask[dataframe]"   # Install requirements for dask dataframe
python -m pip install "dask[diagnostics]" # Install requirements for dask diagnostics
python -m pip install "dask[distributed]" # Install requirements for distributed dask


import dask.dataframe as dd

# Load a single local parquet file
df = dd.read_parquet("path/to/mydata.parquet")

# Load a directory of local parquet files
df = dd.read_parquet("path/to/my/parquet/")

# Load a directory of parquet files from S3
df = dd.read_parquet("s3://bucket-name/my/parquet/")

# engine
df = dd.read_parquet(
     "s3://bucket-name/my/parquet/",
     engine="fastparquet"  # explicitly specify the fastparquet engine
)


-----------------------------------------------------------------------------

Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
pyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.

import pandas as pd
parquet_file = 'tmp/part0001.pq'
df = pd.read_parquet(parquet_file, engine='auto')


-----------------------------------------------------------------------------

https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/
https://mungingdata.com/python/split-csv-write-chunk-pandas/
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet
https://docs.databricks.com/data/data-sources/read-parquet.html
https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html
https://arrow.apache.org/docs/r/reference/read_parquet.html
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://arrow.apache.org/docs/python/parquet.html
https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html
https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare.html
https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html
https://sparkbyexamples.com/spark/spark-read-write-parquet-file-from-amazon-s3/
-----------------------------------------------------------------
####################################################
Parquet to CSV
####################################################

https://coiled.io/blog/write-multiple-parquet-files-to-a-single-csv-using-python/


import pandas as pd
import numpy as np
# use the recommended method for generating random integers with NumPy
rng = np.random.default_rng()
# generate 3 dataframes with similar filenames
for i in range(3):
    df = pd.DataFrame(rng.integers(0, 100, size=(3, 4)), columns=list('ABCD'))
    df.to_parquet(f"df_{i}.parquet")


ddf = dd.read_parquet('df_*.parquet')


# Convert Parquet to CSV
ddf.to_csv("df_all.csv",
           single_file=True,
           index=False
)

df_csv = pd.read_csv("df_all.csv")
df_csv

# Why Use Dask to Convert Parquet to CSV
import coiled
cluster = coiled.Cluster(
    n_workers=25,
)
from distributed import Client
client = Client(cluster)
ddf = dd.read_parquet(
    "s3://coiled-datasets/synthetic-data/synth-reg-104GB.parquet/",
    storage_options={'anon':'True', 'use_ssl':'True'}
)


---------------------------------------------------------------------------------------------
####################################################
print map
####################################################
https://stackoverflow.com/questions/44461551/how-to-print-map-object-with-python-3

def evaluate(x):
    print(x)

mymap = map(evaluate, [1,2,3]) # nothing gets printed yet
print(mymap)

print(list(mymap))
result = list(mymap) # prints 1, 2, 3

------------------------------------------------------------------------------
####################################################
merge  parquet files
####################################################

https://www.faqcode4u.com/faq/389836/load-multiple-parquet-files-into-dataframe-for-analysis
https://www.appsloveworld.com/pandas/100/336/how-to-append-multiple-parquet-files-to-one-dataframe-in-pandas
https://www.anycodings.com/1questions/939264/merging-multiple-parquet-files-and-creating-a-larger-parquet-file-in-s3-using-aws-glue
https://www.codegrepper.com/code-examples/python/frameworks/django/write+dataframe+into+multiple+parquet+files+in+S3+%2B+python


import pandas as pd
 # Loop through files and load into a dataframe
 df = pd.read_parquet('part0.parquet', engine='pyarrow')
 files = ['part1.parquet', 'part2.parquet', 'part3.parquet'] # in total there are 6 files
 for file in files:
     data = pd.read_parque(file)
     df = df.append(data, ignore_index=True)
     del data


# with spark
parquetFile = spark.read.parquet("your_dir_path/")

# with aws
import awswrangler as wr
wr.pandas.to_parquet(
    dataframe=df,
    path="s3://my-bucket/key/my-file.parquet"
)

https://gist.github.com/l1x/76dab6445b6d55396c622f915c755a17

import os
import pyarrow.parquet as pq

def combine_parquet_files(input_folder, target_path):
    try:
        files = []
        for file_name in os.listdir(input_folder):
            files.append(pq.read_table(os.path.join(input_folder, file_name)))
        with pq.ParquetWriter(target_path,
                files[0].schema,
                version='2.0',
                compression='gzip',
                use_dictionary=True,
                data_page_size=2097152, #2MB
                write_statistics=True) as writer:
            for f in files:
                writer.write_table(f)
    except Exception as e:
        print(e)

combine_parquet_files('data', 'combined.parquet')

https://gist.github.com/TonyWuLihu/686b77ea1332fcc6fc6b2fd2f50c06d5

import sys
from datetime import date,datetime,timedelta
import datetime
import string
from pexpect import *
from os import remove,listdir
import os
import pprint

def chunks(l, n):
    for i in range(0, len(l), n):
        yield l[i:i + n]

if __name__ == '__main__':
    s3path_prefix = '/data/s3fs/warehouse/ott_user_info'
    merge_command = 'java -jar /usr/local/parquet-tools-1.9.1-SNAPSHOT.jar merge %s %s'
    year,month,day = (sys.argv[1][0:4],sys.argv[1][4:6],sys.argv[1][6:8])
    filelist = [os.path.join(s3path_prefix+'/year=%s/month=%s/day=%s' % (year,month,day),filename) for filename in listdir(s3path_prefix+'/year=%s/month=%s/day=%s' % (year,month,day)) if filename.startswith('part-')]
    subfilelists = list(chunks(filelist,int(sys.argv[2])))
    #TODO ugly code, flat it later
    i=0
    fx = lambda x: os.remove(x)
    for sublist in subfilelists:
        src = ' '.join(sublist)
        dest = os.path.join(s3path_prefix+'/year=%s/month=%s/day=%s' % (year,month,day),'userinfo_%05d.parquet' % i)
        i+=1
        childp =  spawn(merge_command % (src,dest))
        childp.expect(EOF,timeout=1000)
        childp.close()
        list(map(fx,sublist))
    print("Job Done!")


##############################################################
concatenating parquet files
##############################################################

https://stackoverflow.com/questions/61759297/dask-dataframe-concatenating-parquet-files-throws-out-of-memory
https://docs.dask.org/en/stable/dataframe-parquet.html
https://docs.dask.org/en/stable/generated/dask.dataframe.to_parquet.html
https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html
https://coiled.io/blog/dask-dataframe-merge-join/


import dask.dataframe as dd

# Read all files in `data/`
df = dd.read_parquet("data/", columns=['name', 'address'], engine='pyarrow')

# Export to single `.parquet` file
df.repartition(npartitions=1).to_parquet("data/combined", write_metadata_file=False)


import dask.dataframe as dd

# Load a single local parquet file
df = dd.read_parquet("path/to/mydata.parquet")

# Load a directory of local parquet files
df = dd.read_parquet("path/to/my/parquet/")

# Load a directory of parquet files from S3
df = dd.read_parquet("s3://bucket-name/my/parquet/")




import dask.dataframe as dd
import pandas as pd
# create sample large pandas dataframe
df_large = pd.DataFrame(
    {
        "Name": ["Azza", "Brandon", "Cedric", "Devonte", "Eli", "Fabio"],
        "Age": [29, 30, 21, 57, 32, 19]
    }
)
# create multi-partition dask dataframe from pandas
large = dd.from_pandas(df_large, npartitions=2)
# create sample small pandas dataframe
small = pd.DataFrame(
    {
        "Name": ["Azza", "Cedric", "Fabio"],
        "City": ["Beirut", "Dublin", "Rosario"]
    }
)
# merge dask dataframe to pandas dataframe
join = ddf.merge(df2, how="left", on=["Name"])
# inspect results
join.compute()




##############################################################
working with dask
##############################################################

https://docs.dask.org/en/stable/dataframe-best-practices.html
https://examples.dask.org/dataframes/01-data-access.html
https://docs.dask.org/en/latest/dataframe-create.html

# Reduce, and then use pandas
df = dd.read_parquet('my-giant-file.parquet')
df = df[df.name == 'Alice']              # Select a subsection
result = df.groupby('id').value.mean()   # Reduce to a smaller size
result = result.compute()                # Convert to pandas dataframe
result...                                # Continue working with pandas


df.to_parquet('path/to/my-results/')
df = dd.read_parquet('path/to/my-results/')

df1 = dd.read_parquet('path/to/my-results/', engine='fastparquet')
df2 = dd.read_parquet('path/to/my-results/', engine='pyarrow')



import os
import datetime

if not os.path.exists('data'):
    os.mkdir('data')

def name(i):
    return str(datetime.date(2000, 1, 1) + i * datetime.timedelta(days=1))

df.to_csv('data/*.csv', name_function=name);

# ls data/*.csv | head

import pandas as pd
df = pd.read_csv('data/2000-01-01.csv')
df.head()

import dask.dataframe as dd
df = dd.read_csv('data/2000-*-*.csv')
df
df.head()

df = dd.read_csv('data/2000-*-*.csv', parse_dates=['timestamp'])
df
df.groupby('name').x.mean().compute()
df.to_parquet('data/2000-01.parquet', engine='pyarrow')
df = dd.read_parquet('data/2000-01.parquet', engine='pyarrow')
df
df.groupby('name').x.mean().compute()

df = dd.read_parquet('data/2000-01.parquet', columns=['name', 'x'], engine='pyarrow')
df.groupby('name').x.mean().compute()


------------------------------------------------------------------------------------------------------
##############################################################
What is the problem with shadowing names defined in outer scopes?
##############################################################

https://linuxpip.org/fix-shadows-name-from-outer-scope-in-pycharm/
https://stackoverflow.com/questions/20125172/what-is-the-problem-with-shadowing-names-defined-in-outer-scopes


"Shadows name from outer scope"
the_list = [1, 2, 3]

def data_log(the_list): # Warning: "Shadows 'the_list' from outer scope
    print(the_list)

data_log(the_list)



Avoid "Shadows name from outer scope"

the_list = [1, 2, 3]

def data_log(a_list): # Warning: "Shadows 'the_list' from outer scope
    print(a_list)

data_log(the_list)


------------------------------------------------------------------------------------------------------
##############################################################
AttributeError: 'DataFrame' object has no attribute 'to_sparse'
##############################################################

https://pandas.pydata.org/pandas-docs/version/1.0.0/reference/api/pandas.DataFrame.sparse.to_dense.html
https://stackoverflow.com/questions/71489011/attributeerror-dataframe-object-has-no-attribute-to-sparse
https://www.stackvidhya.com/pandas-change-column-type/
https://pandas.pydata.org/docs/user_guide/text.html
https://www.geeksforgeeks.org/python-pandas-series-astype-to-convert-data-type-of-series/

df = pd.DataFrame({"A": pd.arrays.SparseArray([0, 1, 0])})
df.sparse.to_dense()

>>> df = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0]})
>>> df.dtypes
# A    int64
# dtype: object

>>> sdf = df.astype(pd.SparseDtype(int, fill_value=0))
>>> sdf.dtypes
# A    Sparse[int64, 0]
# dtype: object

df = df.astype('Sparse[int64, 0]')
df = df.astype({"Unit_Price": str})
df.dtypes


df["Unit_Price"] = pd.to_numeric(df["Unit_Price"])
df["No_Of_Units"] = pd.to_numeric(df["No_Of_Units"])
df.dtypes



# importing pandas module
import pandas as pd
# reading csv file from url
data = pd.read_csv("https://media.geeksforgeeks.org/wp-content/uploads/nba.csv")
# dropping null value columns to avoid errors
data.dropna(inplace = True)
# storing dtype before converting
before = data.dtypes
# converting dtypes using astype
data["Salary"]= data["Salary"].astype(int)
data["Number"]= data["Number"].astype(str)
# storing dtype after converting
after = data.dtypes
# printing to compare
print("BEFORE CONVERSION\n", before, "\n")
print("AFTER CONVERSION\n", after, "\n")

------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------
##############################################################
series-to_csv
##############################################################

https://www.w3resource.com/pandas/series/series-to_csv.php
https://pandas.pydata.org/docs/reference/api/pandas.Series.to_csv.html
https://stackoverflow.com/questions/13557559/how-to-write-read-pandas-series-to-from-csv
https://www.geeksforgeeks.org/python-pandas-series-to_csv/
https://www.geeksforgeeks.org/python-pandas-series-to_csv/
https://github.com/huggingface/datasets/issues/1817
https://stackoverflow.com/questions/53082686/mysterious-pyarrow-lib-arrowinvalid-floating-point-value-truncated-error-when

import numpy as np
import pandas as pd
df = pd.DataFrame({'name': ['Leonardo', 'Michelangelo'],
                   'mask': ['blue', 'orange'],
                   'weapon': ['katana', 'nunchaku']})
df.to_csv(index=False)



# importing pandas as pd
import pandas as pd
# Creating the Series
sr = pd.Series(['New York', 'Chicago', 'Toronto', 'Lisbon', 'Rio', 'Moscow'])
# Create the Datetime Index
didx = pd.DatetimeIndex(start ='2014-08-01 10:00', freq ='W',
                     periods = 6, tz = 'Europe / Berlin')
# set the index
sr.index = didx
# Print the series
print(sr)



# importing pandas as pd
import pandas as pd
# Creating the Series
sr = pd.Series([19.5, 16.8, None, 22.78, None, 20.124, None, 18.1002, None])
# Print the series
print(sr)
# convert to comma-separated
sr.to_csv('file.csv')
pd.read_csv('file.csv', index_col=False, dtype='unicode', low_memory=False).to_parquet('file.pqt')

------------------------------------------------------------------------------------------------------
##############################################################
pyarrow.lib.ArrowInvalid: ("Could not convert '31.0' with type str: tried to convert to double", 'Conversion failed for column
##############################################################

https://forums.fast.ai/t/arrowinvalid-could-not-convert-with-type-str-tried-to-convert-to-double-conversion-failed-for-column-booking-rep-with-type-category/28066


import pandas as pd
a=pd.DataFrame([['a','b','c'],['2.42','','3.285']]).T
a.columns=['names', 'nums']
a['nums']=a['nums'][a['nums']!=''].astype(float)

------------------------------------------------------------------------------------------------------

##############################################################
DtypeWarning: Columns (195,269,270) have mixed types. Specify dtype option on import or set low_memory=False
##############################################################

https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options
https://discuss.dizzycoding.com/pandas-read_csv-low_memory-and-dtype-options/


file = pd.read_csv('example.csv', engine='python')
dashboard_df = pd.read_csv(p_file, sep=',', error_bad_lines=False, index_col=False, dtype='unicode')
df = pd.read_csv('somefile.csv', low_memory=False)
dashboard_df = pd.read_csv(p_file, sep=',', error_bad_lines=False, index_col=False, dtype='unicode')

------------------------------------------------------------------------------------------------------


df = pd.read_csv(CsvFileName)
p = df.pivot_table(index=['Hour'], columns='DOW', values='Changes', aggfunc=np.mean).round(0)
p.fillna(0, inplace=True)
columns = ["1Sun", "2Mon", "3Tue", "4Wed", "5Thu", "6Fri", "7Sat"]
p = p.reindex(columns=columns)
p[columns] = p[columns].astype(int)

------------------------------------------------------------------------------------------------------


