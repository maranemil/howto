###################################################
Chunksize Table arrow
###################################################

https://arrow.apache.org/docs/python/parquet.html

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

chunksize=10000 # this is the number of lines

pqwriter = None
for i, df in enumerate(pd.read_csv('sample.csv', chunksize=chunksize)):
    table = pa.Table.from_pandas(df)
    # for the first chunk of records
    if i == 0:
        # create a parquet write object giving it an output file
        pqwriter = pq.ParquetWriter('sample.parquet', table.schema)
    pqwriter.write_table(table)

# close the parquet writer
if pqwriter:
    pqwriter.close()

https://arrow.apache.org/docs/python/api/files.html
https://arrow.apache.org/docs/python/api/formats.html
https://arrow.apache.org/docs/python/api/memory.html
https://arrow.apache.org/docs/python/generated/pyarrow.Table.html
https://arrow.apache.org/docs/python/getstarted.html
https://arrow.apache.org/docs/python/parquet.html
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html

------------------------------------------------------------------------------------------

get count pandas rows

https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe
https://www.tutorialspoint.com/how-to-get-nth-row-in-a-pandas-dataframe

len(df.index)
df.shape[0]
df[df.columns[0]].count()

------------------------------------------------------------------------------------------

read-multiple-parquet-files-in-a-folder
https://stackoverflow.com/questions/51696655/read-multiple-parquet-files-in-a-folder-and-write-to-single-csv-file-using-pytho

import pandas as pd
df = pd.read_parquet('path/to/the/parquet/files/directory')

------------------------------------------------------------------------------------------
https://www.geeksforgeeks.org/pyspark-groupby-and-sort-dataframe-in-descending-order/
https://stackoverflow.com/questions/34514545/sort-in-descending-order-in-pyspark
https://stackoverflow.com/questions/54002006/dask-dataframe-view-entire-row
https://sparkbyexamples.com/pyspark/pyspark-dataframe-groupby-and-sort-by-descending-order/
https://sparkbyexamples.com/pyspark/pyspark-count-distinct-from-dataframe/
https://www.geeksforgeeks.org/get-number-of-rows-and-columns-of-pyspark-dataframe/
------------------------------------------------------------------------------------------

size-or-shape rows columns
https://stackoverflow.com/questions/39652767/how-to-find-the-size-or-shape-of-a-dataframe-in-pyspark

print((df.count(), len(df.columns)))

------------------------------------------------------------------------------------------

https://docs.python.org/3/library/unittest.html
https://docs.python.org/3/library/test.html
https://docs.python-guide.org/writing/tests/
https://www.fullstackpython.com/deployment.html
https://cloud.google.com/appengine/docs/standard/python/migrate-to-python3/testing
https://cloud.google.com/appengine/docs/flexible/python/testing-and-deploying-your-app
https://docs.python-guide.org/scenarios/ci/
https://www.obeythetestinggoat.com/book/chapter_manual_deployment.html
https://docs.microsoft.com/en-us/azure/devops/pipelines/ecosystems/python?view=azure-devops
https://semaphoreci.com/blog/python-continuous-integration-continuous-delivery
https://realpython.com/python-continuous-integration/
https://code.visualstudio.com/docs/python/testing
https://www.jetbrains.com/help/pycharm/testing-your-first-python-application.html
https://docs.microsoft.com/en-us/azure/devops/pipelines/ecosystems/python-webapp?view=azure-devops
https://packaging.python.org/en/latest/tutorials/packaging-projects/
https://www.fullstackpython.com/continuous-integration.html
https://www.nylas.com/blog/packaging-deploying-python/
https://medium.com/@anirbanroydas/testing-microservice-written-in-python-flask-with-continuous-integration-delivery-and-deployment-1999fef560a8
https://python-packaging.readthedocs.io/en/latest/testing.html
------------------------------------------------------------------------------------------

https://www.computerbase.de/downloads/betriebssysteme/manjaro-linux/

------------------------------------------------------------------------------------------

https://pandas.pydata.org/docs/reference/api/pandas.concat.html
https://www.geeksforgeeks.org/python-pandas-series-to_dense/
https://pitrou.net/arrowdevdoc/python/parquet.html
https://stackoverflow.com/questions/59988219/how-can-i-read-each-parquet-row-group-into-a-separate-partition
https://wesm.github.io/arrow-site-test/python/generated/pyarrow.parquet.ParquetFile.html
https://www.programcreek.com/python/example/124547/pyarrow.parquet.ParquetFile

# batches with pyarrow.

import pyarrow as pq
batch_size = 1
_file = pq.parquet.ParquetFile("file.parquet")
batches = _file.iter_batches(batch_size) #batches will be a generator
for batch in batches:
  process(batch)

# get parquet schema

csv2parquet.main_with_args(csv2parquet.convert,
                       ['csvs/simple.csv', '--rename', '0=alpha', 'b=bee'])
pqf = pq.ParquetFile('csvs/simple.parquet')
schema = pqf.schema

# set parquet row-group-size

csv2parquet.main_with_args(csv2parquet.convert, ['csvs/simple.csv', '--row-group-size', '1'])
pqf = pq.ParquetFile('csvs/simple.parquet')


##############################################################
Converting a CSV to Parquet with PyArrow
##############################################################

https://mungingdata.com/pyarrow/parquet-metadata-min-max-statistics/

CSV data.
first_name,last_name
jose,cardona
jon,smith

import pyarrow.csv as pv
import pyarrow.parquet as pq
table = pv.read_csv('./data/people/people1.csv')
pq.write_table(table, './tmp/pyarrow_out/people1.parquet')


# Fetching metadata of Parquet file

import pyarrow.parquet as pq
parquet_file = pq.ParquetFile('./tmp/pyarrow_out/people1.parquet')
parquet_file.metadata
parquet_file.metadata.row_group(0)
parquet_file.metadata.row_group(0).column(0)
parquet_file.metadata.row_group(0).column(0).compression # => 'SNAPPY'


# Fetching Parquet column statistics

CSV data.
nickname,age
fofo,3
tio,1
lulu,9

# Convert the CSV file to a Parquet file.

table = pv.read_csv('./data/pets/pets1.csv')
pq.write_table(table, './tmp/pyarrow_out/pets1.parquet')

Inspect the Parquet metadata statistics

parquet_file = pq.ParquetFile('./tmp/pyarrow_out/pets1.parquet')
print(parquet_file.metadata.row_group(0).column(1).statistics)

--------------------------------------------------------------------------------

Get dictionary keys as a list


https://www.geeksforgeeks.org/python-get-dictionary-keys-as-a-list/
https://stackabuse.com/python-dictionary-tutorial/

dict = {1: 'Geeks', 2: 'for', 3: 'geeks'}
print(dict.keys())

mydict = {1: 'Geeks', 2: 'for', 3: 'geeks'}
keysList = list(mydict.keys())
print(keysList)


-----------------------------------------------------------------------------

Add new keys to a dictionary

https://www.geeksforgeeks.org/python-add-new-keys-to-a-dictionary/

dict = {'key1': 'geeks', 'key2': 'fill_me'}
print("Current Dict is: "
  , dict)

# using the subscript notation
# Dictionary_Name[New_Key_Name] = New_Key_Value
dict['key2'] = 'for'
dict['key3'] = 'geeks'
print("Updated Dict is:"
  , dict)


-----------------------------------------------------------------------------

python dictionary random key and value

https://www.codegrepper.com/code-examples/python/python+dictionary+random+key+and+value
https://www.codegrepper.com/code-examples/python/random+key+from+dictionary+python
https://stackoverflow.com/questions/43772151/programmatically-generate-key-and-values-to-put-in-python-dictionary
https://www.codegrepper.com/code-examples/python/create+a+list+of+n+elements+python+random+numbers
https://www.codegrepper.com/code-examples/python/python+how+to+create+a+list+of+n+elements
https://stackoverflow.com/questions/10712002/create-an-empty-list-with-certain-size-in-python
https://www.geeksforgeeks.org/how-to-create-dummy-variables-in-python-with-pandas/
https://www.geeksforgeeks.org/how-to-create-dummy-variables-in-python-with-pandas/


# import required modules
import pandas as pd
import numpy as np
# create dataset
s = pd.Series(list('abca'))
# display dataset
print(s)
# create dummy variables
pd.get_dummies(s)



random.choice(list(my_dict))

entry_list = list(a_dict.items())


dict = { 'A' : 1, 'A' : 2, 'A' : 3}
random_element = random.choice(list(dict.items())
# Output = (key, value)


print(random_entry)
mydict={'r%s'%n : 'tag %s'%n for n in range(10)}


mydict=dict()
for n in range(10):
mydict.update({'r%s'%n:'tag %s'%n})

mydict={'r{}'.format(n) : 'tag {}'.format(n) for n in range(10)}


import random
random.sample(range(1, 100), 3)

l = [None] * 10
# [None, None, None, None, None, None, None, None, None, None]

l = range(10)
# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

a = [{}] * 3
a[0]['hello'] = 5
# [{'hello': 5}, {'hello': 5}, {'hello': 5}]


-----------------------------------------------------------------------------

Python - Add List Items
https://www.w3schools.com/python/python_lists_add.asp
https://www.digitalocean.com/community/tutorials/python-add-to-list
https://www.programiz.com/python-programming/methods/dictionary/update
https://www.programiz.com/python-programming/methods/dictionary/pop

thislist = ["apple", "banana", "cherry"]
thislist.append("orange")
print(thislist)

thislist = ["apple", "banana", "cherry"]
thislist.insert(1, "orange")
print(thislist)

thislist = ["apple", "banana", "cherry"]
tropical = ["mango", "pineapple", "papaya"]
thislist.extend(tropical)
print(thislist)

thislist = ["apple", "banana", "cherry"]
thistuple = ("kiwi", "orange")
thislist.extend(thistuple)
print(thislist)

-----------------------------------------------------------------------------

Pandas DataFrame: to_parquet() function

https://www.w3resource.com/pandas/dataframe/dataframe-to_parquet.php
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html
https://www.geeksforgeeks.org/how-to-concatenate-two-or-more-pandas-dataframes/
https://fixexception.com/pandas/must-pass-2-d-input-shape-values-shape/
https://stackoverflow.com/questions/50765211/three-dimensional-pandas-dataframe-error-must-pass-2-d-input

import pandas as pd
df = pd.DataFrame(data={'col_1': [2, 3], 'col_2': [4, 5]})
df.to_parquet('df.parquet.gzip',compression='gzip')
pd.read_parquet('df.parquet.gzip')


import pandas as pd
df = pd.DataFrame(np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]),columns=['a', 'b', 'c'])
print(df.shape)


lable_one = np.array(['one', 'two'])
lable_two = np.array(['a', 'b'])
cols = pd.MultiIndex.from_product([lable_one, lable_two])
pd.DataFrame(A.T.reshape(2, -1), columns=cols)

-----------------------------------------------------------------------------

Python: Find in list
https://stackoverflow.com/questions/9542738/python-find-in-list

my_list = ['apple', 'banana', 'cherry']
my_set = set(my_list) # {'apple', 'banana', 'cherry'}
if item in my_set:  # much faster on average than using a list
    # do something


fruits = ['apple', 'banana', 'cherry']
if item in fruits:
	# do something

-----------------------------------------------------------------------------

https://www.programiz.com/python-programming/break-continue
https://www.tutorialspoint.com/python/python_loop_control.htm

if x == y:
	continue

-----------------------------------------------------------------------------

save pandas data frame to parquet file
https://stackoverflow.com/questions/41066582/python-save-pandas-data-frame-to-parquet-file
https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.to_parquet.html
https://pandas.pydata.org/pandas-docs/version/1.1/reference/api/pandas.DataFrame.to_parquet.html
https://stackoverflow.com/questions/33813815/how-to-read-a-parquet-file-into-pandas-dataframe
https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html

df.to_parquet('myfile.parquet')
df.to_parquet('myfile.parquet', engine='fastparquet')

import pandas as pd
pd.read_parquet('example_pa.parquet', engine='pyarrow')

import pandas as pd
pd.read_parquet('example_fp.parquet', engine='fastparquet')

-----------------------------------------------------------------------------
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html
https://pandas.pydata.org/docs/reference/api/pandas.concat.html
https://arrow.apache.org/docs/python/generated/pyarrow.schema.html
https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.from_pandas
https://arrow.apache.org/docs/python/data.html#tables
https://arrow.apache.org/docs/python/parquet.html#writing-to-partitioned-datasets
https://arrow.apache.org/docs/python/dataset.html
https://arrow.apache.org/docs/python/parquet.html#writing-to-partitioned-datasets


https://wesm.github.io/arrow-site-test/python/parquet.html#writing-to-partitioned-datasets
https://enpiar.com/arrow-site/docs/python/parquet.html
https://arrow.apache.org/docs/python/parquet.html#parquet-file-writing-options
https://wesm.github.io/arrow-site-test/python/parquet.html

df = pd.DataFrame({'one': [-1, np.nan, 2.5],
                   'two': ['foo', 'bar', 'baz'],
                   'three': [True, False, True]},
                   index=list('abc'))

table = pa.Table.from_pandas(df, preserve_index=False)
pq.write_table(table, 'example_noindex.parquet')
t = pq.read_table('example_noindex.parquet')
t.to_pandas()

parquet_file = pq.ParquetFile('example.parquet')
parquet_file.metadata
parquet_file.schema
parquet_file.num_row_groups
parquet_file.read_row_group(0)


with pq.ParquetWriter('example2.parquet', table.schema) as writer:
   for i in range(3):
      writer.write_table(table)


pf2 = pq.ParquetFile('example2.parquet')
pf2.num_row_groups

parquet_file = pq.ParquetFile('example.parquet')
metadata = parquet_file.metadata
metadata = pq.read_metadata('example.parquet')
metadata.row_group(0)
metadata.row_group(0).column(0)


pq.write_table(table, where, use_dictionary=False)
pq.write_table(table, where, compression='snappy')
pq.write_table(table, where, compression='gzip')
pq.write_table(table, where, compression='brotli')
pq.write_table(table, where, compression='zstd')
pq.write_table(table, where, compression='lz4')
pq.write_table(table, where, compression='none')

# Partitioned Datasets (Multiple Files)

pq.write_table(table, where, compression='snappy')
pq.write_table(table, where, compression='gzip')
pq.write_table(table, where, compression='brotli')
pq.write_table(table, where, compression='none')

# Writing to Partitioned Datasets
# Local dataset write
pq.write_to_dataset(table, root_path='dataset_name',
                    partition_cols=['one', 'two'])

# Reading from Partitioned Datasets
dataset = pq.ParquetDataset('dataset_name/')
table = dataset.read()
table = pq.read_table('dataset_name')


# Multithreaded Reads
pq.read_table(where, nthreads=4)
pq.ParquetDataset(where).read(nthreads=4)

-----------------------------------------------------------------------------

https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html
https://arrow.apache.org/docs/python/parquet.html

import pyarrow as pa
table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})
import pyarrow.parquet as pq
pq.write_table(table, 'example.parquet')

pq.write_table(table, 'example.parquet', row_group_size=3)
pq.write_table(table, 'example.parquet', compression='none')

pq.write_table(table, 'example.parquet',
               compression={'n_legs': 'snappy', 'animal': 'gzip'},
               use_dictionary=['n_legs', 'animal'])

pq.write_table(table, 'example.parquet',
               column_encoding={'animal':'PLAIN'},
               use_dictionary=False)


-----------------------------------------------------------------------------

https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html
https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_metadata.html
https://arrow.apache.org/docs/cpp/parquet.html
https://arrow.apache.org/cookbook/py/io.htmlä
https://www.programcreek.com/python/example/124548/pyarrow.parquet.write_table
https://stackoverflow.com/questions/49565713/assign-schema-to-pa-table-from-pandas

import pyarrow as pa
table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],
                  'n_legs': [2, 2, 4, 4, 5, 100],
                  'animal': ["Flamingo", "Parrot", "Dog", "Horse",
                             "Brittle stars", "Centipede"]})

# write it to a partitioned dataset
import pyarrow.parquet as pq
pq.write_to_dataset(table, root_path='dataset_name_3',
                    partition_cols=['year'])
pq.ParquetDataset('dataset_name_3', use_legacy_dataset=False).files


# Write a single Parquet file into the root folder
pq.write_to_dataset(table, root_path='dataset_name_4')
pq.ParquetDataset('dataset_name_4/', use_legacy_dataset=False).files


# Assign schema to pa.Table.from_pandas()
fields = [
    pa.field('id', pa.int64()),
    pa.field('secondaryid', pa.int64()),
    pa.field('date', pa.timestamp('ms')),
]

my_schema = pa.schema(fields)
table = pa.Table.from_pandas(sample_df, schema=my_schema, preserve_index=False)


-----------------------------------------------------------------------------


