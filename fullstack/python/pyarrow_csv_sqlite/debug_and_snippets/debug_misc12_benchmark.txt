
################################################################
# bug memory
################################################################

https://wesm.github.io/arrow-site-test/python/parquet.html
https://wesm.github.io/arrow-site-test/python/pandas.html
https://arrow.apache.org/docs/4.0/python/generated/pyarrow.parquet.ParquetWriter.html
https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html
https://enpiar.com/arrow-site/docs/_modules/pyarrow/parquet.html

https://github.com/apache/arrow/issues/2614
https://github.com/apache/arrow/issues/2624

https://arrow.apache.org/docs/cpp/memory.html
https://arrow.apache.org/docs/python/memory.html

https://stackoverflow.com/questions/63792849/pyarrow-version-1-0-bug-throws-out-of-memory-exception-while-reading-large-numbe
https://issues.apache.org/jira/browse/ARROW-9974


# benchmark bug memory
# ------------------------------------------------

# create a big dataframe
import pandas as pd
import numpy as np

df = pd.DataFrame({'A': np.arange(50000000)})
df['F1'] = np.random.randn(50000000) * 100
df['F2'] = np.random.randn(50000000) * 100
df['F3'] = np.random.randn(50000000) * 100
df['F4'] = np.random.randn(50000000) * 100
df['F5'] = np.random.randn(50000000) * 100
df['F6'] = np.random.randn(50000000) * 100
df['F7'] = np.random.randn(50000000) * 100
df['F8'] = np.random.randn(50000000) * 100
df['F9'] = 'ABCDEFGH'
df['F10'] = 'ABCDEFGH'
df['F11'] = 'ABCDEFGH'
df['F12'] = 'ABCDEFGH01234'
df['F13'] = 'ABCDEFGH01234'
df['F14'] = 'ABCDEFGH01234'
df['F15'] = 'ABCDEFGH01234567'
df['F16'] = 'ABCDEFGH01234567'
df['F17'] = 'ABCDEFGH01234567'

# split and save data to 5000 files
for i in range(5000):
df.iloc[i*10000:(i+1)*10000].to_parquet(f'{i}.parquet', index=False)

# use a fresh session to read data

# below code works to read
import pandas as pd
df = []
for i in range(5000):
df.append(pd.read_parquet(f'{i}.parquet'))

df = pd.concat(df)

# below code crashes with memory error in pyarrow 1.0/1.0.1 (works fine with version 0.13.0)
# tried use_legacy_dataset=False, same issue
import pyarrow.parquet as pq

fnames = []
for i in range(5000):
fnames.append(f'{i}.parquet')

len(fnames)

df = pq.ParquetDataset(fnames).read(use_threads=False)


------------------------------------------------------------------

https://pypi.org/project/memory-profiler/

------------------------------------------------------------------


https://stackoverflow.com/questions/22005911/convert-columns-to-string-in-pandas
https://arrow.apache.org/docs/python/dataset.html

import numpy as np
import pandas as pd
import time

#creating four sample dataframes using dummy data
df1 = pd.DataFrame(np.random.randint(1, 1000, size =(10000000, 1)), columns =['A'])
df2 = pd.DataFrame(np.random.randint(1, 1000, size =(10000000, 1)), columns =['A'])
df3 = pd.DataFrame(np.random.randint(1, 1000, size =(10000000, 1)), columns =['A'])
df4 = pd.DataFrame(np.random.randint(1, 1000, size =(10000000, 1)), columns =['A'])

#applying astype(str)
time1 = time.time()
df1['A'] = df1['A'].astype(str)
print('time taken for astype(str) : ' + str(time.time()-time1) + ' seconds')

pd['Column'].map(str)

-----------------------------------------------------------------------



####################################################
memory
####################################################

https://docs.python.org/3/c-api/memory.html
https://pythonspeed.com/articles/python-out-of-memory/
https://hackernoon.com/how-to-solve-the-python-memory-error
https://www.educba.com/python-memory-error/?source=leftnav

## Benchmark
def sample_generator():
for i in range(10000000):
yield i
gen_integ= sample_generator()
for i in gen_integ:
print(i)

# manage CPU usage to prevent Memory Error.
import resource
def limit_memory(Datasize):
min_, max_ = resource.getrlimit(resource.RLIMIT_AS)
resource.setrlimit(resource.RLIMIT_AS, (Datasize, max_))

## Numpy
import numpy
numpy.ones((1_000_000_000_000,))

## Numpy operation which return random unique values
import numpy as np
np.random.uniform(low=1,high=10,size=(10000,100000))

## Functions which return values
def calc_sum(x,y):
	op = x + y
	return(op)


import ctypes
libc = ctypes.CDLL("libc.so.6")
libc.malloc(100)
libc.malloc(1_000_000_000_000)

import gc
gc.collect()

# using resource
import resource
def limit_memory(maxsize):
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))



from subprocess import call
call('ulimit -v 1000 && ulimit -v && ls', shell=True)

https://www.geeksforgeeks.org/ulimit-soft-limits-and-hard-limits-in-linux/
https://itecnote.com/tecnote/python-ulimit-and-nice-for-subprocess-call-subprocess-popen/
https://de.acervolima.com/ulimit-soft-limits-und-hard-limits-in-linux/
https://www.scivision.dev/platform-independent-ulimit-number-of-open-files-increase/

print('ulimit -n soft,hard: {},{}'.format(soft,hard))


pip install pandas_dtype_efficiency

------------------------------------------------------------------


SparseDataFrame.to_parquet fails

https://github.com/pandas-dev/pandas/issues/26378
https://docs.scipy.org/doc/scipy/reference/sparse.html
http://jorisvandenbossche.github.io/example-pandas-docs/html-doc-home/user_guide/sparse.html
https://pandas.pydata.org/pandas-docs/dev/user_guide/sparse.html
https://pandas.pydata.org/docs/user_guide/sparse.html
https://yiyibooks.cn/meikunyuan6/pandas/pandas/sparse.html

import pandas as pd # v0.24.2
import scipy.sparse # v1.1.0

df = pd.SparseDataFrame(scipy.sparse.random(1000, 1000),
                         columns=list(map(str, range(1000))),
                         default_fill_value=0.0)
# df.to_parquet('rpd.pq', engine='pyarrow')
df.to_dense().to_parquetto_parquet('rpd.pq', engine='pyarrow')

------------------------------------------------------------------

####################################################
PerformanceWarning: DataFrame is highly fragmented
####################################################

https://github.com/twopirllc/pandas-ta/issues/340
https://www.anycodings.com/1questions/1207897/performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-of-calling-frameinsert-many-times-which-has-poor-performance
https://tutorialmeta.com/question/too-many-columns-resulting-in-performancewarning-dataframe-is-highly-fragmente


This is usually the result of calling `frame.insert` many times, which has poor performance.
Consider joining all columns at once using pd.concat(axis=1) instead.
To get a de-fragmented frame, use `newframe = frame.copy()`

df1 = df1.append(df, ignore_index =True)

FIX
pd.concat((df1,df),axis=0)

REPRODUCE

import numpy as np
import pandas as pd

# Sample `df`
np.random.seed(5)
df = pd.DataFrame(np.random.randint(1, 100, (4, 5000)))
df.columns = df.columns + 1

num_sims = len(df.columns)  # Placeholder for `num_sims`
ranking = pd.DataFrame()  # Placeholder for `ranking`

for x in range(1, num_sims + 1):
    ranking[x] = df[x].rank(ascending=False, method='min')

FIX

ranking = pd.DataFrame()  # Placeholder for `ranking`
ranking = pd.concat(
    [ranking, df[range(1, num_sims + 1)].rank(ascending=False, method='min')],
    axis=1
)


TEST

import numpy as np
import pandas as pd

np.random.seed(5)
df = pd.DataFrame(np.random.randint(1, 100, (4, 5000)))
df.columns = df.columns + 1
ranking = pd.DataFrame()
num_sims = len(df.columns)

for x in range(1, num_sims + 1):
    ranking[x] = df[x].rank(ascending=False, method='min')

print(ranking.eq(pd.concat(
    [pd.DataFrame(),
     df[range(1, num_sims + 1)].rank(ascending=False, method='min')],
    axis=1
)).all(axis=None))  # True


--------------------------------------------------------------------------------------

####################################################
numpy.core._exceptions._ArrayMemoryError:
Unable to allocate 19.1 GiB for an array with shape (3060, 838005) and data type float64
####################################################

https://stackoverflow.com/questions/57507832/unable-to-allocate-array-with-shape-and-data-type
https://exerror.com/numpy-core-_exceptions-memoryerror-unable-to-allocate-array-with-shape/
https://www.mssqltips.com/sqlservertip/7282/pandas-dataframes-memory-error-memoryerror-unable-to-allocate/

cat /proc/sys/vm/overcommit_memory

>> import numpy as np
>>> a = np.zeros((156816, 36, 53806), dtype='uint8')
>>> a.nbytes


# mask = np.zeros(edges.shape)
# mask = np.zeros(edges.shape,dtype='uint8')
# mask = np.zeros((789412, 78, 98754), dtype='uint8')

--------------------------------------------------------------------------------------


####################################################
Perl and Python scripts to check that the ulimit to
number of open files is enforced per process per user
####################################################

https://gist.github.com/makeittotop/d4974a2dc5a19a25e9d3

#!/usr/bin/perl

# provided that ulimit -n returns 1024
# 1, 2, 3 are stdin, stdout and stderr respectively

$count = 0;
@filedescriptors;

while ($count <= 1024) {
    $FILE = ${count};
    open $FILE, ">", "/tmp/test-$count" or die "\n\n FDs: $count $!";
    push(@filedescriptors, $FILE);
    $count ++;
}
# FDs: 1021 Too many open files at ./test.pl line 8.


# to remove files once the tests are done :D find . -maxdepth 1 -name "test*" -print0 | sudo xargs -0 rm

#!/usr/bin/env python

files = []

try:
    for f in range(1, 1025):
        fh = open("test-{0}".format(f), "w")
        print "Opened file number ...", f
        files.append(fh)
except IOError as e:
    print "IOError: ", str(e)











