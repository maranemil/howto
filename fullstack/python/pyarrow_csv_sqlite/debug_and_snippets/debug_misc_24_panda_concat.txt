
######################################################
pd.concat
######################################################

https://towardsdatascience.com/pandas-concat-tricks-you-should-know-to-speed-up-your-data-analysis-cd3d4fdfe6dd
https://github.com/BindiChen/machine-learning/blob/main/data-analysis/016-pandas-concat/pandas-concat.ipynb
https://pandas.pydata.org/docs/reference/api/pandas.concat.html
https://pandas.pydata.org/docs/user_guide/merging.html
https://datagy.io/pandas-merge-concat/

df1 = pd.DataFrame({
    'name': ['A', 'B', 'C', 'D'],
    'math': [60,89,82,70],
    'physics': [66,95,83,66],
    'chemistry': [61,91,77,70]
})
df2 = pd.DataFrame({
    'name': ['E', 'F', 'G', 'H'],
    'math': [66,95,83,66],
    'physics': [60,89,82,70],
    'chemistry': [90,81,78,90]
})


pd.concat([df1, df2])
pd.concat([df1, df2], ignore_index=True)
pd.concat([df1, df2], axis=1)
pd.concat([df1, df2], keys=['Year 1','Year 2'])

pd.concat(
    [df1, df2],
    keys=['Year 1', 'Year 2'],
    names=['Class', None],
)

pd.concat(
    [df1, df2],
    keys=['Year 1', 'Year 2'],
    names=['Class', None],
).reset_index(level=0)
# reset_index(level='Class')

pd.concat([df1, df2], sort=True)

custom_sort = ['math', 'chemistry', 'physics', 'name']
res = pd.concat([df1, df2])
res[custom_sort]



import pathlib2 as pl2
ps = pl2.Path('data/sp3')
dfs = (
    pd.read_csv(p, encoding='utf8') for p in ps.glob('*.csv')
)
res = pd.concat(dfs)
res

#########################################################################
pd.arrays.SparseArray
#########################################################################

https://pandas.pydata.org/docs/reference/api/pandas.arrays.SparseArray.html
https://programtalk.com/python-more-examples/pandas.arrays.SparseArray/
https://pandas.pydata.org/docs/user_guide/sparse.html
https://www.programcreek.com/python/example/125183/pandas.SparseArray


rom pandas.arrays import SparseArray
arr = SparseArray([0, 0, 1, 2])
arr
[0, 0, 1, 2]
Fill: 0
IntIndex
Indices: array([2, 3], dtype=int32)




arr = np.random.randn(10)
arr[2:-2] = np.nan
ts = pd.Series(pd.arrays.SparseArray(arr))



df = pd.DataFrame(np.random.randn(10000, 4))
df.iloc[:9998] = np.nan
sdf = df.astype(pd.SparseDtype("float", np.nan))
sdf.head()
sdf.sparse.density
'dense : {:0.2f} bytes'.format(df.memory_usage().sum() / 1e3)
'sparse: {:0.2f} bytes'.format(sdf.memory_usage().sum() / 1e3)


arr = np.random.randn(10)
arr[2:5] = np.nan
arr[7:8] = np.nan
sparr = pd.arrays.SparseArray(arr)
sparr
np.asarray(sparr)
sparr.dtype
pd.SparseDtype(np.dtype('datetime64[ns]'))
pd.SparseDtype(np.dtype('datetime64[ns]'),
               fill_value=pd.Timestamp('2017-01-01'))
pd.array([1, 0, 0, 2], dtype='Sparse[int]')


s = pd.Series([0, 0, 1, 2], dtype="Sparse[int]")
s.sparse.density
s.sparse.fill_value




def test_where_sparse():
    # GH#17198 make sure we dont get an AttributeError for sp_index
    ser = pd.Series(pd.arrays.SparseArray([1, 2]))
    result = ser.where(ser >= 2, 0)
    expected = pd.Series(pd.arrays.SparseArray([0, 2]))
    tm.assert_series_equal(result, expected)

def test_where_empty_series_and_empty_cond_having_non_bool_dtypes():
 pass




def test_unary_ufunc(ufunc, sparse):
    # Test that ufunc(Series) == Series(ufunc)
    array = np.random.randint(0, 10, 10, dtype="int64")
    array[::2] = 0
    if sparse:
        array = SparseArray(array, dtype=pd.SparseDtype("int64", 0))

    index = list(string.ascii_letters[:10])
    name = "name"
    series = pd.Series(array, index=index, name=name)

    result = ufunc(series)
    expected = pd.Series(ufunc(array), index=index, name=name)
    tm.assert_series_equal(result, expected)

@pytest.mark.parametrize("ufunc", BINARY_UFUNCS)

...


def test_constructor_from_sparse_array(self):
        # https://github.com/pandas-dev/pandas/issues/35843
        values = [
            Timestamp("2012-05-01T01:00:00.000000"),
            Timestamp("2016-05-01T01:00:00.000000"),
        ]
        arr = pd.arrays.SparseArray(values)
        result = Index(arr)
        expected = DatetimeIndex(values)
        tm.assert_index_equal(result, expected)

def test_construction_caching(self):
	pass

...


def _create_sp_series():
    nan = np.nan
    # nan-based
    arr = np.arange(15, dtype=np.float64)
    arr[7:12] = nan
    arr[-1:] = nan
    bseries = Series(SparseArray(arr, kind="block"))
    bseries.name = "bseries"
    return bseries

def _create_sp_tsseries():

...


def test_unary_ufunc(ufunc, sparse):
    # Test that ufunc(pd.Series) == pd.Series(ufunc)
    arr = np.random.randint(0, 10, 10, dtype="int64")
    arr[::2] = 0
    if sparse:
        arr = SparseArray(arr, dtype=pd.SparseDtype("int64", 0))
    index = list(string.ascii_letters[:10])
    name = "name"
    series = pd.Series(arr, index=index, name=name)
    result = ufunc(series)
    expected = pd.Series(ufunc(arr), index=index, name=name)
    tm.assert_series_equal(result, expected)


@pytest.mark.parametrize("ufunc", BINARY_UFUNCS)

...


def test_concat_dense_sparse():
    # GH 30668
    a = Series(pd.arrays.SparseArray([1, None]), dtype=float)
    b = Series([1], dtype=float)
    expected = Series(data=[1, None, 1], index=[0, 1, 0]).astype(
        pd.SparseDtype(np.float64, None)
    )
    result = pd.concat([a, b], axis=0)
    tm.assert_series_equal(result, expected)

@pytest.mark.parametrize("keys", [["e", "f", "f"], ["f", "e", "f"]])



#########################################################################
Concatenation (Combining Data Tables) with Pandas and Python
#########################################################################

https://www.dataquest.io/blog/pandas-concatenation-tutorial/
https://www.tutorialspoint.com/python_pandas/python_pandas_concatenation.htm
https://linuxhint.com/pandas-concatenate-two-dataframes/
https://www.skytowner.com/explore/pandas_concat_method
https://www.terality.com/post/pandas-concat-vs-append
https://www.anycodings.com/questions/dataframe-add-multiple-columns-from-list-with-each-column-name-created
https://neuralprophet.com/html/lagged_covariates_energy_ercot.html

import pandas as pd
import matplotlib.pyplot as plt

north_america = pd.read_csv('./north_america_2000_2010.csv', index_col=0)
south_america = pd.read_csv('./south_america_2000_2010.csv', index_col=0)
north_america
north_america.plot()

north_america.transpose().plot(title='Average Labor Hours Per Year')
plt.show()

south_america.transpose().plot(title='Average Labor Hours Per Year')
plt.show()

# result = pd.concat([list of DataFrames], axis=0, join='outer', ignore_index=False)
americas = pd.concat([north_america, south_america])
americas

americas_dfs = [americas]
for year in range(2011, 2016):
    filename = "./americas_{}.csv".format(year)
    df = pd.read_csv(filename, index_col=0)
americas_dfs.append(df)
americas_dfs[1]


americas = pd.concat(americas_dfs, axis=1)
americas.index.names = ['Country']
americas

americas.transpose().plot(title='Average Labor Hours Per Year')
plt.show()

asia = pd.read_csv('./asia_2000_2015.csv', index_col=0)
asia
europe = pd.read_csv('./europe_2000_2015.csv', index_col=0)
europe.head()
south_pacific = pd.read_csv('./south_pacific_2000_2015.csv', index_col=0)
south_pacific

world = americas.append([asia, europe, south_pacific])
world.index

world.transpose().plot(title='Average Labor Hours Per Year')
plt.show()


world.transpose().plot(figsize=(10,10), colormap='rainbow', linewidth=2, title='Average Labor Hours Per Year')
plt.legend(loc='right', bbox_to_anchor=(1.3, 0.5))
plt.show()

world_historical = pd.merge(historical, world, left_index=True, right_index=True, how='right')
print(world_historical.shape)
world_historical.head()
world_historical = historical.join(world, how='right')world_historical.head()
world_historical.sort_index(inplace=True)
world_historical.transpose().plot(figsize=(15,10), colormap='rainbow', linewidth=2, title='Average Labor Hours Per Year')
plt.legend(loc='right', bbox_to_anchor=(1.15, 0.5))
plt.show()

#########################################################################
https://www.terality.com/post/pandas-concat-vs-append
#########################################################################


-- CODE language-python --
df = pd.DataFrame(range(1_000_000))
dfs = [df] * 100

%%time
df_result = dfs[0]
for df in dfs[1:]:
   df_result = df_result.append(df)
CPU times: user 7.92 s, sys: 6.28 s, total: 14.2 s
Wall time: 14.2 s

%%time
df_result = pd.concat(dfs)
CPU times: user 157 ms, sys: 134 ms, total: 291 ms
Wall time: 289 ms

#########################################################################
https://pythonfordatascienceorg.wordpress.com/join-and-merge-data-frames-python/
#########################################################################

dataframe1 = pd.DataFrame({'ID': ['0011','0013','0014','0016','0017'],
                           'First Name': ['Joseph', 'Mike', 'Jordan', 'Steven', 'Susan']})
dataframe2 = pd.DataFrame({'ID': ['0010','0011','0013','0014','0017'],
                           'Last Name': ['Gordan', 'Johnson', 'Might' , 'Jackson', 'Shack']})
dataframe3 = pd.DataFrame({'ID': ['0020','0022','0025'],
                           'First Name': ['Adam', 'Jackie', 'Sue']})
dataframe4 = pd.DataFrame({'Key': ['0020','0022','0025'],
                           'Scores': [95, 90, 80]})

new_concat_ROWS_dataframe = pd.concat([dataframe1, dataframe3])
new_concat_ROWS_dataframe
new_concat_ROWS_dataframe = pd.concat([dataframe1, dataframe3], ignore_index= "true")
new_concat_ROWS_dataframe
new_concat_COL_dataframe = pd.concat([dataframe1, dataframe3], axis=1)
new_concat_COL_dataframe
new_merged_dataframe = pd.merge(dataframe1, dataframe2, on= "ID", how= "inner")
new_merged_dataframe
new_OUTER_merged_dataframe = pd.merge(dataframe1, dataframe2, on= "ID", how= "outer")
new_OUTER_merged_dataframe
new_LEFT_merged_dataframe = pd.merge(dataframe1, dataframe2, on= "ID", how= "left")
new_LEFT_merged_dataframe
new_RIGHT_merged_dataframe = pd.merge(dataframe1, dataframe2, on= "ID", how= "right")
new_RIGHT_merged_dataframe
merged_dataframe = pd.merge(dataframe3, dataframe4, left_on= "ID", right_on= "Key", how= "inner")
merged_dataframe


#########################################################################
https://neuralprophet.com/html/lagged_covariates_energy_ercot.html
https://pymodulon.readthedocs.io/en/latest/tutorials/plotting_functions.html
https://stackoverflow.com/questions/68292862/performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-o
#########################################################################

import pandas as pd
from neuralprophet import NeuralProphet, set_log_level

data_location = "https://raw.githubusercontent.com/ourownstory/neuralprophet-data/main/datasets/"
df_ercot = pd.read_csv(data_location + "multivariate/load_ercot_regions.csv")
df_ercot_y = pd.read_csv(data_location + "energy/load_ercot.csv")
df_ercot['y'] = df_ercot_y['y']
df_ercot.head()

regions = list(df_ercot)[1:-1]
df_ercot['y'].isnull().sum()

# Baseline Model

df = pd.DataFrame({"ds": df_ercot["ds"], "y": df_ercot["y"]})
m = NeuralProphet(
    learning_rate=0.1,
)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
# comp = m.plot_components(forecast)
param = m.plot_parameters()

# 3-steps ahead AR Model

df = pd.DataFrame({"ds": df_ercot["ds"], "y": df_ercot["y"]})
m = NeuralProphet(
    n_forecasts=3,
    n_lags=3,
    learning_rate=0.1,
)
m = m.highlight_nth_step_ahead_of_each_forecast(3)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
# comp = m.plot_components(forecast[-7*24:])
param = m.plot_parameters()

# 3-steps ahead AR and Lagged Regressors Model

df = df_ercot
m = NeuralProphet(
    n_forecasts=3,
    n_lags=3,
    learning_rate= 0.1,
)
m = m.add_lagged_regressor(names=regions) #, only_last_value=True)
m.highlight_nth_step_ahead_of_each_forecast(3)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
# comp = m.plot_components(forecast[-7*24:])
param = m.plot_parameters()


# 3-steps ahead AR and Lagged Regressors Model

df = df_ercot
m = NeuralProphet(
    n_forecasts=3,
    n_lags=3,
    learning_rate= 0.1,
)
m = m.add_lagged_regressor(names=regions) #, only_last_value=True)
m.highlight_nth_step_ahead_of_each_forecast(3)
metrics = m.fit(df, freq="H")


forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
# comp = m.plot_components(forecast[-7*24:])
param = m.plot_parameters()


# 24-steps ahead Long AR Model

df = pd.DataFrame({"ds": df_ercot["ds"], "y": df_ercot["y"]})
m = NeuralProphet(
    n_forecasts=24,
    n_lags=7*24,
    learning_rate=0.1,
)
m.highlight_nth_step_ahead_of_each_forecast(24)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
# comp = m.plot_components(forecast[-7*24:])
param = m.plot_parameters()


# 24-steps ahead Long AR Model with last observation of Lagged Regressors

df = df_ercot
m = NeuralProphet(
    n_forecasts=24,
    n_lags=7*24,
    learning_rate=0.1,
)
m = m.add_lagged_regressor(names=regions, only_last_value=True)
m.highlight_nth_step_ahead_of_each_forecast(24)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
# comp = m.plot_components(forecast[-7*24:])
param = m.plot_parameters()

# 24-steps ahead AR Model with full Lagged Regressors

df = df_ercot
m = NeuralProphet(
    n_forecasts=24,
    n_lags=24,
    learning_rate=0.01,
)
m = m.add_lagged_regressor(names=regions)
m.highlight_nth_step_ahead_of_each_forecast(24)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
# comp = m.plot_components(forecast[-7*24:])
param = m.plot_parameters()

24-steps ahead Neural Model with Long AR

df = pd.DataFrame({"ds": df_ercot["ds"], "y": df_ercot["y"]})
m = NeuralProphet(
    n_forecasts=24,
    n_lags=7*24,
    learning_rate=0.01,
    num_hidden_layers=1,
    d_hidden=16,
)
m = m.highlight_nth_step_ahead_of_each_forecast(24)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
comp = m.plot_components(forecast[-7*24:])
# param = m.plot_parameters()


# 24-steps ahead Neural Model with Long AR and Lagged Regressors

df = df_ercot
m = NeuralProphet(
    n_forecasts=24,
    n_lags=7*24,
    learning_rate=0.01,
    num_hidden_layers=1,
    d_hidden=16,
)
m = m.add_lagged_regressor(names=regions)#, only_last_value=True)
m = m.highlight_nth_step_ahead_of_each_forecast(24)
metrics = m.fit(df, freq="H")

forecast = m.predict(df)
# fig = m.plot(forecast)
fig1 = m.plot(forecast[-365*24:])
fig2 = m.plot(forecast[-7*24:])
comp = m.plot_components(forecast[-7*24:])


----------------------------------------------------
#########################################################################
PerformanceWarning: DataFrame is highly fragmented.
This is usually the result of calling `frame.insert` many times, which has poor performance.
Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`
#########################################################################

https://stackoverflow.com/questions/42786804/concatenate-all-columns-in-a-pandas-dataframe
https://pandas.pydata.org/pandas-docs/version/0.20/merging.html


df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
   ...:                     'B': ['B0', 'B1', 'B2', 'B3'],
   ...:                     'C': ['C0', 'C1', 'C2', 'C3'],
   ...:                     'D': ['D0', 'D1', 'D2', 'D3']},
   ...:                     index=[0, 1, 2, 3])
   ...:

In [2]: df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],
   ...:                     'B': ['B4', 'B5', 'B6', 'B7'],
   ...:                     'C': ['C4', 'C5', 'C6', 'C7'],
   ...:                     'D': ['D4', 'D5', 'D6', 'D7']},
   ...:                      index=[4, 5, 6, 7])
   ...:

In [3]: df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],
   ...:                     'B': ['B8', 'B9', 'B10', 'B11'],
   ...:                     'C': ['C8', 'C9', 'C10', 'C11'],
   ...:                     'D': ['D8', 'D9', 'D10', 'D11']},
   ...:                     index=[8, 9, 10, 11])
   ...:

In [4]: frames = [df1, df2, df3]
In [5]: result = pd.concat(frames)

frames = [ process_your_file(f) for f in files ]
result = pd.concat(frames)


In [8]: df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],
   ...:                  'D': ['D2', 'D3', 'D6', 'D7'],
   ...:                  'F': ['F2', 'F3', 'F6', 'F7']},
   ...:                 index=[2, 3, 6, 7])
   ...:

In [9]: result = pd.concat([df1, df4], axis=1)
In [10]: result = pd.concat([df1, df4], axis=1, join='inner')
In [11]: result = pd.concat([df1, df4], axis=1, join_axes=[df1.index])
In [12]: result = df1.append(df2)
In [13]: result = df1.append(df4)
In [14]: result = df1.append([df2, df3])
In [15]: result = pd.concat([df1, df4], ignore_index=True)
In [16]: result = df1.append(df4, ignore_index=True)

In [17]: s1 = pd.Series(['X0', 'X1', 'X2', 'X3'], name='X')
In [18]: result = pd.concat([df1, s1], axis=1)

In [19]: s2 = pd.Series(['_0', '_1', '_2', '_3'])
In [20]: result = pd.concat([df1, s2, s2, s2], axis=1)
In [21]: result = pd.concat([df1, s1], axis=1, ignore_index=True)


In [36]: dicts = [{'A': 1, 'B': 2, 'C': 3, 'X': 4},
   ....:          {'A': 5, 'B': 6, 'C': 7, 'Y': 8}]
   ....:

In [37]: result = df1.append(dicts, ignore_index=True)





In [38]: left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
   ....:                      'A': ['A0', 'A1', 'A2', 'A3'],
   ....:                      'B': ['B0', 'B1', 'B2', 'B3']})
   ....:

In [39]: right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
   ....:                       'C': ['C0', 'C1', 'C2', 'C3'],
   ....:                       'D': ['D0', 'D1', 'D2', 'D3']})
   ....:

In [40]: result = pd.merge(left, right, on='key')




In [41]: left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],
   ....:                      'key2': ['K0', 'K1', 'K0', 'K1'],
   ....:                      'A': ['A0', 'A1', 'A2', 'A3'],
   ....:                      'B': ['B0', 'B1', 'B2', 'B3']})
   ....:

In [42]: right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],
   ....:                       'key2': ['K0', 'K0', 'K0', 'K0'],
   ....:                       'C': ['C0', 'C1', 'C2', 'C3'],
   ....:                       'D': ['D0', 'D1', 'D2', 'D3']})
   ....:

In [43]: result = pd.merge(left, right, on=['key1', 'key2'])




Merge method	SQL Join Name	Description

left	LEFT OUTER JOIN	Use keys from left frame only
right	RIGHT OUTER JOIN	Use keys from right frame only
outer	FULL OUTER JOIN	Use union of keys from both frames
inner	INNER JOIN	Use intersection of keys from both frames


In [44]: result = pd.merge(left, right, how='left', on=['key1', 'key2'])
In [45]: result = pd.merge(left, right, how='right', on=['key1', 'key2'])
In [46]: result = pd.merge(left, right, how='outer', on=['key1', 'key2'])
In [47]: result = pd.merge(left, right, how='inner', on=['key1', 'key2'])



In [48]: left = pd.DataFrame({'A' : [1,2], 'B' : [2, 2]})
In [49]: right = pd.DataFrame({'A' : [4,5,6], 'B': [2,2,2]})
In [50]: result = pd.merge(left, right, on='B', how='outer')



In [74]: left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],
   ....:                      'B': ['B0', 'B1', 'B2']},
   ....:                      index=['K0', 'K1', 'K2'])
   ....:

In [75]: right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],
   ....:                       'D': ['D0', 'D2', 'D3']},
   ....:                       index=['K0', 'K2', 'K3'])
   ....:
In [76]: result = left.join(right)



#########################################################################
https://sparkbyexamples.com/pandas/pandas-concatenate-two-columns/
#########################################################################
import pandas as pd
technologies = ({
     'Courses':["Spark","PySpark","Hadoop","Python","pandas"],
     'Fee' :[20000,25000,26000,22000,24000],
     'Duration':['30days','40days','35days','40days','60days'],
     'Discount':[1000,1500,2500,2100,2000]
               })
df = pd.DataFrame(technologies)
print(df)

# Using + operator to combine two columns
df["Period"] = df['Courses'].astype(str) +"-"+ df["Duration"]
print(df)

# Using apply() method to combine two columns of text
df["Period"] = df[["Courses", "Duration"]].apply("-".join, axis=1)
print(df)


# Using DataFrame.apply() and lambda function
df["Period"] = df[["Courses", "Duration"]].apply(lambda x: " ".join(x), axis =1)
print(df)

#########################################################################
https://github.com/twopirllc/pandas-ta/issues/340
PerformanceWarning: DataFrame is highly fragmented #340
#########################################################################

To Reproduce

I start with a DataFrame populated with just the right columns for pandas-ta. Then add technical indicators like so:

for n in self.__SMA_INTERVALS:

	key = "SMA_{}".format(n)

	df[key] = ta.sma(df["Close"], length=n)
	df[key] = df[key].fillna(0)

	df = df.copy()

#########################################################################
https://jakevdp.github.io/PythonDataScienceHandbook/03.06-concat-and-append.html
#########################################################################
import pandas as pd
import numpy as np

def make_df(cols, ind):
    """Quickly make a DataFrame"""
    data = {c: [str(c) + str(i) for i in ind]
            for c in cols}
    return pd.DataFrame(data, ind)

# example DataFrame
make_df('ABC', range(3))


# Signature in Pandas v0.18
pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,
          keys=None, levels=None, names=None, verify_integrity=False,
          copy=True)


ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])
ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])
pd.concat([ser1, ser2])


df1 = make_df('AB', [1, 2])
df2 = make_df('AB', [3, 4])
display('df1', 'df2', 'pd.concat([df1, df2])')


df3 = make_df('AB', [0, 1])
df4 = make_df('CD', [0, 1])
display('df3', 'df4', "pd.concat([df3, df4], axis='col')")


x = make_df('AB', [0, 1])
y = make_df('AB', [2, 3])
y.index = x.index  # make duplicate indices!
display('x', 'y', 'pd.concat([x, y])')



try:
    pd.concat([x, y], verify_integrity=True)
except ValueError as e:
    print("ValueError:", e)



df5 = make_df('ABC', [1, 2])
df6 = make_df('BCD', [3, 4])
display('df5', 'df6', 'pd.concat([df5, df6])')
display('df1', 'df2', 'df1.append(df2)')

#########################################################################
Two data sets containing the same columns but different rows of data
https://datacarpentry.org/python-socialsci/11-joins/index.html
#########################################################################

import pandas as pd

df_SN7577i_a = pd.read_csv("SN7577i_a.csv")
df_SN7577i_b = pd.read_csv("SN7577i_b.csv")

print(df_SN7577i_a)
print(df_SN7577i_b)

df_all_rows = pd.concat([df_SN7577i_a, df_SN7577i_b])
df_all_rows

df_all_rows=df_all_rows.reset_index(drop=True)
# or, alternatively, there's the `ignore_index` option in the `pd.concat()` function:
df_all_rows = pd.concat([df_SN7577i_a, df_SN7577i_b], ignore_index=True)
df_all_rows

df_SN7577i_aa = pd.read_csv("SN7577i_aa.csv")
df_SN7577i_bb = pd.read_csv("SN7577i_bb.csv")
df_all_rows = pd.concat([df_SN7577i_aa, df_SN7577i_bb])
df_all_rows

# Adding the columns from one Dataframe to those of another Dataframe
df_SN7577i_c = pd.read_csv("SN7577i_c.csv")
df_SN7577i_d = pd.read_csv("SN7577i_d.csv")
df_all_cols = pd.concat([df_SN7577i_c, df_SN7577i_d], axis = 1)
df_all_cols


# Using merge to join columns
df_cd = pd.merge(df_SN7577i_c, df_SN7577i_d, how='inner')
df_cd
df_cd = pd.merge(df_SN7577i_c, df_SN7577i_d, how='inner', on = 'Id')
df_cd = pd.merge(df_SN7577i_c, df_SN7577i_d, how='inner', left_on = 'Id', right_on = 'Id')


